{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxEP3lOmiFsd"
      },
      "source": [
        "#Load torchtext and initialize XLM-R model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P5DEMVJSHrD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "\n",
        "from torchtext.models import RobertaClassificationHead\n",
        "from torchtext.functional import to_tensor\n",
        "\n",
        "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
        "classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\n",
        "model = xlmr_large.get_model(head=classifier_head)\n",
        "\n",
        "# Put model into inference mode (reduces runtime even without BT - esp for GPU execution, required for Better Transformer)\n",
        "model.eval()\n",
        "\n",
        "# Define input transform\n",
        "transform = xlmr_large.transform()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Information"
      ],
      "metadata": {
        "id": "_i8J9wc_kDaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "cpu = platform.processor()\n",
        "gpu = torch.cuda.get_device_name(DEVICE)\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torch cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"CPU type: {cpu}\")\n",
        "print(f\"GPU type: {gpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mJeGWu7kKCf",
        "outputId": "9640fe9f-4d53-437e-da9f-c84c6217afc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 1.12.0+cu113\n",
            "torch cuda available: True\n",
            "CPU type: x86_64\n",
            "GPU type: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check default sparsity support setting\n",
        "Sparsity support enables transformers to skip padding in inputs."
      ],
      "metadata": {
        "id": "EY-1QDY2nS52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq61w_dQndlC",
        "outputId": "4a0696ff-e422-4465-d627-6d543ef82a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark setup"
      ],
      "metadata": {
        "id": "TUokmFVhnvfK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqFcmAcaiDgE"
      },
      "source": [
        "###Define inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2ETItuybcwR"
      },
      "outputs": [],
      "source": [
        "small_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\"\n",
        "]\n",
        "big_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\",\n",
        "               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
        "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
        "if you still try to defend the infamies and horrors perpetrated by\n",
        "that Antichrist- I really believe he is Antichrist- I will have\n",
        "nothing more to do with you and you are no longer my friend, no longer\n",
        "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
        "I have frightened you- sit down and tell me all the news.`\n",
        "\n",
        "It was in July, 1805, and the speaker was the well-known Anna\n",
        "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
        "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
        "of high rank and importance, who was the first to arrive at her\n",
        "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
        "she said, suffering from la grippe; grippe being then a new word in\n",
        "St. Petersburg, used only by the elite.\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rPLNTKXiAZ4"
      },
      "source": [
        "###Select small or big input set\n",
        "\n",
        "Modify the assignment to input_batch below to select either the small_input_batch or big_inoput_batch, or substitute your own inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-21X4muzWbuX",
        "outputId": "6649b871-46a9-440e-8e69-ba08a6ad85b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "input_batch=big_input_batch\n",
        "\n",
        "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
        "output = model(model_input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXSw69v4hxe2"
      },
      "source": [
        "###Iteration count for performance measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF3OFvIdbYnM"
      },
      "outputs": [],
      "source": [
        "ITERATIONS=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHcLbYihta8"
      },
      "source": [
        "#Measure CPU  performance with slow and fast path, without and with sparsity\n",
        "\n",
        "Sparsity support enables transformers to skip padding in inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU performance without BT sparsity"
      ],
      "metadata": {
        "id": "8_td53nHgdQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = False"
      ],
      "metadata": {
        "id": "8aItZ7EeghTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLZsAR15Wkgv",
        "outputId": "cdc77881-f492-4a66-f1fa-2455efc9ebff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::addmm        63.40%       41.691s        64.59%       42.478s      57.403ms           740  \n",
            "                   aten::mm        21.27%       13.990s        21.27%       13.990s      58.291ms           240  \n",
            "                aten::copy_         3.58%        2.356s         3.58%        2.356s     875.793us          2690  \n",
            "                  aten::bmm         2.75%        1.811s         2.75%        1.811s       7.547ms           240  \n",
            "             aten::_softmax         2.40%        1.578s         2.40%        1.578s       6.573ms           240  \n",
            "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 65.761s\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             aten::addmm        34.35%       23.131s        34.73%       23.386s      46.772ms           500  \n",
            "                 aten::_addmm_activation        27.26%       18.353s        29.42%       19.811s      82.548ms           240  \n",
            "                                aten::mm        20.61%       13.875s        20.61%       13.875s      57.811ms           240  \n",
            "                   aten::_masked_softmax         6.52%        4.392s         6.53%        4.395s      18.311ms           240  \n",
            "                               aten::bmm         4.33%        2.914s         4.33%        2.915s       6.072ms           480  \n",
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 67.331s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CPU performance with BT sparsity"
      ],
      "metadata": {
        "id": "PpIGGS5egqRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = True"
      ],
      "metadata": {
        "id": "UdPM9U1Qg5y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq4_0E4-gzwV",
        "outputId": "97b086ba-eab5-4c41-d80c-c81928936dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::addmm        63.66%       41.845s        64.78%       42.575s      57.533ms           740  \n",
            "                   aten::mm        21.40%       14.067s        21.40%       14.067s      58.614ms           240  \n",
            "                aten::copy_         3.39%        2.228s         3.39%        2.228s     828.338us          2690  \n",
            "                  aten::bmm         2.77%        1.824s         2.77%        1.824s       7.599ms           240  \n",
            "             aten::_softmax         2.29%        1.507s         2.29%        1.507s       6.281ms           240  \n",
            "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 65.727s\n",
            "\n",
            "fast path:\n",
            "==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py:232: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at  ../aten/src/ATen/NestedTensorImpl.cpp:50.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             aten::addmm        26.80%        8.664s        27.04%        8.743s      17.487ms           500  \n",
            "                 aten::_addmm_activation        21.29%        6.884s        22.87%        7.395s      30.814ms           240  \n",
            "                                aten::mm        15.88%        5.134s        15.88%        5.134s      21.394ms           240  \n",
            "                   aten::_masked_softmax        12.97%        4.194s        12.98%        4.195s      17.480ms           240  \n",
            "                               aten::bmm         8.82%        2.852s         8.82%        2.852s       5.941ms           480  \n",
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 32.331s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBCDGjd-g2ny"
      },
      "source": [
        "#Measure DEVICE performance with slow and fast path, without and with sparsity\n",
        "\n",
        "Please ensure that the runtime has GPUs enabled to see the performance benefits of Better Transformer fastpath execution on GPUs. You can confirm and change the Runtime type in the Google Colab menu with (Runtime > Change Runtime Type)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "model_input = model_input.to(DEVICE)"
      ],
      "metadata": {
        "id": "bEMADUHesGjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEVICE performance without BT sparsity"
      ],
      "metadata": {
        "id": "rcLpVrBdhEeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor=False"
      ],
      "metadata": {
        "id": "bMlwVTzfhOs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7hMbLFbWwPH",
        "outputId": "fdacccb8-a60e-47ce-9d6b-caeaf61ad8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                               aten::mm         1.01%      14.178ms        44.20%     622.315ms       2.593ms     859.598ms        43.74%     859.598ms       3.582ms           240  \n",
            "                                            aten::addmm         3.29%      46.383ms         5.19%      73.045ms      98.709us     756.916ms        38.52%     761.477ms       1.029ms           740  \n",
            "                                            aten::copy_         1.26%      17.748ms         2.22%      31.256ms      18.386us      49.933ms         2.54%      49.933ms      29.372us          1700  \n",
            "                                          aten::baddbmm         0.89%      12.571ms         1.43%      20.191ms      84.129us      42.811ms         2.18%      57.249ms     238.537us           240  \n",
            "                                              aten::bmm         0.45%       6.283ms         0.54%       7.586ms      31.608us      31.677ms         1.61%      31.677ms     131.988us           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.408s\n",
            "Self CUDA time total: 1.965s\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         2.10%      25.898ms         4.60%      56.673ms     113.346us     434.598ms        33.15%     437.716ms     875.432us           500  \n",
            "                                aten::_addmm_activation         1.15%      14.139ms         1.89%      23.213ms      96.721us     350.782ms        26.75%     375.835ms       1.566ms           240  \n",
            "                                               aten::mm         0.70%       8.600ms         1.00%      12.305ms      51.271us     261.590ms        19.95%     261.590ms       1.090ms           240  \n",
            "                                              aten::bmm         0.82%      10.038ms         1.03%      12.653ms      26.360us      64.215ms         4.90%      64.215ms     133.781us           480  \n",
            "                                  aten::_masked_softmax         0.46%       5.717ms         0.92%      11.280ms      47.000us      29.636ms         2.26%      31.117ms     129.654us           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.231s\n",
            "Self CUDA time total: 1.311s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XC23NVChSYG"
      },
      "source": [
        "### DEVICE performance performance with BT sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4E54ia6YYkc"
      },
      "outputs": [],
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKPGx-3zaGXn",
        "outputId": "8ee5be47-dbab-4509-b23e-e0159fa77426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         6.15%      48.067ms         9.02%      70.473ms      95.234us     764.116ms        55.84%     768.719ms       1.039ms           740  \n",
            "                                               aten::mm         1.40%      10.924ms         1.91%      14.890ms      62.042us     254.936ms        18.63%     254.936ms       1.062ms           240  \n",
            "                                            aten::copy_         2.24%      17.520ms         3.92%      30.668ms      18.040us      50.111ms         3.66%      50.111ms      29.477us          1700  \n",
            "                                          aten::baddbmm         1.61%      12.555ms         2.50%      19.545ms      81.438us      42.793ms         3.13%      57.277ms     238.654us           240  \n",
            "                                              aten::bmm         0.74%       5.805ms         0.89%       6.942ms      28.925us      31.709ms         2.32%      31.709ms     132.121us           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 781.406ms\n",
            "Self CUDA time total: 1.368s\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         3.54%      25.463ms         5.47%      39.314ms      78.628us     161.586ms        21.55%     164.918ms     329.836us           500  \n",
            "                                aten::_addmm_activation         2.00%      14.361ms         3.22%      23.155ms      96.479us     132.618ms        17.68%     141.226ms     588.442us           240  \n",
            "                                               aten::mm         1.23%       8.851ms         1.76%      12.657ms      52.737us      92.039ms        12.27%      92.039ms     383.496us           240  \n",
            "                                              aten::bmm         1.48%      10.635ms         1.87%      13.466ms      28.054us      64.850ms         8.65%      64.850ms     135.104us           480  \n",
            "                   aten::_transformer_encoder_layer_fwd        21.83%     156.867ms        87.09%     625.735ms       2.607ms      49.826ms         6.64%     725.680ms       3.024ms           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 718.471ms\n",
            "Self CUDA time total: 749.945ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.to(DEVICE)\n",
        "model_input = model_input.to(DEVICE)\n",
        "\n",
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}