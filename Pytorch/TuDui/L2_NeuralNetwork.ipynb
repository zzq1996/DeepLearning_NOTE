{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## [nn.Module的使用](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)\n",
    "Base class for all neural network modules.Your models should also subclass this class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets\n",
    "\n",
    "\"\"\"\n",
    "Template:\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,input):\n",
    "        output=input+1\n",
    "        return output\n",
    "\n",
    "myNet=MyNet()\n",
    "x=torch.tensor(1.0)\n",
    "output=myNet(x)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-29T05:25:40.232999514Z",
     "start_time": "2023-06-29T05:25:39.331276986Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "\n",
    "Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "我们可以设计一个卷积核来检测图像的边缘。\n",
    "\n",
    "#### 1.1 [TORCH.NN.FUNCTIONAL.CONV2D](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([5, 5]), torch.Size([3, 3]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n",
    "- input: input tensor of shape (minibatch,in_channels,H,W)\n",
    "- weight: filters of shape (out_channels,in_channels/groups,H,W)\n",
    "\"\"\"\n",
    "input=torch.tensor([[1,2,0,3,1],\n",
    "                    [0,1,2,3,1],\n",
    "                    [1,2,1,0,0],\n",
    "                    [5,2,3,1,1],\n",
    "                    [2,1,0,1,1]])\n",
    "kernel=torch.tensor([[1,2,1],\n",
    "                     [0,1,0],\n",
    "                     [2,1,0]])\n",
    "\n",
    "input.shape,kernel.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 1, 5, 5]), torch.Size([1, 1, 3, 3]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "调用torch.reshape满足参数要求\n",
    "\"\"\"\n",
    "input=torch.reshape(input,[1,1,5,5])\n",
    "kernel=torch.reshape(kernel,[1,1,3,3])\n",
    "input.shape,kernel.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[[10, 12, 12],\n           [18, 16, 16],\n           [13,  9,  3]]]]),\n torch.Size([1, 1, 3, 3]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "output=F.conv2d(input,kernel,stride=1)\n",
    "output,output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "50000"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "二维卷积 Conv2d\n",
    "\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "- in_channels:(N,C,H,W), N is a batch size, C denotes a number of channels, H is a height of input planes in pixels, and W is width in pixels.\n",
    "\n",
    "\"\"\"\n",
    "import torchvision\n",
    "\n",
    "# 自定义数据集存放位置\n",
    "dataDir='/media/zhang/Disk0/dataset/d2l/data'\n",
    "\n",
    "# 构造数据集\n",
    "dataset=torchvision.datasets.CIFAR10(\n",
    "    root=dataDir,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "dataset.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "from torch.utils.data import DataLoader\n",
    "# 构造数据加载器\n",
    "dataloader=DataLoader(dataset,\n",
    "                      batch_size=64,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "MyConvNet(\n  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 搭建神经网络\n",
    "class MyConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=Conv2d(in_channels=3,out_channels=6,kernel_size=3,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        return x\n",
    "\n",
    "myConvNet=MyConvNet()\n",
    "myConvNet  # 查看网络结构"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([64, 3, 32, 32]), torch.Size([64]))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=next(enumerate(dataloader))\n",
    "img,tag=data[1][0],data[1][1]  # 获取第一批dataloader的内容（64个图片+标签）\n",
    "img.shape,tag.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 32, 32])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1=img[0]  # 64张图片中的第一个图片\n",
    "img1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f5481c23e80>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transToPIL=transforms.ToPILImage('RGB')  # Convert a tensor or an ndarray to PIL Image.\n",
    "\n",
    "plt.imshow(transToPIL(img1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6, 30, 30])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=myConvNet(img1)\n",
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should not have > 4 channels. Got 6 channels.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(\u001B[43mtransToPIL\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/transforms/transforms.py:227\u001B[0m, in \u001B[0;36mToPILImage.__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    225\u001B[0m \n\u001B[1;32m    226\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 227\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_pil_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/transforms/functional.py:271\u001B[0m, in \u001B[0;36mto_pil_image\u001B[0;34m(pic, mode)\u001B[0m\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;66;03m# check number of channels\u001B[39;00m\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[0;32m--> 271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should not have > 4 channels. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m channels.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pic, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m}:\n",
      "\u001B[0;31mValueError\u001B[0m: pic should not have > 4 channels. Got 6 channels."
     ]
    }
   ],
   "source": [
    "plt.imshow(transToPIL(output))  # 画不出来"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\"\"\"\n",
    "使用tensorboard进行可视化\n",
    "\"\"\"\n",
    "\n",
    "writer=SummaryWriter('./runsLog/conv2d')\n",
    "\n",
    "step=0\n",
    "for data in dataloader:\n",
    "    imgs,tag=data\n",
    "    output=myConvNet(imgs)\n",
    "    # torch.Size([64, 3, 32, 32]\n",
    "    writer.add_images('input',imgs,step)\n",
    "\n",
    "    # torch.Size([64, 6, 32, 32] -> torch.Size([xx, 3, 32, 32]\n",
    "    output=torch.reshape(output,(-1,3,30,30))\n",
    "    writer.add_images('output',output,step)\n",
    "    step=step+1\n",
    "\n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 查看效果\n",
    "!tensorboard --logdir=/home/zhang/PycharmProjects/d2l/zhang/chap0/pytorch/conv2d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. [Pooling layers](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n",
    "使用最大汇聚层以及大于1的步幅，可减少空间维度（如高度和宽度），减少数据运算量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "\n",
    "- ceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from torch.nn import MaxPool2d\n",
    "\n",
    "\n",
    "class MyPoolNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool=MaxPool2d(kernel_size=3,ceil_mode=False)\n",
    "\n",
    "    def forward(self,input):\n",
    "        output=self.maxpool(input)\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "myPoolNet=MyPoolNet()\n",
    "writer=SummaryWriter('./runsLog/maxpool')\n",
    "\n",
    "step=0\n",
    "for data in dataloader:\n",
    "    imgs,tag=data\n",
    "    output=myPoolNet(imgs)\n",
    "    # torch.Size([64, 3, 32, 32]\n",
    "    writer.add_images('input',imgs,step)\n",
    "    writer.add_images('output',output,step)\n",
    "    step=step+1\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 查看效果\n",
    "!tensorboard --logdir=/home/zhang/PycharmProjects/d2l/zhang/chap0/pytorch/maxpool"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Non-linear Activations\n",
    "为网络引入更多的非线性特征\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "writer=SummaryWriter('./runsLog/nonLinear')\n",
    "\n",
    "m=nn.Sigmoid()\n",
    "\n",
    "step=0\n",
    "for data in dataloader:\n",
    "    imgs,tag=data\n",
    "    output=m(imgs)\n",
    "    # torch.Size([64, 3, 32, 32]\n",
    "    writer.add_images('input',imgs,step)\n",
    "    writer.add_images('output',output,step)\n",
    "    step=step+1\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 查看效果\n",
    "!tensorboard --logdir=/home/zhang/PycharmProjects/d2l/zhang/chap0/pytorch/nonLinear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Linear Layers\n",
    "#### nn.Linear\n",
    "Applies a linear transformation to the incoming data: $y=xA^T+b$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.5493, -0.7204, -1.6284,  ...,  1.0329, -2.1003, -0.3053],\n         [ 0.5471, -1.0510,  1.2853,  ...,  1.0822, -0.6689, -0.1528],\n         [-0.5086,  0.0537,  1.4336,  ...,  0.2113,  0.4385,  0.5704],\n         ...,\n         [ 0.7557,  0.3367, -0.4350,  ...,  1.3493, -1.0269,  0.3579],\n         [-0.4948, -0.5987,  0.1063,  ..., -0.7283, -0.5547, -0.0756],\n         [-0.9159,  0.1178,  2.3531,  ...,  1.2157,  0.4289,  0.9492]]),\n torch.Size([128, 20]))"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "\"\"\"\n",
    "m=nn.Linear(20,10)\n",
    "input=torch.randn(128,20)\n",
    "input,input.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function randn in module torch:\n",
      "\n",
      "randn(...)\n",
      "    randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "    \n",
      "    \n",
      "    Returns a tensor filled with random numbers from a normal distribution\n",
      "    with mean `0` and variance `1` (also called the standard normal\n",
      "    distribution).\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
      "    \n",
      "    The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword args:\n",
      "        generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "        pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "            the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.randn(4)\n",
      "        tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
      "        >>> torch.randn(2, 3)\n",
      "        tensor([[ 1.5954,  2.8929, -1.0923],\n",
      "                [ 1.1719, -0.4709, -0.1996]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.randn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-0.1623,  0.0590,  0.5484,  ...,  0.2414,  0.3306, -0.1461],\n         [-0.2615, -0.3129, -0.4131,  ...,  1.1545, -0.6724, -0.7671],\n         [-0.2735, -0.1474, -0.7691,  ...,  1.0947,  0.2924,  0.0513],\n         ...,\n         [ 0.5980,  0.0273,  0.1005,  ..., -0.3917,  0.9769, -0.5045],\n         [ 0.3824, -0.9847, -0.9157,  ...,  0.1524,  0.2945,  0.0453],\n         [-0.5881,  0.0184,  0.5428,  ...,  0.2795, -0.0060, -0.9985]],\n        grad_fn=<AddmmBackward0>),\n torch.Size([128, 10]))"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=m(input)\n",
    "output,output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "['T_destination',\n '__annotations__',\n '__call__',\n '__class__',\n '__constants__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattr__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_apply',\n '_backward_hooks',\n '_buffers',\n '_call_impl',\n '_forward_hooks',\n '_forward_pre_hooks',\n '_get_backward_hooks',\n '_get_name',\n '_is_full_backward_hook',\n '_load_from_state_dict',\n '_load_state_dict_post_hooks',\n '_load_state_dict_pre_hooks',\n '_maybe_warn_non_full_backward_hook',\n '_modules',\n '_named_members',\n '_non_persistent_buffers_set',\n '_parameters',\n '_register_load_state_dict_pre_hook',\n '_register_state_dict_hook',\n '_replicate_for_data_parallel',\n '_save_to_state_dict',\n '_slow_forward',\n '_state_dict_hooks',\n '_version',\n 'add_module',\n 'apply',\n 'bfloat16',\n 'bias',\n 'buffers',\n 'children',\n 'cpu',\n 'cuda',\n 'double',\n 'dump_patches',\n 'eval',\n 'extra_repr',\n 'float',\n 'forward',\n 'get_buffer',\n 'get_extra_state',\n 'get_parameter',\n 'get_submodule',\n 'half',\n 'in_features',\n 'ipu',\n 'load_state_dict',\n 'modules',\n 'named_buffers',\n 'named_children',\n 'named_modules',\n 'named_parameters',\n 'out_features',\n 'parameters',\n 'register_backward_hook',\n 'register_buffer',\n 'register_forward_hook',\n 'register_forward_pre_hook',\n 'register_full_backward_hook',\n 'register_load_state_dict_post_hook',\n 'register_module',\n 'register_parameter',\n 'requires_grad_',\n 'reset_parameters',\n 'set_extra_state',\n 'share_memory',\n 'state_dict',\n 'to',\n 'to_empty',\n 'train',\n 'training',\n 'type',\n 'weight',\n 'xpu',\n 'zero_grad']"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(m)  # 可通过m.weight和m.bias设置该线性层的权值和偏置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. [sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)\n",
    "\n",
    "A sequential container.\n",
    "Modules will be added to it in the order they are passed in the constructor. Alternatively, an `OrderedDict` of modules can be passed in. The `forward()` method of `Sequential` accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\n",
    "\n",
    "The value a `Sequential` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the `Sequential` applies to each of the modules it stores (which are each a registered submodule of the `Sequential`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "from torch.nn import Flatten, Linear\n",
    "from collections import OrderedDict\n",
    "\n",
    "\"\"\"\n",
    "torch.nn.Sequential(*args: Module)\n",
    "\n",
    "torch.nn.Sequential(arg: OrderedDict[str, Module])\n",
    "\"\"\"\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,padding=2),\n",
    "            MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,padding=2),\n",
    "            MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024,64),\n",
    "            Linear(64,10)\n",
    "        )\n",
    "\n",
    "writer=SummaryWriter('./runsLog/sequential')\n",
    "\n",
    "# get some random training images\n",
    "# 注意这种写法\n",
    "dataiter=iter(dataloader)\n",
    "images,labels=next(dataiter)\n",
    "\n",
    "\"\"\"\n",
    "visualize complex model structures.\n",
    "\"\"\"\n",
    "writer.add_graph(model,images)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 查看效果\n",
    "!tensorboard --logdir= /home/zhang/PycharmProjects/d2l/zhang/chap0/pytorch/sequential"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
