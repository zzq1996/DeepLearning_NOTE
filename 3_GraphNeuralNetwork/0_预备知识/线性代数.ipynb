{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 第1部分：四个基本子空间（The Four Fundamental Subspaces）\n",
    "\n",
    "### 背景介绍\n",
    "\n",
    "给定一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，它表示一个线性变换：\n",
    "\n",
    "$$\n",
    "A: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\n",
    "$$\n",
    "\n",
    "线性代数中有四个非常重要的子空间构成矩阵 $A$ 的**结构核心**，分别是：\n",
    "\n",
    "| 子空间名称         | 记号              | 所在空间           | 维度                   | 含义                                    |\n",
    "| ------------- | --------------- | -------------- | -------------------- | ------------------------------------- |\n",
    "| 列空间（列子空间）     | $\\text{Col}(A)$ | $\\mathbb{R}^m$ | $\\text{rank}(A)$     | $A$ 所有列向量的线性组合空间                      |\n",
    "| 空间核（零空间）      | $\\text{N}(A)$   | $\\mathbb{R}^n$ | $n - \\text{rank}(A)$ | 解方程 $Ax = 0$ 的所有解构成的空间                |\n",
    "| 行空间（行子空间）     | $\\text{Row}(A)$ | $\\mathbb{R}^n$ | $\\text{rank}(A)$     | $A$ 所有行向量的线性组合空间                      |\n",
    "| 左零空间（零空间的正交补） | $\\text{N}(A^T)$ | $\\mathbb{R}^m$ | $m - \\text{rank}(A)$ | 解 $A^Ty = 0$ 的所有 $y \\in \\mathbb{R}^m$ |\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 列空间 $\\text{Col}(A)$\n",
    "\n",
    "**定义：**\n",
    "\n",
    "$$\n",
    "\\text{Col}(A) = \\text{span}\\{ a_1, a_2, ..., a_n \\},\\quad a_i 为 A 的第 i 列\n",
    "$$\n",
    "\n",
    "**含义：** 所有线性组合 $Ax$ 的结果都在这个空间中，因此：\n",
    "\n",
    "* **是线性变换 $A$ 的像（Image）**\n",
    "* **列空间维度 = 秩 rank(A)**\n",
    "\n",
    "**几何解释：**\n",
    "如果 $A \\in \\mathbb{R}^{3 \\times 2}$，那么列空间是 3D 空间中的一个平面（二维子空间）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 空间核 $\\text{N}(A)$\n",
    "\n",
    "**定义：**\n",
    "\n",
    "$$\n",
    "\\text{N}(A) = \\{ x \\in \\mathbb{R}^n \\mid Ax = 0 \\}\n",
    "$$\n",
    "\n",
    "**含义：** 所有被映射到零向量的输入向量 $x$，即线性系统的解空间。\n",
    "\n",
    "**维度：**\n",
    "\n",
    "$$\n",
    "\\dim \\text{N}(A) = n - \\text{rank}(A)\n",
    "$$\n",
    "\n",
    "**几何解释：** 如果 $A \\in \\mathbb{R}^{3 \\times 3}$，秩为 2，那么核空间是三维空间中的一条直线（1D子空间）。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 行空间 $\\text{Row}(A)$\n",
    "\n",
    "**定义：**\n",
    "\n",
    "$$\n",
    "\\text{Row}(A) = \\text{span of the rows of } A\n",
    "$$\n",
    "\n",
    "**性质：**\n",
    "\n",
    "* 和列空间一样，其维度 = $\\text{rank}(A)$\n",
    "* 实际上是 $A^T$ 的列空间：$\\text{Row}(A) = \\text{Col}(A^T)$\n",
    "\n",
    "**几何意义：**\n",
    "\n",
    "* 表示线性方程组中“约束”的方向\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 左零空间 $\\text{N}(A^T)$\n",
    "\n",
    "**定义：**\n",
    "\n",
    "$$\n",
    "\\text{N}(A^T) = \\{ y \\in \\mathbb{R}^m \\mid A^Ty = 0 \\}\n",
    "$$\n",
    "\n",
    "**含义：** 左乘 $A$ 时消去的方向（正交于列空间）\n",
    "\n",
    "**维度：**\n",
    "\n",
    "$$\n",
    "\\dim \\text{N}(A^T) = m - \\text{rank}(A)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 四个子空间之间的正交关系\n",
    "\n",
    "对于 $A \\in \\mathbb{R}^{m \\times n}$：\n",
    "\n",
    "* $\\text{N}(A) \\perp \\text{Row}(A)$\n",
    "* $\\text{N}(A^T) \\perp \\text{Col}(A)$\n",
    "\n",
    "也就是说：\n",
    "\n",
    "$$\n",
    "\\mathbb{R}^n = \\text{Row}(A) \\oplus \\text{N}(A) \\\\\n",
    "\\mathbb{R}^m = \\text{Col}(A) \\oplus \\text{N}(A^T)\n",
    "$$\n",
    "\n",
    "这个“直和”分解让我们可以从代数结构中明确看出整个空间被“分块”。\n",
    "\n",
    "---\n",
    "\n",
    "### 举例分析\n",
    "\n",
    "考虑：\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{2 \\times 3}\n",
    "$$\n",
    "\n",
    "* 列空间是 $\\mathbb{R}^2$ 中的二维子空间（秩为2）\n",
    "* 空间核是 $\\mathbb{R}^3$ 中的一维空间（因为 $n - \\text{rank}(A) = 1$）\n",
    "* 行空间是 $\\mathbb{R}^3$ 中秩为2的平面\n",
    "* 左零空间是 $\\mathbb{R}^2$ 中的零空间（因为秩已满，左零空间为 0）\n",
    "\n",
    "---\n",
    "\n",
    "### 应用场景\n",
    "\n",
    "* **机器学习** 中维度压缩、降维（PCA中投影在主子空间）\n",
    "* **最小二乘拟合**中，误差垂直于列空间\n",
    "* **神经网络中矩阵秩**影响模型表达能力\n",
    "* **图神经网络中拉普拉斯矩阵的零空间**对应图的连通成分\n",
    "\n",
    "---\n",
    "\n",
    "### 小结表格\n",
    "\n",
    "| 子空间        | 空间所在           | 基本定义                  | 维度                   |\n",
    "| ---------- | -------------- | --------------------- | -------------------- |\n",
    "| 列空间 Col(A) | $\\mathbb{R}^m$ | 所有列向量组合 $Ax$          | $\\text{rank}(A)$     |\n",
    "| 空间核 N(A)   | $\\mathbb{R}^n$ | 满足 $Ax = 0$ 的所有 $x$   | $n - \\text{rank}(A)$ |\n",
    "| 行空间 Row(A) | $\\mathbb{R}^n$ | 所有行向量组合               | $\\text{rank}(A)$     |\n",
    "| 左零空间 N(Aᵗ) | $\\mathbb{R}^m$ | 满足 $A^Ty = 0$ 的所有 $y$ | $m - \\text{rank}(A)$ |\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "745e770bba33e9d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 第2部分：矩阵空间、秩 / 矩阵与小世界图\n",
    "\n",
    "---\n",
    "\n",
    "### 第一节：矩阵空间与秩的核心理解\n",
    "\n",
    "#### 1. 什么是矩阵空间？\n",
    "\n",
    "矩阵空间是指**由所有指定维度的矩阵组成的向量空间**，例如：\n",
    "\n",
    "* 所有 $m \\times n$ 实矩阵构成的空间，记作 $\\mathbb{R}^{m \\times n}$\n",
    "* 可以在这个空间内定义加法、数乘等运算\n",
    "\n",
    "**常见子空间：**\n",
    "\n",
    "* 所有对称矩阵构成的子空间\n",
    "* 所有对角矩阵、稀疏矩阵、上三角矩阵等\n",
    "\n",
    "#### 向量空间公理验证（以 $\\mathbb{R}^{m \\times n}$ 为例）：\n",
    "\n",
    "* 闭合性（加法和数乘）\n",
    "* 交换律与结合律\n",
    "* 零矩阵作为加法单位元\n",
    "* 存在负元\n",
    "* 数乘的分配律、结合律等\n",
    "\n",
    "> 所有 $m \\times n$ 矩阵在加法和标量乘法下，构成了一个 $mn$ 维的向量空间。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 秩（Rank）的几何与代数意义\n",
    "\n",
    "**定义：**\n",
    "\n",
    "$$\n",
    "\\text{rank}(A) = \\text{矩阵 A 的最大线性无关列数（列秩） = 最大线性无关行数（行秩）}\n",
    "$$\n",
    "\n",
    "**直觉理解：**\n",
    "\n",
    "* 一个矩阵能把输入向量空间“挤压”成多小的维度？这就是秩。\n",
    "* Rank 表示矩阵的“信息量”或“维度压缩能力”。\n",
    "\n",
    "**代数性质：**\n",
    "\n",
    "* $\\text{rank}(A) = \\dim \\text{Col}(A) = \\dim \\text{Row}(A)$\n",
    "* $\\text{rank}(A) + \\dim \\text{N}(A) = n$\n",
    "* $\\text{rank}(A) + \\dim \\text{N}(A^T) = m$\n",
    "\n",
    "**几何解释（可视化）**：\n",
    "\n",
    "* $A \\in \\mathbb{R}^{3 \\times 3}$ 且秩为 1：把空间压缩到一条直线\n",
    "* 秩为 2：压缩成一个平面\n",
    "* 秩为 3：满秩，不压缩\n",
    "\n",
    "**计算方式：**\n",
    "\n",
    "* 高斯消元后保留下来的非零行数即为秩\n",
    "* 也可以通过 SVD 中非零奇异值的个数判断\n",
    "\n",
    "---\n",
    "\n",
    "### 第二节：矩阵与图的联系（以小世界图为例）\n",
    "\n",
    "图神经网络中的很多概念依赖图的矩阵表示，本节讲解图结构（尤其是小世界图）与矩阵的关系。\n",
    "\n",
    "#### 图的矩阵表示（Graph Matrices）\n",
    "\n",
    "给定一个无向图 $G = (V, E)$，其中有 $n$ 个节点，常见的矩阵表示包括：\n",
    "\n",
    "1. **邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$**\n",
    "\n",
    "$$\n",
    "A_{ij} =\n",
    "\\begin{cases}\n",
    "1, & \\text{if there is an edge between node } i \\text{ and } j \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "2. **度矩阵 $D$**（对角矩阵）\n",
    "\n",
    "$$\n",
    "D_{ii} = \\sum_j A_{ij}\n",
    "$$\n",
    "\n",
    "3. **拉普拉斯矩阵 $L = D - A$**\n",
    "\n",
    "* 用于捕捉图中节点的连接与“变动阻力”\n",
    "* 连接谱图理论与信号处理\n",
    "\n",
    "---\n",
    "\n",
    "#### 小世界图（Small-World Networks）\n",
    "\n",
    "**背景：** 小世界网络（如社交网络、蛋白质互作网络）同时具有：\n",
    "\n",
    "* **高聚类系数**（邻居之间也互相连接）\n",
    "* **小平均路径长度**（任意两点之间路径较短）\n",
    "\n",
    "**最著名模型：Watts-Strogatz Model**\n",
    "\n",
    "* 从规则环状图出发（每个点连接 $k$ 个近邻）\n",
    "* 以概率 $p$ 随机重连边，形成跳跃边（shortcuts）\n",
    "\n",
    "---\n",
    "\n",
    "#### 小世界图的矩阵特征\n",
    "\n",
    "1. **邻接矩阵结构稀疏但具有块状聚集性**\n",
    "\n",
    "   * 非零元素集中在对角线附近（局部连接）\n",
    "   * 随机重连带来远离对角线的“跳跃边”\n",
    "\n",
    "2. **拉普拉斯矩阵谱分布具有特殊性质**\n",
    "\n",
    "   * 第二小特征值（Fiedler value）小，意味着图结构紧凑但分块明显\n",
    "\n",
    "3. **奇异值 / 特征值降得慢**\n",
    "\n",
    "   * 表示小世界图的复杂性和信号可压缩性较低（重要用于图信号处理和降噪）\n",
    "\n",
    "---\n",
    "\n",
    "#### 小世界图与矩阵分析的实际应用\n",
    "\n",
    "| 应用场景         | 所用矩阵             | 分析目的                    |\n",
    "| ------------ | ---------------- | ----------------------- |\n",
    "| 社交网络聚类       | 邻接矩阵、拉普拉斯矩阵      | 社区发现（Cluster Detection） |\n",
    "| 图神经网络传播建模    | 度矩阵归一化传播         | 控制信息流扩散                 |\n",
    "| 图信号降噪        | 图傅里叶变换（基于L的特征向量） | 滤波图中高频噪声                |\n",
    "| 生物网络分析（如PPI） | 邻接矩阵             | 分析功能模块的联通性              |\n",
    "\n",
    "---\n",
    "\n",
    "### 小结\n",
    "\n",
    "| 概念                             | 含义                | 与图神经网络关系        |\n",
    "| ------------------------------ | ----------------- | --------------- |\n",
    "| 矩阵空间 $\\mathbb{R}^{m \\times n}$ | 所有 m 行 n 列矩阵构成的空间 | 描述输入特征与邻接结构转换关系 |\n",
    "| 秩 Rank                         | 最大线性无关行/列数        | 模型表达能力上限，降维基础   |\n",
    "| 小世界图                           | 高聚类 + 小路径         | 现实网络建模、GNN卷积基础  |\n",
    "| 邻接矩阵 $A$                       | 描述图结构             | GNN中用于信息传播      |\n",
    "| 拉普拉斯矩阵 $L$                     | $D - A$           | 图频谱分析、图平滑、GFT   |\n",
    "\n",
    "\n"
   ],
   "id": "e5e7ad9c83cb43c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 第3部分：正交向量和正交子空间（Orthogonal Vectors and Subspaces）\n",
    "\n",
    "---\n",
    "\n",
    "### 一、什么是正交（Orthogonality）\n",
    "\n",
    "#### 1. 正交的定义\n",
    "\n",
    "给定两个向量 $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n$，如果它们的**内积为 0**，我们称它们**正交**（perpendicular）：\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i = 0\n",
    "$$\n",
    "\n",
    "或者：\n",
    "\n",
    "$$\n",
    "\\mathbf{u}^T \\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 几何直觉\n",
    "\n",
    "* 内积为 0 ⇨ 两向量垂直（夹角为 90°）\n",
    "* 单位正交（orthonormal） ⇨ 彼此正交且模长为 1\n",
    "\n",
    "---\n",
    "\n",
    "### 二、正交向量的性质\n",
    "\n",
    "| 性质       | 内容               |\n",
    "| -------- | ---------------- |\n",
    "| 向量组正交    | 向量之间两两正交         |\n",
    "| 正交组线性无关  | 任意一组非零正交向量一定线性无关 |\n",
    "| 正交向量易于表示 | 在正交基下，投影与分解更简单   |\n",
    "| 正交矩阵方便运算 | 正交矩阵的逆等于转置       |\n",
    "\n",
    "---\n",
    "\n",
    "### 三、正交子空间（Orthogonal Subspaces）\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 正交子空间的定义\n",
    "\n",
    "若两个子空间 $S \\subseteq \\mathbb{R}^n$、$T \\subseteq \\mathbb{R}^n$，满足：\n",
    "\n",
    "$$\n",
    "\\forall \\mathbf{s} \\in S,\\ \\forall \\mathbf{t} \\in T,\\quad \\mathbf{s}^T \\mathbf{t} = 0\n",
    "$$\n",
    "\n",
    "则称 $S$ 与 $T$ 正交，记作：\n",
    "\n",
    "$$\n",
    "S \\perp T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 核与行空间的正交关系\n",
    "\n",
    "对于任意矩阵 $A \\in \\mathbb{R}^{m \\times n}$：\n",
    "\n",
    "* 核空间 $\\text{N}(A)$ 与行空间 $\\text{Row}(A)$ 正交：\n",
    "\n",
    "$$\n",
    "\\text{N}(A) \\perp \\text{Row}(A)\n",
    "$$\n",
    "\n",
    "* 左零空间 $\\text{N}(A^T)$ 与列空间 $\\text{Col}(A)$ 正交：\n",
    "\n",
    "$$\n",
    "\\text{N}(A^T) \\perp \\text{Col}(A)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 正交补（Orthogonal Complement）\n",
    "\n",
    "给定子空间 $S \\subseteq \\mathbb{R}^n$，其正交补为：\n",
    "\n",
    "$$\n",
    "S^\\perp = \\{ \\mathbf{x} \\in \\mathbb{R}^n \\mid \\forall \\mathbf{s} \\in S,\\ \\mathbf{x}^T \\mathbf{s} = 0 \\}\n",
    "$$\n",
    "\n",
    "**性质：**\n",
    "\n",
    "* $\\dim(S) + \\dim(S^\\perp) = n$\n",
    "* $\\mathbb{R}^n = S \\oplus S^\\perp$\n",
    "\n",
    "这就是正交分解空间的基础，如后续的**最小二乘法**与**投影**会大量用到。\n",
    "\n",
    "---\n",
    "\n",
    "### 四、正交投影与向量分解（预备）\n",
    "\n",
    "若 $\\mathbf{v} \\in \\mathbb{R}^n$，$\\mathbf{u} \\in \\mathbb{R}^n$ 是单位向量，则：\n",
    "\n",
    "* $\\mathbf{v}$ 在 $\\mathbf{u}$ 方向上的投影为：\n",
    "\n",
    "$$\n",
    "\\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = (\\mathbf{v}^T \\mathbf{u}) \\mathbf{u}\n",
    "$$\n",
    "\n",
    "* $\\mathbf{v}$ 可以正交分解为：\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\text{proj}_{\\mathbf{u}}(\\mathbf{v}) + \\text{remainder}\n",
    "$$\n",
    "\n",
    "> 这一分解是后续“最小二乘”、“Gram-Schmidt 正交化”、“QR 分解”的基础。\n",
    "\n",
    "---\n",
    "\n",
    "### 五、实例讲解：正交空间之间的构造与关系\n",
    "\n",
    "考虑矩阵：\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}\n",
    "$$\n",
    "\n",
    "* $\\text{Col}(A) \\subset \\mathbb{R}^3$：列空间为二维平面\n",
    "* $\\text{N}(A^T) \\subset \\mathbb{R}^3$：维度为 $3 - 2 = 1$，即一条直线\n",
    "* $\\text{Col}(A) \\perp \\text{N}(A^T)$\n",
    "\n",
    "可通过求解 $A^T y = 0$ 得到左零空间方向，验证其与 $A$ 的列空间正交。\n",
    "\n",
    "---\n",
    "\n",
    "### 六、正交在图神经网络中的应用\n",
    "\n",
    "图神经网络中“信息的传播”和“噪声的滤除”都强烈依赖正交性概念。\n",
    "\n",
    "| 应用方向      | 正交相关内容        | 解释                |\n",
    "| --------- | ------------- | ----------------- |\n",
    "| 图卷积中的频谱滤波 | 拉普拉斯矩阵的特征向量正交 | 实现图信号变换           |\n",
    "| 图嵌入与表示学习  | 正交基分解图结构      | 保持低维嵌入间的正交性可提高区分性 |\n",
    "| 图节点特征去冗余  | 基于正交约束的解耦机制   | 避免特征共线性，提升表达能力    |\n",
    "| 图对比学习     | 增加正负样本的正交性    | 提高判别性与鲁棒性         |\n",
    "\n",
    "---\n",
    "\n",
    "### 小结表格\n",
    "\n",
    "| 概念    | 定义                | 几何意义 | 应用场景      |\n",
    "| ----- | ----------------- | ---- | --------- |\n",
    "| 向量正交  | 内积为0              | 垂直   | 投影、分解     |\n",
    "| 子空间正交 | 所有向量两两正交          | 正交补  | 最小二乘、QR分解 |\n",
    "| 正交组   | 多个互相正交的非零向量组      | 易分解  | 正交矩阵构造    |\n",
    "| 正交补   | 所有与子空间正交的向量组成的子空间 | 空间直和 | 解空间分解     |\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1ab714571daa1cc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 📘 第4部分：投影、投影矩阵和最小二乘法（Projection, Projection Matrices & Least Squares）\n",
    "\n",
    "---\n",
    "\n",
    "###  一、投影的几何直觉\n",
    "\n",
    "####  定义\n",
    "\n",
    "给定一个向量 $\\mathbf{b} \\in \\mathbb{R}^n$，将其**投影**到子空间 $S$ 上，就是在 $S$ 中找到一个向量 $\\mathbf{p} \\in S$，使得：\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\arg\\min_{\\mathbf{x} \\in S} \\|\\mathbf{b} - \\mathbf{x}\\|_2\n",
    "$$\n",
    "\n",
    "也就是说，**投影是使残差最小的近似解**。\n",
    "\n",
    "---\n",
    "\n",
    "###  二、投影到一个向量（直线）上\n",
    "\n",
    "#### ⭐ 投影公式（单位向量）\n",
    "\n",
    "若 $\\mathbf{u} \\in \\mathbb{R}^n$ 是单位向量，则：\n",
    "\n",
    "$$\n",
    "\\text{proj}_{\\mathbf{u}}(\\mathbf{b}) = (\\mathbf{b}^T \\mathbf{u}) \\mathbf{u}\n",
    "$$\n",
    "\n",
    "若 $\\mathbf{u}$ 非单位向量，投影为：\n",
    "\n",
    "$$\n",
    "\\text{proj}_{\\mathbf{u}}(\\mathbf{b}) = \\frac{\\mathbf{b}^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} \\cdot \\mathbf{u}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  投影矩阵（一个向量）\n",
    "\n",
    "我们可以写出一个“投影操作”的矩阵表示：\n",
    "\n",
    "设 $\\mathbf{u} \\in \\mathbb{R}^n$，则：\n",
    "\n",
    "$$\n",
    "P = \\frac{\\mathbf{u} \\mathbf{u}^T}{\\mathbf{u}^T \\mathbf{u}} \\quad \\text{是一个 } n \\times n \\text{ 投影矩阵}\n",
    "$$\n",
    "\n",
    "有：\n",
    "\n",
    "$$\n",
    "P \\mathbf{b} = \\text{投影后的向量}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  三、投影到子空间（多个正交基）\n",
    "\n",
    "若 $S \\subseteq \\mathbb{R}^n$ 是一个 $k$-维子空间，由 $\\mathbf{u}_1, \\dots, \\mathbf{u}_k$ 正交向量张成（列向量组成矩阵 $U \\in \\mathbb{R}^{n \\times k}$），则：\n",
    "\n",
    "$$\n",
    "P = U U^T\n",
    "$$\n",
    "\n",
    "* $P$ 是正交投影矩阵\n",
    "* $P^2 = P$，$P^T = P$（投影矩阵是对称幂等的）\n",
    "\n",
    "---\n",
    "\n",
    "###  四、投影到一般子空间：最小二乘法（Least Squares）\n",
    "\n",
    "现在考虑一个方程组：\n",
    "\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "* 若 $A \\in \\mathbb{R}^{m \\times n}$ 且 $m > n$，则是**超定系统**\n",
    "* 通常**无精确解**\n",
    "* 目标是找到**最接近解** $\\hat{\\mathbf{x}}$，使：\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\arg\\min_{\\mathbf{x}} \\| A \\mathbf{x} - \\mathbf{b} \\|_2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  最小二乘法核心公式\n",
    "\n",
    "推导如下：\n",
    "\n",
    "目标函数为平方误差：\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x}} \\| A \\mathbf{x} - \\mathbf{b} \\|_2^2\n",
    "$$\n",
    "\n",
    "令梯度为 0：\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}} = 2 A^T (A \\mathbf{x} - \\mathbf{b}) = 0\n",
    "$$\n",
    "\n",
    "得到**法方程（Normal Equation）**：\n",
    "\n",
    "$$\n",
    "A^T A \\mathbf{x} = A^T \\mathbf{b}\n",
    "$$\n",
    "\n",
    "若 $A^T A$ 可逆，解为：\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = (A^T A)^{-1} A^T \\mathbf{b}\n",
    "$$\n",
    "\n",
    "这个解就是将 $\\mathbf{b}$ 投影到列空间 $\\text{Col}(A)$ 上，使残差 $\\mathbf{b} - A\\hat{\\mathbf{x}}$ 垂直于 $\\text{Col}(A)$。\n",
    "\n",
    "---\n",
    "\n",
    "####  残差向量的正交性质\n",
    "\n",
    "设：\n",
    "\n",
    "* $\\mathbf{p} = A\\hat{\\mathbf{x}}$：是 $\\mathbf{b}$ 在 $\\text{Col}(A)$ 上的投影\n",
    "* $\\mathbf{e} = \\mathbf{b} - \\mathbf{p}$：是误差\n",
    "\n",
    "则：\n",
    "\n",
    "$$\n",
    "A^T \\mathbf{e} = 0 \\Rightarrow \\mathbf{e} \\perp \\text{Col}(A)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  五、最小二乘的投影矩阵\n",
    "\n",
    "将投影操作记作矩阵：\n",
    "\n",
    "$$\n",
    "P = A (A^T A)^{-1} A^T\n",
    "$$\n",
    "\n",
    "则：\n",
    "\n",
    "* $P \\in \\mathbb{R}^{m \\times m}$ 是**投影矩阵**\n",
    "* $P \\mathbf{b} = A \\hat{\\mathbf{x}}$ 是 $\\mathbf{b}$ 在列空间上的投影\n",
    "* $P^2 = P$, $P^T = P$\n",
    "\n",
    "---\n",
    "\n",
    "###  六、投影矩阵性质总结\n",
    "\n",
    "| 性质            | 说明                                         |\n",
    "| ------------- | ------------------------------------------ |\n",
    "| 幂等性 $P^2 = P$ | 多次投影等于一次投影                                 |\n",
    "| 对称性 $P^T = P$ | 自伴（Symmetric）                              |\n",
    "| 特征值只有 0 和 1   | 1 对应于投影空间，0 对应于正交补                         |\n",
    "| 残差正交于列空间      | $A^T (\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$ |\n",
    "\n",
    "---\n",
    "\n",
    "###  七、应用场景举例\n",
    "\n",
    "| 场景       | 使用内容                  | 目的           |\n",
    "| -------- | --------------------- | ------------ |\n",
    "| 线性回归     | 最小二乘求参数 $\\hat{\\beta}$ | 拟合数据         |\n",
    "| 信号降噪     | 投影到主成分方向              | 消除噪声         |\n",
    "| 图嵌入      | 将节点特征投影到低维空间          | 表征学习         |\n",
    "| 图神经网络    | 控制信息传播方向              | 避免过平滑、冗余信息积累 |\n",
    "| 神经网络参数压缩 | 低秩投影保留重要方向            | 模型压缩、加速推理    |\n",
    "\n",
    "---\n",
    "\n",
    "###  小结\n",
    "\n",
    "| 概念                         | 说明                       |\n",
    "| -------------------------- | ------------------------ |\n",
    "| 投影                         | 将向量逼近到子空间上               |\n",
    "| 最小二乘解                      | 使 $A x \\approx b$ 的最佳近似解 |\n",
    "| 投影矩阵 $P = A(A^TA)^{-1}A^T$ | 用于计算投影向量                 |\n",
    "| 残差 $e = b - Ax$ 正交于列空间     | 解的几何解释                   |\n",
    "\n",
    "---\n",
    "\n",
    "###  练习题（推荐自测）\n",
    "\n",
    "1. 给出 $A = \\begin{bmatrix}1 & 1 \\\\ 1 & -1\\end{bmatrix}$，计算：\n",
    "\n",
    "   * $P = A(A^TA)^{-1}A^T$\n",
    "   * 对任意 $b \\in \\mathbb{R}^2$，计算 $Pb$\n",
    "2. 用 numpy 代码实现：\n",
    "\n",
    "   * 最小二乘解\n",
    "   * 投影矩阵可视化\n",
    "3. 证明投影矩阵是对称且幂等的\n",
    "\n",
    "\n"
   ],
   "id": "2025ae1f3c1ccc77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "##  第5部分：正交矩阵与正交化方法\n",
    "\n",
    "---\n",
    "\n",
    "###  一、什么是正交矩阵（Orthogonal Matrix）\n",
    "\n",
    "####  定义\n",
    "\n",
    "一个**方阵** $Q \\in \\mathbb{R}^{n \\times n}$ 是**正交矩阵**，当它的列（或行）向量组成一个**标准正交基**，即：\n",
    "\n",
    "$$\n",
    "Q^T Q = I \\quad \\text{或} \\quad Q Q^T = I\n",
    "$$\n",
    "\n",
    "换句话说：\n",
    "\n",
    "$$\n",
    "Q^{-1} = Q^T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  正交矩阵的性质\n",
    "\n",
    "| 性质          | 说明                                                    |\n",
    "| ----------- | ----------------------------------------------------- |\n",
    "| 保长度         | $\\| Qx \\| = \\| x \\|$（长度不变）                            |\n",
    "| 保角度         | $\\langle Qx, Qy \\rangle = \\langle x, y \\rangle$（内积不变） |\n",
    "| 保正交         | $x \\perp y \\Rightarrow Qx \\perp Qy$                   |\n",
    "| 行列式为 ±1     | $\\det(Q) = \\pm 1$                                     |\n",
    "| 对称正交矩阵是反射矩阵 | $Q^T = Q$, $Q^2 = I$                                  |\n",
    "\n",
    "---\n",
    "\n",
    "###  二、正交矩阵的几何意义\n",
    "\n",
    "* 正交矩阵是**刚体变换**：旋转 + 反射，不会拉伸或压缩空间\n",
    "* **应用广泛**：图像旋转、信号旋转、空间变换、稳定数值运算\n",
    "\n",
    "---\n",
    "\n",
    "###  三、如何将一组向量正交化？——Gram-Schmidt 正交化过程\n",
    "\n",
    "---\n",
    "\n",
    "####  问题背景\n",
    "\n",
    "给定线性无关向量组 $\\{ a_1, a_2, \\dots, a_n \\}$，如何构造一个**正交（甚至标准正交）向量组** $\\{ q_1, q_2, \\dots, q_n \\}$？\n",
    "\n",
    "---\n",
    "\n",
    "####  Gram-Schmidt 算法步骤\n",
    "\n",
    "1. 令：\n",
    "\n",
    "$$\n",
    "q_1 = \\frac{a_1}{\\|a_1\\|}\n",
    "$$\n",
    "\n",
    "2. 对于 $k = 2$ 到 $n$，依次计算：\n",
    "\n",
    "   * 从原始向量 $a_k$ 中减去它在之前每个 $q_j$ 上的投影：\n",
    "\n",
    "$$\n",
    "\\tilde{q}_k = a_k - \\sum_{j=1}^{k-1} \\text{proj}_{q_j}(a_k)\n",
    "= a_k - \\sum_{j=1}^{k-1} (a_k^T q_j) q_j\n",
    "$$\n",
    "\n",
    "* 标准化：\n",
    "\n",
    "$$\n",
    "q_k = \\frac{\\tilde{q}_k}{\\|\\tilde{q}_k\\|}\n",
    "$$\n",
    "\n",
    "最终得到正交（或标准正交）向量组。\n",
    "\n",
    "---\n",
    "\n",
    "####  Gram-Schmidt 特点\n",
    "\n",
    "| 特点     | 内容                           |\n",
    "| ------ | ---------------------------- |\n",
    "| 输入     | 一组线性无关向量                     |\n",
    "| 输出     | 一组正交（或标准正交）向量                |\n",
    "| 可数值不稳定 | 当向量接近线性相关时，误差积累              |\n",
    "| 改进方法   | Modified Gram-Schmidt（数值更稳定） |\n",
    "\n",
    "---\n",
    "\n",
    "###  四、QR 分解：将矩阵分解为正交×上三角\n",
    "\n",
    "---\n",
    "\n",
    "####  定义\n",
    "\n",
    "给定一个 $m \\times n$ 的矩阵 $A$，其中 $m \\geq n$，可以将其分解为：\n",
    "\n",
    "$$\n",
    "A = QR\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $Q \\in \\mathbb{R}^{m \\times n}$：列正交矩阵（即 $Q^T Q = I$）\n",
    "* $R \\in \\mathbb{R}^{n \\times n}$：上三角矩阵\n",
    "\n",
    "---\n",
    "\n",
    "####  QR 分解的步骤（基于 Gram-Schmidt）\n",
    "\n",
    "1. 对 $A$ 的列向量 $a_1, \\dots, a_n$ 应用 Gram-Schmidt，得到 $q_1, \\dots, q_n$\n",
    "2. 将这些列向量组装为 $Q$\n",
    "3. 计算：\n",
    "\n",
    "$$\n",
    "R_{ij} = q_i^T a_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  QR 分解的几何理解\n",
    "\n",
    "* $Q$：提供正交基底，定义子空间方向\n",
    "* $R$：提供在这些方向上的坐标（投影系数）\n",
    "\n",
    "---\n",
    "\n",
    "####  QR 分解的应用\n",
    "\n",
    "| 应用      | 内容                              |\n",
    "| ------- | ------------------------------- |\n",
    "| 解最小二乘问题 | 利用 $A = QR$，把问题变成求解 $Rx = Q^Tb$ |\n",
    "| 计算特征值   | QR 迭代法                          |\n",
    "| 正交投影    | $QQ^T$ 即为投影矩阵                   |\n",
    "| 数值稳定性强  | 不用求逆，适合大规模矩阵                    |\n",
    "\n",
    "---\n",
    "\n",
    "###  五、QR 与最小二乘法的联系\n",
    "\n",
    "若我们要解：\n",
    "\n",
    "$$\n",
    "A x = b,\\quad A = QR\n",
    "$$\n",
    "\n",
    "则：\n",
    "\n",
    "$$\n",
    "Rx = Q^T b \\Rightarrow x = R^{-1} Q^T b\n",
    "$$\n",
    "\n",
    "* 避免计算 $A^T A$\n",
    "* 更适合数值实现（尤其在机器学习中处理大矩阵）\n",
    "\n",
    "---\n",
    "\n",
    "###  六、图神经网络中的正交矩阵应用\n",
    "\n",
    "| 场景    | 正交性作用                           |\n",
    "| ----- | ------------------------------- |\n",
    "| 图卷积核  | 保持传播中信号能量不变                     |\n",
    "| 特征提取  | 图嵌入空间中正交方向表示不同语义                |\n",
    "| 表征学习  | 增强表达多样性，减少冗余特征                  |\n",
    "| 图对比学习 | 引入正交正则项强化对比目标（如 BGRL、SimGCL）    |\n",
    "| 参数学习  | 保持变换矩阵的正交性防止过拟合（Orthogonal GCN） |\n",
    "\n",
    "---\n",
    "\n",
    "### 小结\n",
    "\n",
    "| 概念           | 定义 / 意义                 |\n",
    "| ------------ | ----------------------- |\n",
    "| 正交矩阵         | $Q^T Q = I$，长度角度不变，变换稳定 |\n",
    "| Gram-Schmidt | 从任意线性无关向量组构造正交向量组       |\n",
    "| QR 分解        | $A = QR$，将矩阵分解为方向 × 坐标  |\n",
    "| 投影矩阵         | $QQ^T$，最小二乘与信息压缩基础      |\n",
    "| 正交约束         | 保持学习表示之间的“独立性”          |\n",
    "\n",
    "---\n",
    "\n",
    "###  补充练习（推荐自测）\n",
    "\n",
    "1. 对向量组 $a_1 = [1, 1, 0]^T,\\ a_2 = [1, 0, 1]^T$，使用 Gram-Schmidt 构造正交组\n",
    "2. 对 $A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$，手工做 QR 分解\n",
    "3. 实现 QR 分解的 Python 代码，并验证 $Q^T Q = I$\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2445b18766a300b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "##  第6部分：特征值与特征向量（Eigenvalues & Eigenvectors）\n",
    "\n",
    "---\n",
    "\n",
    "###  一、基本定义与直觉理解\n",
    "\n",
    "####  定义\n",
    "\n",
    "设 $A \\in \\mathbb{R}^{n \\times n}$，若存在非零向量 $\\mathbf{v} \\in \\mathbb{R}^n$ 和标量 $\\lambda \\in \\mathbb{R}$，使得：\n",
    "\n",
    "$$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "则称：\n",
    "\n",
    "* $\\lambda$ 是 $A$ 的**特征值（eigenvalue）**\n",
    "* $\\mathbf{v}$ 是对应的**特征向量（eigenvector）**\n",
    "\n",
    "> 特征向量是在线性变换 $A$ 下，只发生“拉伸”或“压缩”而不改变方向的向量。\n",
    "\n",
    "---\n",
    "\n",
    "####  几何直觉\n",
    "\n",
    "* 一般线性变换会改变向量方向和大小\n",
    "* 只有**特征向量**在作用下方向不变，仅被按特征值比例缩放\n",
    "\n",
    "例如，旋转矩阵没有实数特征值；对称矩阵具有互相正交的实特征向量。\n",
    "\n",
    "---\n",
    "\n",
    "###  二、特征值与特征向量的求解方法\n",
    "\n",
    "---\n",
    "\n",
    "####  求解步骤\n",
    "\n",
    "1. 由定义：\n",
    "\n",
    "$$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v} \\Rightarrow (A - \\lambda I) \\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "2. 为了有非零解（即特征向量存在）：\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "这就是**特征方程**，解出所有 $\\lambda$\n",
    "\n",
    "3. 对于每个 $\\lambda$，求解：\n",
    "\n",
    "$$\n",
    "(A - \\lambda I) \\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "即可得到对应特征向量\n",
    "\n",
    "---\n",
    "\n",
    "####  例子\n",
    "\n",
    "设：\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "1. 解特征值：\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = \\begin{vmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{vmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = 0\n",
    "\\Rightarrow \\lambda = 1,\\ 3\n",
    "$$\n",
    "\n",
    "2. 分别求出两个特征值下的特征向量即可。\n",
    "\n",
    "---\n",
    "\n",
    "###  三、特征空间、重数、对角化\n",
    "\n",
    "---\n",
    "\n",
    "####  特征空间\n",
    "\n",
    "对于每个特征值 $\\lambda$，所有对应的特征向量（加零向量）组成一个子空间，称为**特征空间**\n",
    "\n",
    "---\n",
    "\n",
    "####  重数的概念\n",
    "\n",
    "* **代数重数（algebraic multiplicity）**：特征值 $\\lambda$ 在特征方程中的重根次数\n",
    "* **几何重数（geometric multiplicity）**：特征空间的维数（即线性无关的特征向量个数）\n",
    "\n",
    "总有：\n",
    "\n",
    "$$\n",
    "\\text{几何重数} \\leq \\text{代数重数}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  可对角化条件\n",
    "\n",
    "若 $A \\in \\mathbb{R}^{n \\times n}$ 有 $n$ 个线性无关的特征向量，则可对角化：\n",
    "\n",
    "$$\n",
    "A = PDP^{-1}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $P$：由特征向量组成的矩阵\n",
    "* $D$：对角矩阵，对角线上是特征值\n",
    "\n",
    "---\n",
    "\n",
    "###  四、特征分解与对称矩阵\n",
    "\n",
    "####  对称矩阵的性质\n",
    "\n",
    "若 $A = A^T$，则：\n",
    "\n",
    "| 性质        | 内容                                                   |\n",
    "| --------- | ---------------------------------------------------- |\n",
    "| 所有特征值都是实数 | 不会出现复数特征值                                            |\n",
    "| 存在正交特征向量组 | 可构造正交矩阵对角化                                           |\n",
    "| 可正交对角化    | $A = Q \\Lambda Q^T$（类似于 $PDP^{-1}$，但 $Q^T = Q^{-1}$） |\n",
    "\n",
    "---\n",
    "\n",
    "###  五、特征值与矩阵性质关系\n",
    "\n",
    "| 矩阵类型               | 特征值含义                             |\n",
    "| ------------------ | --------------------------------- |\n",
    "| 对称矩阵               | 实数、正交特征向量                         |\n",
    "| 正定矩阵               | 所有特征值 > 0                         |\n",
    "| 奇异矩阵（不可逆）          | 存在特征值 0                           |\n",
    "| 正交矩阵               | 所有特征值模长为 1                        |\n",
    "| 拉普拉斯矩阵 $L = D - A$ | 半正定，最小特征值为 0，第二小值揭示连通性（Fiedler 值） |\n",
    "\n",
    "---\n",
    "\n",
    "###  六、特征分解在图神经网络中的应用\n",
    "\n",
    "图神经网络常使用的**图拉普拉斯矩阵**就是对称半正定矩阵，常用其**特征分解**进行**图傅里叶变换**、频谱滤波等操作。\n",
    "\n",
    "####  图中的特征值用途示例\n",
    "\n",
    "| 应用          | 说明                                    |\n",
    "| ----------- | ------------------------------------- |\n",
    "| 图傅里叶变换（GFT） | 用特征向量构建变换基                            |\n",
    "| 谱图卷积        | 滤波器作用于特征值上的谱域乘积                       |\n",
    "| 图嵌入学习       | 最小化高阶特征向量之间的距离（如 Laplacian Eigenmaps） |\n",
    "| 社区发现        | 基于特征向量（如 Fiedler vector）划分图的子结构       |\n",
    "\n",
    "---\n",
    "\n",
    "###  七、总结表格\n",
    "\n",
    "| 概念                | 定义 / 性质                                    |\n",
    "| ----------------- | ------------------------------------------ |\n",
    "| 特征值 $\\lambda$     | 满足 $A\\mathbf{v} = \\lambda\\mathbf{v}$ 的缩放因子 |\n",
    "| 特征向量 $\\mathbf{v}$ | 被 $A$ 缩放而不改变方向的非零向量                        |\n",
    "| 特征空间              | 每个特征值对应的所有向量组成的空间                          |\n",
    "| 可对角化              | 存在 $A = P D P^{-1}$，便于简化计算                 |\n",
    "| 对称矩阵分解            | $A = Q \\Lambda Q^T$，正交对角化                  |\n",
    "| 图中意义              | 特征值 = 频率，特征向量 = 模式                         |\n",
    "\n",
    "---\n",
    "\n",
    "###  自测练习建议\n",
    "\n",
    "1. 手工求解：\n",
    "\n",
    "   * $A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}$ 的特征值与特征向量\n",
    "2. 用 `numpy.linalg.eig` 编程验证你的结果\n",
    "3. 证明对称矩阵的特征向量正交\n",
    "4. 画出图拉普拉斯矩阵 $L$ 的特征值分布，对比不同图结构的谱特征（如链、环、小世界图）\n",
    "\n",
    "\n"
   ],
   "id": "bd83a601e3251230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "##  第7部分：对称矩阵与正定矩阵（Symmetric & Positive Definite Matrices）\n",
    "\n",
    "---\n",
    "\n",
    "###  一、对称矩阵（Symmetric Matrix）\n",
    "\n",
    "---\n",
    "\n",
    "####  定义\n",
    "\n",
    "一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是**对称矩阵**，当：\n",
    "\n",
    "$$\n",
    "A = A^T\n",
    "$$\n",
    "\n",
    "即矩阵关于主对角线对称。\n",
    "\n",
    "---\n",
    "\n",
    "####  对称矩阵的基本性质\n",
    "\n",
    "| 性质       | 说明                                              |\n",
    "| -------- | ----------------------------------------------- |\n",
    "| 所有特征值为实数 | 不会出现复数特征值                                       |\n",
    "| 特征向量可正交  | 可构造正交特征向量组                                      |\n",
    "| 可正交对角化   | 存在 $A = Q \\Lambda Q^T$，其中 $Q$ 为正交矩阵             |\n",
    "| 等价于自伴算子  | 对称矩阵 = 实自伴算子（Hermitian if complex）              |\n",
    "| 保内积结构    | $\\langle Ax, y \\rangle = \\langle x, Ay \\rangle$ |\n",
    "\n",
    "---\n",
    "\n",
    "####  几何意义\n",
    "\n",
    "* 对称矩阵代表的是“方向一致性”的线性变换。\n",
    "* 在高维空间中，作用于向量时不会扭曲方向，只在正交方向上缩放。\n",
    "\n",
    "---\n",
    "\n",
    "###  二、正定矩阵（Positive Definite Matrix）\n",
    "\n",
    "---\n",
    "\n",
    "####  定义\n",
    "\n",
    "一个对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是**正定矩阵**，当对任意非零向量 $x \\in \\mathbb{R}^n$，都有：\n",
    "\n",
    "$$\n",
    "x^T A x > 0\n",
    "$$\n",
    "\n",
    "如果满足 $x^T A x \\geq 0$，则是**半正定矩阵（positive semi-definite, PSD）**。\n",
    "\n",
    "---\n",
    "\n",
    "####  常见例子\n",
    "\n",
    "| 矩阵                  | 是否正定     | 理由                                    |\n",
    "| ------------------- | -------- | ------------------------------------- |\n",
    "| 单位矩阵 $I$            | 正定       | 任意 $x^T x > 0$                        |\n",
    "| 协方差矩阵               | 半正定      | 统计中常用，描述变量相关性                         |\n",
    "| $A^T A$             | 半正定 / 正定 | 若 $A$ 满秩则正定                           |\n",
    "| 图拉普拉斯矩阵 $L = D - A$ | 半正定      | $x^T L x = \\sum (x_i - x_j)^2 \\geq 0$ |\n",
    "\n",
    "---\n",
    "\n",
    "####  判断正定性的等价条件\n",
    "\n",
    "若 $A \\in \\mathbb{R}^{n \\times n}$ 是对称矩阵，以下条件等价于“正定”：\n",
    "\n",
    "| 条件                       | 说明           |\n",
    "| ------------------------ | ------------ |\n",
    "| 所有特征值 $\\lambda_i > 0$    | 代数判据         |\n",
    "| 所有主子式 > 0                | Sylvester 判据 |\n",
    "| 存在满秩 $R$ 使 $A = R^T R$   | Cholesky 分解  |\n",
    "| 任意非零 $x$，有 $x^T A x > 0$ | 函数型判据        |\n",
    "\n",
    "---\n",
    "\n",
    "###  三、正定矩阵与优化的关系\n",
    "\n",
    "---\n",
    "\n",
    "####  凸函数与 Hessian 矩阵\n",
    "\n",
    "设函数 $f(x)$ 可微，若其 Hessian 矩阵 $H(x) = \\nabla^2 f(x)$ 满足：\n",
    "\n",
    "* $H(x) \\succ 0$（正定） ⇒ 严格凸函数\n",
    "* $H(x) \\succeq 0$（半正定） ⇒ 凸函数\n",
    "\n",
    "> 这是凸优化理论的基础，用于判断最小值是否存在、是否唯一。\n",
    "\n",
    "---\n",
    "\n",
    "####  应用场景\n",
    "\n",
    "| 应用              | 正定性角色                         |\n",
    "| --------------- | ----------------------------- |\n",
    "| 最小二乘法中的 $A^T A$ | $A$ 满秩时为正定，确保解唯一              |\n",
    "| 机器学习模型损失函数      | 二阶导数正定 ⇒ 凸损失函数 ⇒ 易优化          |\n",
    "| 主成分分析（PCA）      | 协方差矩阵为半正定，特征值决定主成分方向          |\n",
    "| 图嵌入             | 拉普拉斯矩阵正半定，谱嵌入源自最小化 Rayleigh 商 |\n",
    "| 多变量高斯分布         | 协方差矩阵必须正定，否则无法构造密度函数          |\n",
    "\n",
    "---\n",
    "\n",
    "###  四、Cholesky 分解\n",
    "\n",
    "---\n",
    "\n",
    "####  定义\n",
    "\n",
    "若 $A \\in \\mathbb{R}^{n \\times n}$ 是正定对称矩阵，则存在唯一下三角矩阵 $L$，使得：\n",
    "\n",
    "$$\n",
    "A = L L^T\n",
    "$$\n",
    "\n",
    "这叫做**Cholesky 分解**。\n",
    "\n",
    "---\n",
    "\n",
    "####  优点\n",
    "\n",
    "* 比 $LU$ 更快、更稳定\n",
    "* 避免求逆，适用于解正定线性系统\n",
    "* 可用于高斯过程、贝叶斯回归等模型推断\n",
    "\n",
    "---\n",
    "\n",
    "###  五、图神经网络中的正定矩阵\n",
    "\n",
    "---\n",
    "\n",
    "####  图拉普拉斯矩阵的正定性\n",
    "\n",
    "图的标准拉普拉斯矩阵 $L = D - A$ 是对称正半定：\n",
    "\n",
    "* 最小特征值为 0 ⇒ 反映图的连通性\n",
    "* 若图连通，则 $\\lambda_1 = 0 < \\lambda_2 \\leq \\dots \\leq \\lambda_n$\n",
    "\n",
    "---\n",
    "\n",
    "####  拉普拉斯正定性在 GNN 中的用途\n",
    "\n",
    "| 用途        | 正定矩阵的意义                             |\n",
    "| --------- | ----------------------------------- |\n",
    "| 谱图卷积（GCN） | 利用 $L$ 的谱分解设计滤波器                    |\n",
    "| 图聚类       | 使用拉普拉斯特征向量构造低维表示                    |\n",
    "| GNN 正则项设计 | $x^T L x$ 表示节点之间差异，常用于平滑正则化         |\n",
    "| 图自监督学习    | 拉普拉斯正定性用于设计 contrastive loss 的稳定性下界 |\n",
    "\n",
    "---\n",
    "\n",
    "###  六、小结表格\n",
    "\n",
    "| 概念    | 内容                         |\n",
    "| ----- | -------------------------- |\n",
    "| 对称矩阵  | $A = A^T$，具有实特征值，正交特征向量    |\n",
    "| 正定矩阵  | 任意 $x^T A x > 0$，特征值全正     |\n",
    "| 半正定矩阵 | 任意 $x^T A x \\geq 0$，允许零特征值 |\n",
    "| 判断标准  | 特征值、主子式、Cholesky、代数式       |\n",
    "| 应用    | 优化、协方差分析、谱方法、GNN           |\n",
    "\n",
    "---\n",
    "\n",
    "###  推荐练习\n",
    "\n",
    "1. 判断以下矩阵是否正定/半正定：\n",
    "\n",
    "   * $A = \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$\n",
    "   * $B = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$\n",
    "\n",
    "2. 编写 Python 程序：\n",
    "\n",
    "   * 利用 `numpy.linalg.eig` 计算特征值判断正定性\n",
    "   * 使用 `scipy.linalg.cholesky` 实现 Cholesky 分解\n",
    "\n",
    "3. 设计一个图结构，计算其拉普拉斯矩阵的特征值，解释图结构与正定性的关系。\n",
    "\n",
    "\n"
   ],
   "id": "1b0a700437107847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "##  第8部分：复矩阵与快速傅里叶变换（Complex Matrices & FFT）\n",
    "\n",
    "---\n",
    "\n",
    "###  一、复数与复向量基础\n",
    "\n",
    "---\n",
    "\n",
    "####  复数定义\n",
    "\n",
    "复数 $z \\in \\mathbb{C}$ 的形式为：\n",
    "\n",
    "$$\n",
    "z = a + bi, \\quad a, b \\in \\mathbb{R},\\ i^2 = -1\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $a$ 是实部，$b$ 是虚部\n",
    "* $|z| = \\sqrt{a^2 + b^2}$ 是模\n",
    "* $\\bar{z} = a - bi$ 是共轭\n",
    "\n",
    "---\n",
    "\n",
    "####  复向量与复矩阵\n",
    "\n",
    "* 复向量 $\\mathbf{z} \\in \\mathbb{C}^n$\n",
    "* 复矩阵 $A \\in \\mathbb{C}^{m \\times n}$\n",
    "\n",
    "复矩阵的**共轭转置**定义为：\n",
    "\n",
    "$$\n",
    "A^H = \\bar{A}^T\n",
    "$$\n",
    "\n",
    "即先取复共轭，再转置。\n",
    "\n",
    "---\n",
    "\n",
    "####  Hermitian（自伴）矩阵\n",
    "\n",
    "若：\n",
    "\n",
    "$$\n",
    "A = A^H\n",
    "$$\n",
    "\n",
    "则称 $A$ 是**Hermitian 矩阵**，是复数域中对称矩阵的推广。\n",
    "\n",
    "**性质：**\n",
    "\n",
    "* 所有特征值为实数\n",
    "* 特征向量可以正交\n",
    "\n",
    "---\n",
    "\n",
    "### 二、复特征值与谱分解\n",
    "\n",
    "---\n",
    "\n",
    "####  正交性定义（复数域）\n",
    "\n",
    "对复向量 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{C}^n$，定义内积为：\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{y}^H \\mathbf{x}\n",
    "$$\n",
    "\n",
    "若 $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0$，则 $\\mathbf{x} \\perp \\mathbf{y}$\n",
    "\n",
    "---\n",
    "\n",
    "####  复特征分解的拓展\n",
    "\n",
    "对于 Hermitian 矩阵 $A \\in \\mathbb{C}^{n \\times n}$，可对角化为：\n",
    "\n",
    "$$\n",
    "A = U \\Lambda U^H\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $U \\in \\mathbb{C}^{n \\times n}$ 是单位酉矩阵（复正交矩阵）\n",
    "* $\\Lambda \\in \\mathbb{R}^{n \\times n}$ 是实对角矩阵（特征值）\n",
    "\n",
    "---\n",
    "\n",
    "###  三、傅里叶变换与离散傅里叶变换（DFT）\n",
    "\n",
    "---\n",
    "\n",
    "####  傅里叶变换简介\n",
    "\n",
    "傅里叶变换是一种将**时域信号**转换为**频域表示**的方法。其核心思想是：**任意信号都可以看作一组不同频率的正弦波的叠加**。\n",
    "\n",
    "---\n",
    "\n",
    "####  离散傅里叶变换（DFT）\n",
    "\n",
    "对于长度为 $n$ 的复向量 $x = (x_0, x_1, \\dots, x_{n-1})$，其 DFT 是：\n",
    "\n",
    "$$\n",
    "X_k = \\sum_{j=0}^{n-1} x_j \\cdot e^{-2\\pi i k j / n}, \\quad k = 0, \\dots, n-1\n",
    "$$\n",
    "\n",
    "可以写成矩阵形式：\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = F_n \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "其中 $F_n \\in \\mathbb{C}^{n \\times n}$ 是 DFT 矩阵，定义为：\n",
    "\n",
    "$$\n",
    "(F_n)_{k,j} = \\omega_n^{k j}, \\quad \\omega_n = e^{-2\\pi i / n}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  DFT 矩阵的性质\n",
    "\n",
    "| 性质           | 说明                      |\n",
    "| ------------ | ----------------------- |\n",
    "| $F_n$ 是单位酉矩阵 | $F_n^H F_n = nI$（可逆）    |\n",
    "| 正交变换         | DFT 保长度和能量（Parseval 定理） |\n",
    "| 特征值在单位圆上     | 所有特征值模为 1               |\n",
    "| 傅里叶基         | $F_n$ 的列向量为复指数信号，构成基    |\n",
    "\n",
    "---\n",
    "\n",
    "###  四、快速傅里叶变换（FFT）\n",
    "\n",
    "---\n",
    "\n",
    "####  背景\n",
    "\n",
    "* 直接计算 DFT 的时间复杂度是 $O(n^2)$\n",
    "* **快速傅里叶变换（FFT）** 是一种高效算法，将复杂度降为：\n",
    "\n",
    "$$\n",
    "O(n \\log n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "####  FFT 的基本思想\n",
    "\n",
    "利用 DFT 的对称性和周期性，将 DFT 分解为**偶数项**与**奇数项**：\n",
    "\n",
    "$$\n",
    "X_k = E_k + \\omega_n^k O_k\n",
    "$$\n",
    "\n",
    "其中 $E_k$ 是偶项 DFT，$O_k$ 是奇项 DFT，可递归计算。\n",
    "\n",
    "> 这称为 **Cooley-Tukey FFT 算法**，是目前最常用的 FFT 实现。\n",
    "\n",
    "---\n",
    "\n",
    "####  应用举例\n",
    "\n",
    "| 应用             | 内容                                  |\n",
    "| -------------- | ----------------------------------- |\n",
    "| 信号处理           | 滤波、频谱分析、语音识别                        |\n",
    "| 卷积加速           | 使用频域乘法进行快速卷积（CNN、GNN）               |\n",
    "| 谱图学习           | 图信号 → 频域表示                          |\n",
    "| Transformer 变体 | FourierFormer、FNet 利用 FFT 替代注意力     |\n",
    "| GNN 中谱滤波器      | $g(L) x$ 可在频域设计为 $g(\\Lambda) U^T x$ |\n",
    "\n",
    "---\n",
    "\n",
    "###  五、在图神经网络中的作用\n",
    "\n",
    "---\n",
    "\n",
    "####  图信号频谱分析\n",
    "\n",
    "* 给定图拉普拉斯矩阵 $L = U \\Lambda U^T$\n",
    "* $U$：拉普拉斯特征向量，构成“图傅里叶基”\n",
    "* 图信号 $x$ 在频域中的表示为：\n",
    "\n",
    "$$\n",
    "\\hat{x} = U^T x\n",
    "$$\n",
    "\n",
    "* 对信号进行谱滤波：\n",
    "\n",
    "$$\n",
    "g(L) x = U g(\\Lambda) U^T x\n",
    "$$\n",
    "\n",
    "> 这就是**谱图卷积**的核心思想！\n",
    "\n",
    "---\n",
    "\n",
    "####  相关GNN模型\n",
    "\n",
    "| 模型                   | FFT/DFT 应用方式                    |\n",
    "| -------------------- | ------------------------------- |\n",
    "| Spectral GCN         | 使用 $L = U \\Lambda U^T$，在频域设计滤波器 |\n",
    "| ChebNet              | 使用 Chebyshev 多项式近似频谱卷积          |\n",
    "| FNet / FourierFormer | 替代 attention 使用 DFT 实现编码器       |\n",
    "| GraphWave            | 用频谱特征分布表达节点结构特征                 |\n",
    "\n",
    "---\n",
    "\n",
    "###  推荐练习与代码实现\n",
    "\n",
    "1. 用 numpy 编写：\n",
    "\n",
    "   * DFT 矩阵构造\n",
    "   * 使用 `np.fft.fft()` 与矩阵乘法结果对比\n",
    "2. 实现 1D 卷积与 FFT 加速卷积对比\n",
    "3. 对小型图构建拉普拉斯矩阵，手动进行谱分解和图傅里叶变换\n",
    "\n",
    "---\n",
    "\n",
    "###  总结表格\n",
    "\n",
    "| 概念          | 内容                        |\n",
    "| ----------- | ------------------------- |\n",
    "| 复矩阵         | 具有复数元素，可用于傅里叶分析、谱分析       |\n",
    "| Hermitian   | 满足 $A = A^H$，特征值实数，特征向量正交 |\n",
    "| DFT         | 离散傅里叶变换，将信号变换到频域          |\n",
    "| DFT矩阵 $F_n$ | 酉矩阵，构造频率基底                |\n",
    "| FFT         | 快速算法，复杂度降为 $O(n \\log n)$  |\n",
    "| GNN 应用      | 频域滤波、谱嵌入、频谱卷积核设计等         |\n",
    "\n",
    "\n"
   ],
   "id": "4037717121f42ee9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 第9部分：相似矩阵与若尔当标准型（Similarity & Jordan Form）\n",
    "\n",
    "---\n",
    "\n",
    "### 一、什么是相似矩阵（Similar Matrices）\n",
    "\n",
    "---\n",
    "\n",
    "#### 定义\n",
    "\n",
    "两个 $n \\times n$ 的方阵 $A$、$B$ 是**相似矩阵**，当存在一个可逆矩阵 $P$，使得：\n",
    "\n",
    "$$\n",
    "B = P^{-1} A P\n",
    "$$\n",
    "\n",
    "即 $A$ 与 $B$ 是通过**相似变换**联系的。\n",
    "\n",
    "---\n",
    "\n",
    "#### 相似矩阵的性质\n",
    "\n",
    "| 性质       | 内容                                                 |\n",
    "| -------- | -------------------------------------------------- |\n",
    "| 相同的特征值   | 相似矩阵具有完全相同的特征值（含重数）                                |\n",
    "| 相同的行列式和迹 | $\\det(A) = \\det(B),\\ \\text{Tr}(A) = \\text{Tr}(B)$  |\n",
    "| 保持矩阵多项式  | $f(A) \\sim f(B)$（如 $A^2 + A + I \\sim B^2 + B + I$） |\n",
    "| 表示相同线性变换 | 仅在不同基底下的表达不同                                       |\n",
    "\n",
    "---\n",
    "\n",
    "#### 直观理解\n",
    "\n",
    "相似矩阵代表了“同一个线性变换在不同坐标系中的表示”。\n",
    "\n",
    "> 如同二维平面上的向量 $[1,0]$ 和 $[\\cosθ, \\sinθ]$，在不同基下坐标不同，但本质一样。\n",
    "\n",
    "---\n",
    "\n",
    "### 二、对角化回顾与限制\n",
    "\n",
    "---\n",
    "\n",
    "#### 可对角化的充要条件\n",
    "\n",
    "矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 可对角化，若存在 $A = P D P^{-1}$，其中：\n",
    "\n",
    "* $D$：对角矩阵，包含特征值\n",
    "* $P$：列为特征向量\n",
    "\n",
    "---\n",
    "\n",
    "#### 但有些矩阵不可对角化\n",
    "\n",
    "示例：\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其特征值只有一个 $\\lambda = 1$，且只存在一个线性无关特征向量，**不能对角化**。\n",
    "\n",
    "这类矩阵需要更广义的表示方式 ⇒ **若尔当标准型（Jordan Canonical Form）**\n",
    "\n",
    "---\n",
    "\n",
    "### 三、若尔当块（Jordan Block）\n",
    "\n",
    "---\n",
    "\n",
    "#### Jordan 块定义\n",
    "\n",
    "若 $\\lambda$ 是某矩阵的特征值，则对应的**若尔当块**是：\n",
    "\n",
    "$$\n",
    "J_k(\\lambda) =\n",
    "\\begin{bmatrix}\n",
    "\\lambda & 1      &        & 0 \\\\\n",
    "0      & \\lambda & \\ddots &   \\\\\n",
    "       & \\ddots & \\ddots & 1 \\\\\n",
    "0      &        & 0      & \\lambda \\\\\n",
    "\\end{bmatrix}_{k \\times k}\n",
    "$$\n",
    "\n",
    "即：对角线为 $\\lambda$，上对角线为 1，其余为 0\n",
    "\n",
    "---\n",
    "\n",
    "#### 若尔当块的结构说明\n",
    "\n",
    "* 上对角线的 1 表示这个块内的向量之间不是完全线性独立\n",
    "* 用于表达“广义特征向量”之间的嵌套关系\n",
    "* 若只有对角线 ⇒ 就是普通可对角化情况\n",
    "\n",
    "---\n",
    "\n",
    "### 四、若尔当标准型（Jordan Canonical Form）\n",
    "\n",
    "---\n",
    "\n",
    "#### 定义\n",
    "\n",
    "任意一个 $n \\times n$ 的方阵 $A$，都存在一个可逆矩阵 $P$，使得：\n",
    "\n",
    "$$\n",
    "A = P J P^{-1}\n",
    "$$\n",
    "\n",
    "其中 $J$ 是由若干个若尔当块组成的**分块上三角矩阵**，称为 $A$ 的**若尔当标准型**。\n",
    "\n",
    "---\n",
    "\n",
    "#### 若尔当形式的意义\n",
    "\n",
    "* 若尔当标准型是比对角化更广义的相似化表达\n",
    "* 它揭示了矩阵**线性变换的本质结构**\n",
    "* 提供了处理**不可对角化矩阵**（如缺少足够特征向量）的方法\n",
    "\n",
    "---\n",
    "\n",
    "#### 例子：不可对角化矩阵的 Jordan 表示\n",
    "\n",
    "设：\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 3 & 1 \\\\ 0 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "只有一个特征向量 ⇒ 不可对角化\n",
    "\n",
    "Jordan 形式：\n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix} 3 & 1 \\\\ 0 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "这已经是若尔当形式（一个 $2 \\times 2$ 的 Jordan 块）\n",
    "\n",
    "---\n",
    "\n",
    "### 五、广义特征向量与 Jordan 链\n",
    "\n",
    "---\n",
    "\n",
    "#### 原因：当一个特征值的**几何重数**小于其**代数重数**时，需构造广义特征向量链：\n",
    "\n",
    "广义特征向量定义：\n",
    "\n",
    "若 $(A - \\lambda I)^k \\mathbf{v} = 0$，但 $(A - \\lambda I)^{k-1} \\mathbf{v} \\neq 0$，则称 $\\mathbf{v}$ 是**阶数为 k 的广义特征向量**\n",
    "\n",
    "这些广义特征向量会组成一个链：\n",
    "\n",
    "$$\n",
    "(A - \\lambda I)\\mathbf{v}_k = \\mathbf{v}_{k-1}, \\dots, (A - \\lambda I)\\mathbf{v}_2 = \\mathbf{v}_1\n",
    "$$\n",
    "\n",
    "最终可以构造出 Jordan 块。\n",
    "\n",
    "---\n",
    "\n",
    "### 六、Jordan 形式的计算（简要步骤）\n",
    "\n",
    "---\n",
    "\n",
    "#### 步骤概要\n",
    "\n",
    "1. 求特征值及其代数重数\n",
    "2. 求每个特征值的特征空间（几何重数）\n",
    "3. 判断是否可对角化（若所有几何重数 = 代数重数 ⇒ 可对角化）\n",
    "4. 若不能对角化，则构造广义特征向量链，构造若尔当块\n",
    "5. 拼接各 Jordan 块形成 $J$，构造对应的变换矩阵 $P$\n",
    "\n",
    "---\n",
    "\n",
    "#### 实际计算挑战大，一般使用数值库（如 MATLAB 的 `jordan()`）\n",
    "\n",
    "---\n",
    "\n",
    "### 七、Jordan 形式的应用场景\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 矩阵幂 / 指数的简化计算\n",
    "\n",
    "$$\n",
    "A^k = P J^k P^{-1}\n",
    "$$\n",
    "\n",
    "若 $J$ 为对角矩阵或 Jordan 块形式，则 $J^k$ 更易计算。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 矩阵微分方程解\n",
    "\n",
    "解如下线性系统：\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathbf{x}}{dt} = A \\mathbf{x}\n",
    "$$\n",
    "\n",
    "若 $A$ 可对角化，解为：\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = e^{At} \\mathbf{x}_0 = P e^{Jt} P^{-1} \\mathbf{x}_0\n",
    "$$\n",
    "\n",
    "其中 $e^{Jt}$ 可在 Jordan 块上逐块计算。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 深度学习稳定性分析\n",
    "\n",
    "某些理论分析中涉及 Jacobian、Hessian 的结构与谱特性 —— 若尔当分解帮助理解其收敛性与收敛速度。\n",
    "\n",
    "---\n",
    "\n",
    "### 八、与图神经网络的联系（扩展思考）\n",
    "\n",
    "---\n",
    "\n",
    "虽然 GNN 中多数操作基于**对称矩阵**（如拉普拉斯矩阵），不涉及复杂的 Jordan 分解，但在如下高级问题中仍有价值：\n",
    "\n",
    "| 场景      | 作用                                          |\n",
    "| ------- | ------------------------------------------- |\n",
    "| 图稳定性分析  | 检查传播矩阵是否幂稳定，需要分析幂级行为 ⇒ Jordan 形式            |\n",
    "| 矩阵函数计算  | $f(A) = P f(J) P^{-1}$，用于扩展谱滤波形式            |\n",
    "| 图卷积理论推广 | 若考虑非对称图或非 Hermitian 邻接矩阵时，Jordan 形式比对角分解更通用 |\n",
    "\n",
    "---\n",
    "\n",
    "### 总结表格\n",
    "\n",
    "| 概念        | 内容                               |\n",
    "| --------- | -------------------------------- |\n",
    "| 相似矩阵      | 存在 $P$ 使 $B = P^{-1} A P$，共享核心性质 |\n",
    "| Jordan 块  | 单个特征值对应的一类不可对角化结构                |\n",
    "| Jordan 形式 | 任何矩阵都可相似变换为的标准形式（含多个块）           |\n",
    "| 广义特征向量    | 解决特征向量不足的问题                      |\n",
    "| 应用        | 幂计算、微分方程、稳定性分析、谱方法拓展             |\n",
    "\n",
    "---\n",
    "\n",
    "### 建议练习题\n",
    "\n",
    "1. 判断下列矩阵是否可对角化，若不可，写出其 Jordan 形式：\n",
    "\n",
    "   $$\n",
    "   A = \\begin{bmatrix} 4 & 1 \\\\ 0 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\n",
    "   $$\n",
    "2. 利用 Python 中的 `scipy.linalg` 包，计算任意矩阵的特征值、广义特征向量\n",
    "3. 推导一个 $3 \\times 3$ 矩阵的 Jordan 形式，并用其计算 $A^3$\n",
    "\n",
    "\n"
   ],
   "id": "d9291b14e0fdf7b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 第10部分：奇异值分解（Singular Value Decomposition, SVD）\n",
    "\n",
    "---\n",
    "\n",
    "### 一、奇异值分解的定义\n",
    "\n",
    "---\n",
    "\n",
    "给定任意矩阵 $A \\in \\mathbb{R}^{m \\times n}$，不要求方阵或对称，奇异值分解将其分解为三个矩阵的乘积：\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $U \\in \\mathbb{R}^{m \\times m}$ 是正交矩阵（列向量两两正交且范数为1），称为**左奇异向量矩阵**\n",
    "* $\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，对角线上为非负实数，称为**奇异值矩阵**\n",
    "* $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵（列向量两两正交且范数为1），称为**右奇异向量矩阵**\n",
    "\n",
    "奇异值按大小顺序排列：\n",
    "\n",
    "$$\n",
    "\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r > 0\n",
    "$$\n",
    "\n",
    "其中 $r = \\text{rank}(A)$。\n",
    "\n",
    "---\n",
    "\n",
    "### 二、奇异值、奇异向量的含义\n",
    "\n",
    "---\n",
    "\n",
    "* **奇异值 $\\sigma_i$**：反映矩阵 $A$ 在对应奇异向量方向上的伸缩因子\n",
    "* **左奇异向量 $u_i$**：表示在 $\\mathbb{R}^m$ 空间中，输入空间被映射到的正交基\n",
    "* **右奇异向量 $v_i$**：表示在 $\\mathbb{R}^n$ 空间中输入矩阵的正交基\n",
    "\n",
    "---\n",
    "\n",
    "### 三、奇异值与特征值的关系\n",
    "\n",
    "---\n",
    "\n",
    "* $A^T A \\in \\mathbb{R}^{n \\times n}$ 和 $A A^T \\in \\mathbb{R}^{m \\times m}$ 都是对称半正定矩阵。\n",
    "* $A^T A$ 的特征值为 $\\sigma_i^2$，对应的特征向量即为 $V$ 中的列向量。\n",
    "* $A A^T$ 的特征值也为 $\\sigma_i^2$，对应特征向量为 $U$ 中的列向量。\n",
    "\n",
    "---\n",
    "\n",
    "### 四、奇异值分解的计算过程\n",
    "\n",
    "---\n",
    "\n",
    "1. 计算 $A^T A$ 的特征值分解：\n",
    "\n",
    "$$\n",
    "A^T A = V \\Lambda V^T\n",
    "$$\n",
    "\n",
    "其中 $\\Lambda = \\text{diag}(\\sigma_1^2, \\ldots, \\sigma_r^2)$\n",
    "\n",
    "2. 计算左奇异向量：\n",
    "\n",
    "$$\n",
    "U = A V \\Sigma^{-1}\n",
    "$$\n",
    "\n",
    "其中 $\\Sigma^{-1}$ 是奇异值的倒数对角矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "### 五、奇异值分解的几何意义\n",
    "\n",
    "---\n",
    "\n",
    "* 将 $A$ 看作从 $\\mathbb{R}^n$ 映射到 $\\mathbb{R}^m$ 的线性变换。\n",
    "* $V$ 给出输入空间的正交基，$U$ 给出输出空间的正交基。\n",
    "* $\\Sigma$ 给出对应基向量的缩放因子。\n",
    "\n",
    "---\n",
    "\n",
    "### 六、奇异值分解的性质\n",
    "\n",
    "---\n",
    "\n",
    "| 性质           | 说明                                   |\n",
    "| ------------ | ------------------------------------ |\n",
    "| 所有奇异值非负      | $\\sigma_i \\geq 0$                    |\n",
    "| 奇异值唯一        | 奇异值唯一确定，奇异向量不唯一                      |\n",
    "| 矩阵范数         | 最大奇异值等于矩阵的谱范数（算子范数）                  |\n",
    "| Frobenius 范数 | $\\|A\\|_F = \\sqrt{\\sum_i \\sigma_i^2}$ |\n",
    "| 低秩近似         | 利用前 $k$ 个奇异值和对应向量构造最佳秩 $k$ 近似        |\n",
    "\n",
    "---\n",
    "\n",
    "### 七、奇异值分解在应用中的核心价值\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 低秩矩阵近似（矩阵压缩）\n",
    "\n",
    "$$\n",
    "A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "它是所有秩不超过 $k$ 的矩阵中，最接近 $A$ 的矩阵（最小 Frobenius 范数误差）\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 主成分分析（PCA）\n",
    "\n",
    "* PCA 通过 SVD 计算数据协方差矩阵的特征向量，实现降维和数据压缩。\n",
    "* 右奇异向量矩阵 $V$ 对应主成分方向。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 图嵌入与降维\n",
    "\n",
    "* 使用 SVD 对邻接矩阵或拉普拉斯矩阵进行分解，提取节点低维表示。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 逆矩阵与伪逆计算\n",
    "\n",
    "* Moore-Penrose 伪逆：\n",
    "\n",
    "$$\n",
    "A^+ = V \\Sigma^+ U^T\n",
    "$$\n",
    "\n",
    "其中 $\\Sigma^+$ 是将非零奇异值倒数并转置形成的矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "### 八、奇异值分解示例\n",
    "\n",
    "---\n",
    "\n",
    "假设：\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\\\ 1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "通过计算，得到：\n",
    "\n",
    "* 奇异值 $\\sigma_1, \\sigma_2$\n",
    "* 矩阵 $U \\in \\mathbb{R}^{3 \\times 3}$、$\\Sigma \\in \\mathbb{R}^{3 \\times 2}$、$V \\in \\mathbb{R}^{2 \\times 2}$\n",
    "\n",
    "---\n",
    "\n",
    "### 九、Python实现示例\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[3,1],\n",
    "              [1,3],\n",
    "              [1,1]])\n",
    "\n",
    "U, S, VT = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "print(\"U =\", U)\n",
    "print(\"奇异值 =\", S)\n",
    "print(\"V^T =\", VT)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 十、总结表格\n",
    "\n",
    "| 概念             | 说明               |\n",
    "| -------------- | ---------------- |\n",
    "| 奇异值 $\\sigma_i$ | 非负实数，衡量伸缩强度      |\n",
    "| 左奇异向量 $u_i$    | $A$ 映射到输出空间的基    |\n",
    "| 右奇异向量 $v_i$    | 输入空间基向量          |\n",
    "| 低秩近似           | 保持最大能量的秩 $k$ 近似  |\n",
    "| 伪逆             | 广义逆，计算最小二乘解      |\n",
    "| 应用             | 降维、数据压缩、图嵌入、矩阵近似 |\n",
    "\n",
    "---\n",
    "\n",
    "###  十一、扩展阅读与思考\n",
    "\n",
    "* SVD 与 PCA 的关系\n",
    "* 如何利用 SVD 优化图卷积滤波器设计\n",
    "* SVD 的数值稳定性和快速算法（随机 SVD、截断 SVD）\n",
    "* 奇异值谱对深度神经网络权重矩阵训练稳定性的影响\n",
    "\n"
   ],
   "id": "b29e31fd670a79e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 第11部分：基变换及图像压缩（Change of Basis & Image Compression）\n",
    "\n",
    "---\n",
    "\n",
    "### 一、基变换（Change of Basis）\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 向量基底（Basis）回顾\n",
    "\n",
    "* 在向量空间 $V$ 中，一组线性无关向量 $\\{ \\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_n \\}$ 称为该空间的基底。\n",
    "\n",
    "* 任何向量 $\\mathbf{v} \\in V$ 都可以唯一表示为基底的线性组合：\n",
    "\n",
    "  $$\n",
    "  \\mathbf{v} = x_1 \\mathbf{b}_1 + x_2 \\mathbf{b}_2 + \\cdots + x_n \\mathbf{b}_n\n",
    "  $$\n",
    "\n",
    "* 向量 $\\mathbf{v}$ 在基底 $B$ 下的坐标是向量 $\\mathbf{x} = [x_1, \\ldots, x_n]^T$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 基变换的定义\n",
    "\n",
    "* 设有两个基底：\n",
    "\n",
    "  $$\n",
    "  B = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}, \\quad\n",
    "  B' = \\{\\mathbf{b}_1', \\ldots, \\mathbf{b}_n'\\}\n",
    "  $$\n",
    "\n",
    "* 一个向量在 $B$ 中的坐标为 $\\mathbf{x}$，在 $B'$ 中的坐标为 $\\mathbf{x}'$\n",
    "\n",
    "* 存在一个**基变换矩阵** $P$，满足：\n",
    "\n",
    "  $$\n",
    "  \\mathbf{x} = P \\mathbf{x}'\n",
    "  $$\n",
    "\n",
    "  或\n",
    "\n",
    "  $$\n",
    "  \\mathbf{x}' = P^{-1} \\mathbf{x}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 基变换矩阵 $P$ 的构造\n",
    "\n",
    "* $P$ 的列向量是基底 $B'$ 中向量在基底 $B$ 下的坐标表示：\n",
    "\n",
    "  $$\n",
    "  P = [ [\\mathbf{b}_1']_B, [\\mathbf{b}_2']_B, \\ldots, [\\mathbf{b}_n']_B ]\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 矩阵在基底变换下的变化\n",
    "\n",
    "* 矩阵 $A$ 表示线性变换在基底 $B$ 下的矩阵表示\n",
    "\n",
    "* 在基底 $B'$ 下的表示为：\n",
    "\n",
    "  $$\n",
    "  A' = P^{-1} A P\n",
    "  $$\n",
    "\n",
    "* 这就是相似变换的定义，连接基变换与相似矩阵的概念\n",
    "\n",
    "---\n",
    "\n",
    "### 二、基变换的意义与作用\n",
    "\n",
    "---\n",
    "\n",
    "* 基变换让我们能够用更方便的基底来表达和计算问题，比如对角基底让线性变换变得简单\n",
    "\n",
    "* 在信号处理中，基变换是频域转换、压缩编码的数学基础\n",
    "\n",
    "* 通过合适基变换，复杂信号可以被稀疏表示，从而有效压缩\n",
    "\n",
    "---\n",
    "\n",
    "### 三、图像压缩中的基变换应用\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 图像的矩阵表示\n",
    "\n",
    "* 灰度图像用二维矩阵表示，每个元素对应像素灰度值\n",
    "\n",
    "* 彩色图像可看作三个矩阵（R,G,B通道）\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 利用基变换进行压缩\n",
    "\n",
    "* **目标**：找到基底使图像在新基底下有较少非零系数（稀疏），方便压缩\n",
    "\n",
    "* 常用方法：奇异值分解（SVD）、离散余弦变换（DCT）、小波变换（Wavelet）\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 基于 SVD 的图像压缩步骤\n",
    "\n",
    "---\n",
    "\n",
    "1. **对图像矩阵 $A$ 做 SVD 分解**：\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "2. **保留前 $k$ 个最大的奇异值及对应奇异向量，构造低秩近似**：\n",
    "\n",
    "$$\n",
    "A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "3. **压缩**：存储较小的 $k$，节省存储空间\n",
    "\n",
    "4. **重构**：用 $A_k$ 近似原图像，误差可控\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 示例说明\n",
    "\n",
    "* 原图尺寸：$256 \\times 256$ 共 65536 个像素点\n",
    "\n",
    "* 使用 $k=50$ 个奇异值，存储量约 $50 \\times (256 + 256 + 1) = 25650$（远小于原图）\n",
    "\n",
    "* 近似重构质量较好，压缩效果显著\n",
    "\n",
    "---\n",
    "\n",
    "### 四、其他常见基变换与图像压缩技术\n",
    "\n",
    "---\n",
    "\n",
    "| 技术            | 基底类型  | 说明         | 应用场景       |\n",
    "| ------------- | ----- | ---------- | ---------- |\n",
    "| 离散余弦变换（DCT）   | 正交余弦基 | JPEG标准压缩核心 | JPEG图像压缩   |\n",
    "| 小波变换（Wavelet） | 多尺度基  | 较好时频局部化特性  | JPEG2000压缩 |\n",
    "| 傅里叶变换（FFT）    | 复指数基  | 频域分析       | 图像频谱滤波     |\n",
    "\n",
    "---\n",
    "\n",
    "### 五、基变换与压缩的数学原理\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 能量集中性\n",
    "\n",
    "* 好的基变换能让信号能量集中在少数几个基向量的系数上\n",
    "\n",
    "* 这样在截断后仍能保留大部分信息\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 稀疏表示与压缩率\n",
    "\n",
    "* 稀疏表示使得数据压缩效果显著\n",
    "\n",
    "* 通过压缩编码（如量化、熵编码）进一步减少数据大小\n",
    "\n",
    "---\n",
    "\n",
    "### 六、基变换及图像压缩的 Python 实践示例\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, color\n",
    "\n",
    "# 读取示例图像并转为灰度\n",
    "img = color.rgb2gray(data.astronaut())\n",
    "A = img.astype(float)\n",
    "\n",
    "# SVD 分解\n",
    "U, S, VT = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# 保留 k 个奇异值进行重构\n",
    "k = 50\n",
    "A_k = np.dot(U[:, :k], np.dot(np.diag(S[:k]), VT[:k, :]))\n",
    "\n",
    "# 显示压缩前后图像\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(A, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(f\"Compressed Image (k={k})\")\n",
    "plt.imshow(A_k, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 七、小结表格\n",
    "\n",
    "| 主题     | 内容                |\n",
    "| ------ | ----------------- |\n",
    "| 基变换    | 不同基底下向量的坐标变换      |\n",
    "| 基变换矩阵  | 描述基底之间的线性变换       |\n",
    "| 矩阵相似变换 | 矩阵表示在不同基底下的形式     |\n",
    "| 图像压缩   | 利用基变换使图像稀疏，便于压缩存储 |\n",
    "| SVD压缩  | 利用低秩近似实现高效图像压缩    |\n",
    "\n",
    "---\n",
    "\n",
    "### 八、扩展思考\n",
    "\n",
    "* 选择合适的基底对压缩率和质量的影响\n",
    "* 小波变换与 DCT 在图像压缩中的比较\n",
    "* 基变换思想在图神经网络频谱卷积中的体现\n",
    "\n",
    "\n"
   ],
   "id": "9ae7a6543915443b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 第12部分：左右逆和伪逆（Left and Right Inverses & Pseudoinverse）\n",
    "\n",
    "---\n",
    "\n",
    "### 一、矩阵逆的基础回顾\n",
    "\n",
    "---\n",
    "\n",
    "* 对于方阵 $A \\in \\mathbb{R}^{n \\times n}$，如果存在矩阵 $A^{-1}$，满足：\n",
    "\n",
    "$$\n",
    "A A^{-1} = A^{-1} A = I_n\n",
    "$$\n",
    "\n",
    "则称 $A$ 可逆，$A^{-1}$ 为其逆矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "* 逆矩阵唯一且存在的充分必要条件是 $A$ 满秩（秩等于维度 $n$）。\n",
    "\n",
    "---\n",
    "\n",
    "### 二、非方阵的逆问题\n",
    "\n",
    "---\n",
    "\n",
    "* 当矩阵 $A$ 不是方阵（如 $m \\neq n$），或方阵但不满秩时，**逆矩阵不存在**。\n",
    "\n",
    "* 但在某些应用中，仍希望定义类似“逆”的矩阵，满足部分逆性质。\n",
    "\n",
    "---\n",
    "\n",
    "### 三、左右逆矩阵定义\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 左逆矩阵（Left Inverse）\n",
    "\n",
    "* 对于 $A \\in \\mathbb{R}^{m \\times n}$，如果存在矩阵 $L \\in \\mathbb{R}^{n \\times m}$，使得：\n",
    "\n",
    "$$\n",
    "L A = I_n\n",
    "$$\n",
    "\n",
    "则称 $L$ 是 $A$ 的**左逆矩阵**。\n",
    "\n",
    "* 左逆存在条件：$A$ 列满秩（即 $\\text{rank}(A) = n$，列向量线性无关）\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 右逆矩阵（Right Inverse）\n",
    "\n",
    "* 对于 $A \\in \\mathbb{R}^{m \\times n}$，如果存在矩阵 $R \\in \\mathbb{R}^{n \\times m}$，使得：\n",
    "\n",
    "$$\n",
    "A R = I_m\n",
    "$$\n",
    "\n",
    "则称 $R$ 是 $A$ 的**右逆矩阵**。\n",
    "\n",
    "* 右逆存在条件：$A$ 行满秩（即 $\\text{rank}(A) = m$，行向量线性无关）\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 左逆和右逆不是同时存在（除非是方阵且满秩）\n",
    "\n",
    "---\n",
    "\n",
    "### 四、如何求左右逆矩阵\n",
    "\n",
    "---\n",
    "\n",
    "* 若 $A$ 列满秩，$m \\ge n$，左逆可由：\n",
    "\n",
    "$$\n",
    "L = (A^T A)^{-1} A^T\n",
    "$$\n",
    "\n",
    "满足：\n",
    "\n",
    "$$\n",
    "L A = (A^T A)^{-1} A^T A = I_n\n",
    "$$\n",
    "\n",
    "* 若 $A$ 行满秩，$m \\le n$，右逆为：\n",
    "\n",
    "$$\n",
    "R = A^T (A A^T)^{-1}\n",
    "$$\n",
    "\n",
    "满足：\n",
    "\n",
    "$$\n",
    "A R = A A^T (A A^T)^{-1} = I_m\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 五、Moore-Penrose 伪逆（Pseudoinverse）\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 伪逆定义\n",
    "\n",
    "* 对任意矩阵 $A \\in \\mathbb{R}^{m \\times n}$，存在唯一的矩阵 $A^+ \\in \\mathbb{R}^{n \\times m}$，称为 Moore-Penrose 伪逆，满足以下四个性质：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "A A^+ A = A \\\\\n",
    "A^+ A A^+ = A^+ \\\\\n",
    "(A A^+)^T = A A^+ \\\\\n",
    "(A^+ A)^T = A^+ A\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 伪逆的意义\n",
    "\n",
    "* 伪逆是一种广义逆，可用于解决线性方程组无解或多解问题，计算最小二乘解等。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 伪逆的计算方法\n",
    "\n",
    "---\n",
    "\n",
    "##### （1）基于奇异值分解（SVD）\n",
    "\n",
    "假设 $A = U \\Sigma V^T$，则：\n",
    "\n",
    "$$\n",
    "A^+ = V \\Sigma^+ U^T\n",
    "$$\n",
    "\n",
    "其中 $\\Sigma^+$ 是通过对 $\\Sigma$ 非零奇异值取倒数后转置得到的矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "##### （2）基于左右逆的特殊情况\n",
    "\n",
    "* 如果 $A$ 列满秩，左逆存在，伪逆与左逆相同：\n",
    "\n",
    "$$\n",
    "A^+ = (A^T A)^{-1} A^T\n",
    "$$\n",
    "\n",
    "* 如果 $A$ 行满秩，右逆存在，伪逆与右逆相同：\n",
    "\n",
    "$$\n",
    "A^+ = A^T (A A^T)^{-1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 六、伪逆在最小二乘问题中的应用\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 线性方程组 $A \\mathbf{x} = \\mathbf{b}$ 最小二乘解\n",
    "\n",
    "* 当 $A \\mathbf{x} = \\mathbf{b}$ 无解或不唯一时，最小二乘解为：\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = A^+ \\mathbf{b}\n",
    "$$\n",
    "\n",
    "* 它使得 $\\|A \\mathbf{x} - \\mathbf{b}\\|_2$ 最小。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 特殊情况说明\n",
    "\n",
    "| 情况               | 伪逆计算公式                   | 结果          |\n",
    "| ---------------- | ------------------------ | ----------- |\n",
    "| $m > n$, $A$ 列满秩 | $A^+ = (A^T A)^{-1} A^T$ | 最小二乘解唯一     |\n",
    "| $m < n$, $A$ 行满秩 | $A^+ = A^T (A A^T)^{-1}$ | 有无穷解，求最小范数解 |\n",
    "\n",
    "---\n",
    "\n",
    "### 七、伪逆的性质总结\n",
    "\n",
    "---\n",
    "\n",
    "| 性质  | 内容                   |\n",
    "| --- | -------------------- |\n",
    "| 唯一性 | Moore-Penrose 伪逆唯一存在 |\n",
    "| 一般性 | 对所有矩阵均适用             |\n",
    "| 兼容性 | 满足四个Penrose条件        |\n",
    "| 应用广 | 求最小二乘解，数据拟合，信号恢复     |\n",
    "\n",
    "---\n",
    "\n",
    "### 八、Python 实现示例\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 非方阵示例\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# 计算伪逆\n",
    "A_pinv = np.linalg.pinv(A)\n",
    "\n",
    "print(\"矩阵 A:\\n\", A)\n",
    "print(\"伪逆 A^+:\\n\", A_pinv)\n",
    "\n",
    "# 验证最小二乘解\n",
    "b = np.array([7, 8, 9])\n",
    "x_ls = A_pinv @ b\n",
    "print(\"最小二乘解 x:\\n\", x_ls)\n",
    "\n",
    "# 误差\n",
    "residual = np.linalg.norm(A @ x_ls - b)\n",
    "print(\"残差范数:\", residual)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 九、扩展与应用\n",
    "\n",
    "---\n",
    "\n",
    "* 伪逆是机器学习中线性回归、岭回归、Lasso等方法基础\n",
    "\n",
    "* 在神经网络中用于权重更新及稳定训练\n",
    "\n",
    "* 在图神经网络中的消息传递可视作矩阵乘积，伪逆帮助解决欠定问题\n",
    "\n",
    "---\n",
    "\n",
    "### 十、小结表格\n",
    "\n",
    "| 主题   | 内容                           |\n",
    "| ---- | ---------------------------- |\n",
    "| 左逆矩阵 | 满秩列矩阵的左侧逆矩阵，满足 $L A = I$     |\n",
    "| 右逆矩阵 | 满秩行矩阵的右侧逆矩阵，满足 $A R = I$     |\n",
    "| 伪逆   | Moore-Penrose 广义逆，唯一且适用于所有矩阵 |\n",
    "| 计算方法 | 基于 SVD 及左右逆公式                |\n",
    "| 应用   | 最小二乘解、信号恢复、数据拟合              |\n",
    "\n",
    "\n"
   ],
   "id": "77ccf53a16ba4372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c02447ef7b85842f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86342e69ed26cbbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
