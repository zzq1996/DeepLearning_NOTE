{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Tensor\n",
    "## 1.1 Tensor的基本概念\n",
    "![](./imgs/1_1.png)\n",
    "![](./imgs/1_2.png)\n",
    "![](./imgs/1_3.png)\n",
    "\n",
    "## 1.2 Tensor的操作\n",
    "![](./imgs/1_4.png)\n",
    "\n",
    "![](./imgs/1_5.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) \n",
      " torch.FloatTensor \n",
      " torch.Size([2, 2]) \n",
      " 2 \n",
      " torch.Size([2, 2])\n",
      "['H', 'T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__dlpack__', '__dlpack_device__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_dispatch__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_addmm_activation', '_autocast_to_full_precision', '_autocast_to_reduced_precision', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_conj', '_conj_physical', '_dimI', '_dimV', '_fix_weakref', '_grad', '_grad_fn', '_has_symbolic_sizes_strides', '_indices', '_is_all_true', '_is_any_true', '_is_view', '_is_zerotensor', '_make_subclass', '_make_wrapper_subclass', '_neg_view', '_nested_tensor_size', '_nested_tensor_storage_offsets', '_nested_tensor_strides', '_nnz', '_post_accumulate_grad_hooks', '_python_dispatch', '_reduce_ex_internal', '_sparse_mask_projection', '_to_dense', '_to_sparse', '_to_sparse_bsc', '_to_sparse_bsr', '_to_sparse_csc', '_to_sparse_csr', '_typed_storage', '_update_names', '_values', '_version', '_view_func', '_view_func_unsafe', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'adjoint', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'aminmax', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan2', 'arctan2_', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'argwhere', 'as_strided', 'as_strided_', 'as_strided_scatter', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_left_shift', 'bitwise_left_shift_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_right_shift', 'bitwise_right_shift_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'broadcast_to', 'byte', 'cauchy_', 'ccol_indices', 'cdouble', 'ceil', 'ceil_', 'cfloat', 'chalf', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'col_indices', 'conj', 'conj_physical', 'conj_physical_', 'contiguous', 'copy_', 'copysign', 'copysign_', 'corrcoef', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cov', 'cpu', 'cross', 'crow_indices', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumprod_', 'cumsum', 'cumsum_', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'diagonal_scatter', 'diff', 'digamma', 'digamma_', 'dim', 'dim_order', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dsplit', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'float_power', 'float_power_', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmax', 'fmin', 'fmod', 'fmod_', 'frac', 'frac_', 'frexp', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'histogram', 'hsplit', 'hypot', 'hypot_', 'i0', 'i0_', 'igamma', 'igamma_', 'igammac', 'igammac_', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_reduce', 'index_reduce_', 'index_select', 'indices', 'inner', 'int', 'int_repr', 'inverse', 'ipu', 'is_coalesced', 'is_complex', 'is_conj', 'is_contiguous', 'is_cpu', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_inference', 'is_ipu', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_mps', 'is_mtia', 'is_neg', 'is_nested', 'is_nonzero', 'is_ort', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'is_sparse_csr', 'is_vulkan', 'is_xla', 'is_xpu', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'itemsize', 'kron', 'kthvalue', 'layout', 'lcm', 'lcm_', 'ldexp', 'ldexp_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'mH', 'mT', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'moveaxis', 'movedim', 'msort', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nan_to_num', 'nan_to_num_', 'nanmean', 'nanmedian', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'nbytes', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_empty_strided', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'nonzero_static', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'positive', 'pow', 'pow_', 'prelu', 'prod', 'put', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'ravel', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'register_post_accumulate_grad_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'resize_as_sparse_', 'resolve_conj', 'resolve_neg', 'retain_grad', 'retains_grad', 'roll', 'rot90', 'round', 'round_', 'row_indices', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'scatter_reduce', 'scatter_reduce_', 'select', 'select_scatter', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinc', 'sinc_', 'sinh', 'sinh_', 'size', 'slice_scatter', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'swapaxes', 'swapaxes_', 'swapdims', 'swapdims_', 'symeig', 't', 't_', 'take', 'take_along_dim', 'tan', 'tan_', 'tanh', 'tanh_', 'tensor_split', 'tile', 'to', 'to_dense', 'to_mkldnn', 'to_padded_tensor', 'to_sparse', 'to_sparse_bsc', 'to_sparse_bsr', 'to_sparse_coo', 'to_sparse_csc', 'to_sparse_csr', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'untyped_storage', 'values', 'var', 'vdot', 'view', 'view_as', 'vsplit', 'where', 'xlogy', 'xlogy_', 'xpu', 'zero_']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "创建tensor\n",
    "\"\"\"\n",
    "\n",
    "# 直接定义\n",
    "a=torch.Tensor(data=\n",
    "[\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size(dim=1),'\\n',a.size())\n",
    "\n",
    "print(dir(a))  # 列出了张量对象a的所有属性和方法"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:13:53.504801Z",
     "start_time": "2024-04-28T11:13:53.501130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6785e-35, 0.0000e+00, 4.3259e-35],\n",
      "        [0.0000e+00, 1.1210e-43, 0.0000e+00]]) \n",
      " torch.FloatTensor \n",
      " torch.Size([2, 3]) \n",
      " <built-in method size of Tensor object at 0x78a13d6bd580>\n"
     ]
    }
   ],
   "source": [
    "# 指定shape定义\n",
    "a=torch.Tensor(2,3)  # 随机初始化\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:04:19.990421Z",
     "start_time": "2024-04-28T11:04:19.987630Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]]) \n",
      " torch.FloatTensor \n",
      " torch.Size([4, 2, 3]) \n",
      " <built-in method size of Tensor object at 0x7f8661083040>\n"
     ]
    }
   ],
   "source": [
    "# 定义全1的tensor\n",
    "a=torch.ones(4,2,3)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      " torch.FloatTensor \n",
      " torch.Size([2, 3]) \n",
      " <built-in method size of Tensor object at 0x7f8650de17c0>\n"
     ]
    }
   ],
   "source": [
    "# 定义全0的tensor\n",
    "a=torch.zeros(2,3)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]]) \n",
      " torch.FloatTensor \n",
      " torch.Size([2, 3]) \n",
      " <built-in method size of Tensor object at 0x7f8650df9360>\n"
     ]
    }
   ],
   "source": [
    "# 定义对角线为1的tensor\n",
    "a=torch.eye(2,3)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# *_like\n",
    "b=torch.zeros_like(a)\n",
    "c=torch.ones_like(a)\n",
    "print(b,'\\n',c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.1 随机生成tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0405, 0.4856, 0.6845],\n",
      "        [0.6667, 0.1971, 0.5799]]) \n",
      " torch.FloatTensor \n",
      " torch.Size([2, 3]) \n",
      " <built-in method size of Tensor object at 0x7f866cbcb680>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.rand\n",
    "\"\"\"\n",
    "a=torch.rand(2,3)  # 返回[0,1]的均匀分布随机值\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2480,  1.7892, -1.0888, -0.4945,  0.1636]) \n",
      " torch.FloatTensor \n",
      " torch.Size([5]) \n",
      " <built-in method size of Tensor object at 0x7f8650ddd900>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.normal\n",
    "\"\"\"\n",
    "# 从mean=0.0,std=torch.rand(5)构成的五组不同的正态分布中采样出5个数据\n",
    "a=torch.normal(mean=0.0,std=torch.rand(5))\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0108, 0.4210, 0.1349, 1.4880, 0.3310]) \n",
      " torch.FloatTensor \n",
      " torch.Size([5]) \n",
      " <built-in method size of Tensor object at 0x7f866cbcb270>\n"
     ]
    }
   ],
   "source": [
    "# 从mean=torch.rand(5),std=torch.rand(5)构成的五组不同的正态分布中采样出5个数据\n",
    "# 常用于参数初始化\n",
    "a=torch.normal(mean=torch.rand(5),std=torch.rand(5))\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9892,  0.1001],\n",
      "        [-0.9229, -0.5472]]) \n",
      " torch.FloatTensor \n",
      " torch.Size([2, 2]) \n",
      " <built-in method size of Tensor object at 0x7f8650de1590>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.Tensor.uniform\n",
    "- 将生成一个2x2的张量，其中每个元素都是在-1到1之间均匀分布的随机数。\n",
    "\"\"\"\n",
    "\n",
    "a=torch.Tensor(2,2).uniform_(-1,1)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 6, 9]) \n",
      " torch.LongTensor \n",
      " torch.Size([4]) \n",
      " <built-in method size of Tensor object at 0x7f8650d73bd0>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.arange\n",
    "- 序列\n",
    "\"\"\"\n",
    "a=torch.arange(0,11,3)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  6., 10.]) \n",
      " torch.FloatTensor \n",
      " torch.Size([3]) \n",
      " <built-in method size of Tensor object at 0x7f8650de1180>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.linspace\n",
    "- 获取等间隔的n个数字序列\n",
    "\"\"\"\n",
    "a=torch.linspace(2,10,4)  # 获取4个等间隔的数\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 8, 3, 6, 7, 0, 4, 9, 5]) \n",
      " torch.LongTensor \n",
      " torch.Size([10]) \n",
      " <built-in method size of Tensor object at 0x7f86502a6c20>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.randperm\n",
    "- 打乱索引\n",
    "\"\"\"\n",
    "a=torch.randperm(10)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Tensor的属性\n",
    "![](./imgs/1_6.png)\n",
    "![](./imgs/1_7.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], device='cuda:0', dtype=torch.float64) \n",
      " torch.cuda.DoubleTensor \n",
      " torch.Size([2]) \n",
      " <built-in method size of Tensor object at 0x78a05415a5c0> \n",
      " torch.strided\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tensor.layout\n",
    "- torch.strided：这是默认的布局方式，也是大多数张量使用的布局方式。在这种布局下，张量的元素在内存中是连续存储的，通过步幅（stride）来确定每个维度的元素在内存中的存储位置。\n",
    "- torch.sparse_coo：这种布局适用于稀疏张量（Sparse Tensor），它使用 COO（Coordinate）格式来存储数据，即通过坐标来表示非零元素的位置和值。\n",
    "- torch.sparse_csr：这也是一种稀疏张量的布局方式，使用 CSR（Compressed Sparse Row）格式来存储数据，通常用于稠密维度比较大的稀疏张量。\n",
    "\"\"\"\n",
    "dev=torch.device('cuda:0')\n",
    "# a=torch.tensor([2,3],dtype=float).to(dev)\n",
    "a=torch.tensor([2,3],dtype=float,device=dev)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size,'\\n',a.layout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:21:06.606339Z",
     "start_time": "2024-04-28T11:21:06.327391Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 1, 2],\n",
      "                       [0, 1, 2]]),\n",
      "       values=tensor([1, 2, 3]),\n",
      "       size=(4, 4), nnz=3, layout=torch.sparse_coo) \n",
      " torch.sparse.LongTensor \n",
      " torch.Size([4, 4]) \n",
      " <built-in method size of Tensor object at 0x78a05415b510> \n",
      " torch.sparse_coo\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "定义稀疏张量\n",
    "- 非零元素坐标\n",
    "- 非零元素具体的值\n",
    "- shape\n",
    "\"\"\"\n",
    "\n",
    "# 坐标，即(0,0)(1,1)(2,2)\n",
    "i=torch.tensor([\n",
    "    [0,1,2],\n",
    "    [0,1,2]\n",
    "])\n",
    "\n",
    "# 对应的值\n",
    "v=torch.tensor([1,2,3])\n",
    "\n",
    "# create sparse tensor\n",
    "a=torch.sparse_coo_tensor(indices=i,values=v,size=(4,4))\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size,'\\n',a.layout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:31:17.235611Z",
     "start_time": "2024-04-28T11:31:17.231710Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 2, 0, 0],\n",
      "        [0, 0, 3, 0],\n",
      "        [0, 0, 0, 0]]) \n",
      " torch.LongTensor \n",
      " torch.Size([4, 4]) \n",
      " <built-in method size of Tensor object at 0x78a05415a980> \n",
      " torch.strided\n"
     ]
    }
   ],
   "source": [
    "# covert sparse tensor to dense tensor\n",
    "a=a.to_dense()\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size,'\\n',a.layout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:35:24.827631Z",
     "start_time": "2024-04-28T11:35:24.824661Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 3., 0.],\n",
      "        [0., 0., 0., 0.]], device='cuda:0', dtype=torch.float64) \n",
      " torch.cuda.DoubleTensor \n",
      " torch.Size([4, 4]) \n",
      " <built-in method size of Tensor object at 0x78a0543b8f90> \n",
      " torch.strided\n"
     ]
    }
   ],
   "source": [
    "# assign type and device\n",
    "a=a.to(float)\n",
    "a=a.to(dev)\n",
    "print(a,'\\n',a.type(),'\\n',a.shape,'\\n',a.size,'\\n',a.layout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:35:39.015822Z",
     "start_time": "2024-04-28T11:35:38.995576Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Tensor的运算\n",
    "### 基本运算\n",
    "- 加法：torch.add\n",
    "- 减法：torch.sub\n",
    "- 乘法：torch.mul，即哈达马积（element wise，点乘，对应元素相乘）\n",
    "- 除法：torch.div\n",
    "- 幂运算：torch.pow\n",
    "- 开方运算：a.sqrt();a.sqrt_()\n",
    "- 对数运算：torch.log\n",
    "\n",
    "---\n",
    "\n",
    "### 矩阵乘法\n",
    "$C_{ij} = \\sum_k A_{ik} B_{kj}$\n",
    "\n",
    "- torch.matmul\n",
    "- @\n",
    "\n",
    "对高维tensor，矩阵乘法仅在**最后的两个维度上**，要求前面的维度必须保持一致，就像矩阵的索引一样并且运算操作只有torch.matmul\n",
    "\n",
    "#### 应用场景\n",
    "- 通常在深度学习中用于计算层之间的连接。\n",
    "\n",
    "---\n",
    "\n",
    "### 外积(Cross Product)(叉积)\n",
    "外积是两个**向量**的乘积，结果中每个元素是A中的元素与B中的元素的乘积。$C_{ij} = a_i \\cdot b_j$\n",
    "- torch.outer\n",
    "#### 应用场景：\n",
    "- 构造协方差矩阵：在统计学中，可以通过外积来计算数据集的协方差矩阵。\n",
    "- 张量运算：在深度学习中，外积可用于生成高维张量，常见于一些特定的模型，如图神经网络（GNN）和强化学习。\n",
    "- 特征交互：在某些特征工程中，可以用外积来捕获特征之间的交互关系。\n",
    "\n",
    "---\n",
    "\n",
    "### 内积(Inner Product)(点积、点乘)\n",
    "内积是两个**向量**对应位置元素的乘积之和，结果为标量（一个数）。$\\text{dot}(a, b) = \\sum a_i \\cdot b_i $\n",
    "- torch.dot()\n",
    "#### 应用场景：\n",
    "- 计算相似性：在推荐系统中，内积常用于计算用户和物品之间的相似性。\n",
    "- 特征提取：在机器学习中，可以用内积来计算输入特征和权重之间的关系，帮助模型进行预测。\n",
    "- 神经网络：在全连接层中，输入向量和权重矩阵之间的计算通常是内积运算。\n",
    "\n",
    "---\n",
    "\n",
    "### 哈达玛(Hadamard)积(点乘)\n",
    "哈达玛积是两个具有相同维度的**矩阵**对应位置元素的乘积。$C_{ij} = A_{ij} \\cdot B_{ij}$\n",
    "- \\*\n",
    "- torch.mul\n",
    "#### 应用场景\n",
    "- 特征组合：在神经网络中，哈达玛积可以用于对特征进行组合，特别是在注意力机制和图卷积网络中。\n",
    "- 图像处理：在图像处理中，哈达玛积常用于调整图像的亮度、对比度等操作。\n",
    "- 神经网络中的激活函数：某些网络层（如元素级操作）会使用哈达玛积进行激活函数计算，允许模型进行非线性变换。\n",
    "\n",
    "---\n",
    "\n",
    "### Kronecker积\n",
    "如果 \\(A\\) 的维度为 \\(m \\times n\\)，而 \\(B\\) 的维度为 \\(p \\times q\\)，那么 \\(A\\) 和 \\(B\\) 的Kronecker积 \\(C = A \\otimes B\\) 将是一个维度为 \\(mp \\times nq\\) 的矩阵。具体来说，Kronecker积的每个元素 \\(C_{ij}\\) 是通过将 \\(A\\) 中的每个元素与整个矩阵 \\(B\\) 相乘得到的。\n",
    "#### 定义示例\n",
    "假设有两个矩阵：\n",
    "\\[ A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix},  B = \\begin{pmatrix} e & f \\\\ g & h \\end{pmatrix} \\]\n",
    "则它们的Kronecker积为：\n",
    "\\[ C = A $\\otimes$ B = \\begin{pmatrix} aB & bB \\\\ cB & dB \\end{pmatrix} = \\begin{pmatrix} ae & af & be & bf \\\\ ag & ah & bg & bh \\\\ ce & cf & de & df \\\\ cg & ch & dg & dh \\end{pmatrix} \\]\n",
    "Kronecker积是两个矩阵之间的一种张量积，它的结果是一个大的矩阵，由其中一个矩阵的每个元素与另一个矩阵的所有元素相乘组成。\n",
    "- torch.kron\n",
    "#### 应用场景\n",
    "- 信号处理：在滤波器设计中。\n",
    "- 图像处理：用于生成高分辨率图像。\n",
    "- 量子计算：在量子态的表示中。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]) \n",
      " tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "a+b:tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18]])\n",
      "a: tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "算术运算\n",
    "\"\"\"\n",
    "a = torch.tensor([[1, 2, 3],[4,5,6]])\n",
    "b = torch.tensor([[7, 8, 9],[10,11,12]])\n",
    "print(a,'\\n',b)\n",
    "print(f'a+b:{a+b}')\n",
    "a.add_(b)  # 修改a的值\n",
    "print('a:',a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T12:02:39.218216Z",
     "start_time": "2024-04-28T12:02:39.214508Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 3, 3])\n",
      "torch.Size([3, 1, 3, 3])\n",
      "True torch.Size([2, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "外积\n",
    "\n",
    "广播机制需满足的条件（满足一个即可）\n",
    "- 两个数组的后缘维度(trailing dimension)，即从末尾开始算起的维度的轴长度相符\n",
    "- 或其中的一方的长度为1\n",
    "\"\"\"\n",
    "a=torch.rand(2,1,3,4)\n",
    "b=torch.rand(1,1,4,3)\n",
    "c=torch.rand(3,1,4,3)\n",
    "d=torch.rand(3,1,3,4)\n",
    "\n",
    "# print(a@c)  # error 最高维度2与3不同，若相同或有一方为1,则可相乘\n",
    "\n",
    "print((d@b).shape)\n",
    "\n",
    "print((d@c).shape)\n",
    "\n",
    "print(torch.equal( a@b,a.matmul(b) ),(a@b).shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:10:53.925947Z",
     "start_time": "2024-04-28T13:10:53.922820Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.1606)\n",
      "tensor(217)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "内积\n",
    "- 需先将高维tensor转为一维，再进行计算\n",
    "\"\"\"\n",
    "\n",
    "a=torch.rand(3,1,4,3)\n",
    "b=torch.rand(3,1,3,4)\n",
    "\n",
    "c = torch.tensor([[1, 2, 3],[4,5,6]])\n",
    "d = torch.tensor([[7, 8, 9],[10,11,12]])\n",
    "\n",
    "print(torch.dot(a.view(-1, ),b.view(-1, )))\n",
    "\n",
    "print(torch.dot(c.view(-1,),d.view(-1,)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:21:20.256884Z",
     "start_time": "2024-04-28T13:21:20.253527Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7, 16, 27],\n",
      "        [40, 55, 72]])\n",
      "tensor([[ 7, 16, 27],\n",
      "        [40, 55, 72]])\n",
      "tensor([[ 7, 16, 27],\n",
      "        [40, 55, 72]])\n",
      "tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "哈达玛积(点乘)\n",
    "\"\"\"\n",
    "a = torch.tensor([[1, 2, 3],[4,5,6]])\n",
    "b = torch.tensor([[7, 8, 9],[10,11,12]])\n",
    "print(a*b)\n",
    "print(a.mul_(b))\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T12:05:32.792464Z",
     "start_time": "2024-04-28T12:05:32.788708Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kronecker积\n",
    "- 给定 任意两个矩阵，两者之间进行 Kronecker 积得到的是一个分块矩阵。\n",
    "\"\"\"\n",
    "mat1=torch.eye(3)\n",
    "mat2=torch.ones(3,3)\n",
    "\n",
    "print(mat1)\n",
    "print(mat2)\n",
    "\n",
    "# 将mat2整个填在mat1中值为1的位置\n",
    "print(torch.kron(mat1,mat2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:25:21.440311Z",
     "start_time": "2024-04-28T13:25:21.436367Z"
    }
   },
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor的取整/取余运算\n",
    "![](./imgs/1_8.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1762, 1.8254],\n",
      "        [8.7411, 1.3613]])\n",
      "tensor([[2., 1.],\n",
      "        [8., 1.]])\n",
      "tensor([[3., 2.],\n",
      "        [9., 2.]])\n",
      "tensor([[2., 2.],\n",
      "        [9., 1.]])\n",
      "tensor([[0.1762, 0.8254],\n",
      "        [0.7411, 0.3613]])\n",
      "tensor([[0.1762, 1.8254],\n",
      "        [0.7411, 1.3613]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(2,2)*10\n",
    "print(a)\n",
    "print(torch.floor(a))  # 向下取整\n",
    "print(torch.ceil(a))  # 向上取整\n",
    "print(torch.round(a))  # 四舍五入\n",
    "print(torch.frac(a))  # 取整数部分\n",
    "print(a%2)  # 除2取余数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor比较运算与排序\n",
    "![](./imgs/1_9.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2004, 0.3933, 0.1248],\n",
      "        [0.8212, 0.4767, 0.9203]])\n",
      "tensor([[0.9263, 0.5702, 0.4827],\n",
      "        [0.3942, 0.9593, 0.3970]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "False\n",
      "tensor([[False, False, False],\n",
      "        [ True, False,  True]])\n",
      "tensor([[False, False, False],\n",
      "        [ True, False,  True]])\n",
      "tensor([[ True,  True,  True],\n",
      "        [False,  True, False]])\n",
      "tensor([[ True,  True,  True],\n",
      "        [False,  True, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "相同shape比较\n",
    "\"\"\"\n",
    "a=torch.rand(2,3)\n",
    "b=torch.rand(2,3)\n",
    "print(a)\n",
    "print(b)\n",
    "print(torch.eq(a,b))\n",
    "print(torch.equal(a,b))\n",
    "\n",
    "print(torch.ge(a,b))\n",
    "print(torch.gt(a,b))\n",
    "print(torch.le(a,b))\n",
    "print(torch.lt(a,b))\n",
    "\n",
    "print(torch.ne(a,b))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./imgs/1_10.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.return_types.sort(\n",
      "values=tensor([[1, 3, 4, 4, 5],\n",
      "        [0, 1, 2, 3, 4]]),\n",
      "indices=tensor([[0, 3, 1, 2, 4],\n",
      "        [0, 2, 1, 3, 4]]))\n",
      "torch.return_types.sort(\n",
      "values=tensor([[5, 4, 4, 3, 1],\n",
      "        [4, 3, 2, 1, 0]]),\n",
      "indices=tensor([[4, 1, 2, 3, 0],\n",
      "        [4, 3, 1, 2, 0]]))\n",
      "torch.return_types.sort(\n",
      "values=tensor([[0, 2, 1, 3, 4],\n",
      "        [1, 4, 4, 3, 5]]),\n",
      "indices=tensor([[1, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 1, 0]]))\n",
      "torch.return_types.sort(\n",
      "values=tensor([[1, 3, 4, 4, 5],\n",
      "        [0, 1, 2, 3, 4]]),\n",
      "indices=tensor([[0, 3, 1, 2, 4],\n",
      "        [0, 2, 1, 3, 4]]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "排序\n",
    "\"\"\"\n",
    "\n",
    "a=torch.tensor([[1,4,4,3,5],\n",
    "                [0,2,1,3,4]])\n",
    "print(a.shape)\n",
    "print(torch.sort(a))  # 默认升序方式\n",
    "print(torch.sort(a,descending=True))  # 改为降序\n",
    "\n",
    "# torch.size(2,5)\n",
    "# dim=0,对2维度进行排序(两个(1,5)之间的比较)\n",
    "print(torch.sort(a,dim=0))  # 指定轴\n",
    "\n",
    "# dim=1，对5维度进行排序((5)内部的比较)\n",
    "print(torch.sort(a,dim=1))  # 指定轴"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[2, 4, 5, 1, 5],\n",
      "        [2, 3, 3, 1, 4]]),\n",
      "indices=tensor([[0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 1]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[5, 4],\n",
      "        [5, 4]]),\n",
      "indices=tensor([[4, 1],\n",
      "        [2, 4]]))\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([2, 4, 5, 1, 5]),\n",
      "indices=tensor([1, 0, 1, 1, 0]))\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([2, 2]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "topk\n",
    "- 返回最值\n",
    "\"\"\"\n",
    "a=torch.tensor([\n",
    "    [2,4,3,1,5],\n",
    "    [2,3,5,1,4]\n",
    "])\n",
    "print(a.shape)\n",
    "\n",
    "# 前k个最大值\n",
    "print(torch.topk(a,k=2,dim=0))\n",
    "print(torch.topk(a,k=2,dim=1))\n",
    "\n",
    "# k个最小值\n",
    "# 这里取第二小的\n",
    "print(torch.kthvalue(a,k=2,dim=0))\n",
    "print(torch.kthvalue(a,k=2,dim=1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor判定是否为finite/inf/nan\n",
    "![](./imgs/1_11.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7890, 0.2414, 0.7186],\n",
      "        [0.7817, 0.6854, 0.5243]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=torch.rand(2,3)\n",
    "print(a)\n",
    "print(torch.isfinite(a))\n",
    "print(torch.isfinite(a/0))\n",
    "print(torch.isinf(a/0))\n",
    "print(torch.isnan(a))\n",
    "\n",
    "b=torch.tensor([1,2,np.nan])\n",
    "print(torch.isnan(b))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 Pytorch中in-place操作与广播机制\n",
    "### in-place操作\n",
    "就地操作，即不允许使用临时变量，也称为原位操作。例如：add_、sub_、mul_等等\n",
    "### 广播机制\n",
    "\n",
    "若有矩阵a.shape=(m,n),b.shape=(1,n),则a-b时，b会被复制m次，成为(m,n)的矩阵，然后再逐元素进行减法操作。同样对(m,1)的矩阵成立。\n",
    "若a.shape=(1,2),b.shape=(2,3)，则a、b不满足广播机制。因为a中有个维度1，要想满足广播机制就必须是(1,2)和(2,2)，否则就需要满足维度必须相等(2,3)和(2,3)\n",
    "\n",
    "张量参数可以自动扩展为相同大小，广播机制需满足：\n",
    "- 每个张量至少有一个维度\n",
    "- 满足右对齐,例如：torch.rand(2,1,1)+torch.rand(3)，末位有一个是“1”，或者末位相等"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[0.0699, 0.2432, 0.1409],\n",
      "        [0.4260, 0.0686, 0.2563]])\n",
      "b: tensor([0.7613, 0.8781, 0.0762])\n",
      "c: tensor([[0.8311, 1.1213, 0.2170],\n",
      "        [1.1872, 0.9467, 0.3325]]) \n",
      " c.shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "广播机制\n",
    "\"\"\"\n",
    "a=torch.rand(2,3)\n",
    "b=torch.rand(3)\n",
    "c=a+b  # (2,3) + (1,3)\n",
    "print('a:',a)\n",
    "print('b:',b)\n",
    "print('c:',c,'\\n','c.shape:',c.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:28:23.796983Z",
     "start_time": "2024-04-28T13:28:23.792831Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9642],\n",
      "        [0.9135]])\n",
      "tensor([0.4446, 0.7081, 0.4873])\n",
      "tensor([[1.4088, 1.6723, 1.4516],\n",
      "        [1.3581, 1.6216, 1.4008]]) \n",
      " torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(2,1)\n",
    "b=torch.rand(3)\n",
    "c=a+b  # (2,1) + (1,3)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c,'\\n',c.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5604, 0.9429, 0.9213]]],\n",
      "\n",
      "\n",
      "        [[[0.1432, 0.3054, 0.1729]]]])\n",
      "tensor([[[2.3782e-01, 6.2080e-01, 1.9663e-02],\n",
      "         [6.2127e-01, 3.6706e-01, 8.0018e-01]],\n",
      "\n",
      "        [[4.6856e-01, 2.1447e-01, 8.7940e-01],\n",
      "         [8.9617e-01, 1.8870e-01, 9.4896e-01]],\n",
      "\n",
      "        [[5.4447e-01, 7.8154e-01, 1.1063e-04],\n",
      "         [2.8396e-01, 6.8833e-01, 5.2915e-01]],\n",
      "\n",
      "        [[7.2092e-01, 8.0755e-01, 9.9905e-01],\n",
      "         [4.7248e-01, 8.3102e-01, 9.9366e-01]]])\n",
      "tensor([[[[0.7982, 1.5637, 0.9410],\n",
      "          [1.1817, 1.3100, 1.7215]],\n",
      "\n",
      "         [[1.0290, 1.1574, 1.8007],\n",
      "          [1.4566, 1.1316, 1.8703]],\n",
      "\n",
      "         [[1.1049, 1.7245, 0.9214],\n",
      "          [0.8444, 1.6312, 1.4505]],\n",
      "\n",
      "         [[1.2813, 1.7505, 1.9204],\n",
      "          [1.0329, 1.7739, 1.9150]]],\n",
      "\n",
      "\n",
      "        [[[0.3810, 0.9262, 0.1925],\n",
      "          [0.7645, 0.6724, 0.9730]],\n",
      "\n",
      "         [[0.6118, 0.5199, 1.0523],\n",
      "          [1.0394, 0.4941, 1.1218]],\n",
      "\n",
      "         [[0.6877, 1.0869, 0.1730],\n",
      "          [0.4272, 0.9937, 0.7020]],\n",
      "\n",
      "         [[0.8641, 1.1129, 1.1719],\n",
      "          [0.6157, 1.1364, 1.1665]]]]) \n",
      " torch.Size([2, 4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "右对齐规则\n",
    "- 依次右对齐\n",
    "\"\"\"\n",
    "a=torch.rand(2,1,1,3)\n",
    "b=torch.rand(4,2,3)\n",
    "c=a+b  # (2,1,1,3) + (1,4,2,3)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c,'\\n',c.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6555, 0.5803, 0.0255]]],\n",
      "\n",
      "\n",
      "        [[[0.2541, 0.9762, 0.6395]]]])\n",
      "tensor([[[[0.6191, 0.4448, 0.5327],\n",
      "          [0.9799, 0.9297, 0.7650]],\n",
      "\n",
      "         [[0.9528, 0.1133, 0.1849],\n",
      "          [0.1179, 0.7933, 0.2089]],\n",
      "\n",
      "         [[0.2436, 0.1130, 0.3314],\n",
      "          [0.7020, 0.7686, 0.2757]],\n",
      "\n",
      "         [[0.2373, 0.7147, 0.4690],\n",
      "          [0.5175, 0.0516, 0.4548]]]])\n",
      "tensor([[[[1.2746, 1.0251, 0.5581],\n",
      "          [1.6354, 1.5100, 0.7905]],\n",
      "\n",
      "         [[1.6083, 0.6936, 0.2104],\n",
      "          [0.7733, 1.3736, 0.2344]],\n",
      "\n",
      "         [[0.8991, 0.6934, 0.3569],\n",
      "          [1.3575, 1.3490, 0.3011]],\n",
      "\n",
      "         [[0.8928, 1.2951, 0.4945],\n",
      "          [1.1730, 0.6319, 0.4802]]],\n",
      "\n",
      "\n",
      "        [[[0.8733, 1.4209, 1.1721],\n",
      "          [1.2341, 1.9059, 1.4045]],\n",
      "\n",
      "         [[1.2070, 1.0894, 0.8244],\n",
      "          [0.3720, 1.7694, 0.8484]],\n",
      "\n",
      "         [[0.4978, 1.0892, 0.9709],\n",
      "          [0.9562, 1.7448, 0.9151]],\n",
      "\n",
      "         [[0.4915, 1.6909, 1.1085],\n",
      "          [0.7716, 1.0278, 1.0942]]]]) \n",
      " torch.Size([2, 4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "右对齐规则\n",
    "- 依次右对齐\n",
    "\"\"\"\n",
    "a=torch.rand(2,1,1,3)\n",
    "b=torch.rand(1,4,2,3)\n",
    "c=a+b  # (2,1,1,3) + (1,4,2,3)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c,'\\n',c.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:29:45.899430Z",
     "start_time": "2024-04-28T13:29:45.895896Z"
    }
   },
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m a\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m      6\u001B[0m b\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m4\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m c\u001B[38;5;241m=\u001B[39m\u001B[43ma\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43mb\u001B[49m  \u001B[38;5;66;03m# (2,1,1,3) + (1,4,2,3)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(a)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(b)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "右对齐规则(error)\n",
    "- 依次右对齐\n",
    "\"\"\"\n",
    "a=torch.rand(2,1,1,3)\n",
    "b=torch.rand(3,4,2,3)\n",
    "c=a+b  # (2,1,1,3) + (1,4,2,3)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c,'\\n',c.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:29:26.289824Z",
     "start_time": "2024-04-28T13:29:26.273091Z"
    }
   },
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.6 Tensor的三角函数\n",
    "![](./imgs/1_12.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3732, 0.9598, 0.8095],\n",
      "        [0.4267, 0.0257, 0.0756]])\n",
      "tensor([[0.9312, 0.5737, 0.6899],\n",
      "        [0.9103, 0.9997, 0.9971]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(2,3)\n",
    "print(a)\n",
    "print(torch.cos(a))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.7 Tensor中其他的数学函数\n",
    "![](./imgs/1_13.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
