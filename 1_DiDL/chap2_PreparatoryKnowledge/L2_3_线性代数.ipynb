{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2.3 线性代数\n",
    "## 2.3.1 标量\n",
    "- 仅包含一个数值的叫标量（scalar）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x ** y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.2 向量\n",
    "- 可以将向量视为标量值组成的列表\n",
    "    - 将这些标量值称为向量的元素（element）或分量（component）\n",
    "- 维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。\n",
    "    - 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。\n",
    "    - 张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4)\n",
    "x,x[3]  # (tensor([0, 1, 2, 3]), tensor(3))\n",
    "len(x)  # 获取张量的长度，4\n",
    "x.shape  # torch.Size([4])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.3 矩阵\n",
    "- 尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中， 将每个数据样本作为矩阵中的行向量更为常见。\n",
    "    - 例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(20).reshape(5,4)\n",
    "A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  4,  8, 12, 16],\n        [ 1,  5,  9, 13, 17],\n        [ 2,  6, 10, 14, 18],\n        [ 3,  7, 11, 15, 19]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.4 张量\n",
    "- 张量为我们提供了描述具有任意数量轴的n维数组的通用方法。\n",
    "- 当我们开始处理图像时，张量将变得更加重要，图像以n维数组形式出现， 其中3个轴对应于高度、宽度，以及一个通道（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15]],\n\n        [[16, 17, 18, 19],\n         [20, 21, 22, 23],\n         [24, 25, 26, 27],\n         [28, 29, 30, 31]],\n\n        [[32, 33, 34, 35],\n         [36, 37, 38, 39],\n         [40, 41, 42, 43],\n         [44, 45, 46, 47]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.arange(48).reshape(3,4,4)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.5 张量算法的基本性质\n",
    "- 任何按元素的一元运算都不会改变其操作数的形状\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.],\n         [16., 17., 18., 19.]]),\n tensor([[ 0.,  2.,  4.,  6.],\n         [ 8., 10., 12., 14.],\n         [16., 18., 20., 22.],\n         [24., 26., 28., 30.],\n         [32., 34., 36., 38.]]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量\n",
    "\"\"\"\n",
    "A=torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "B=A.clone()\n",
    "A,A+B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0.,   1.,   4.,   9.],\n        [ 16.,  25.,  36.,  49.],\n        [ 64.,  81., 100., 121.],\n        [144., 169., 196., 225.],\n        [256., 289., 324., 361.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "两个矩阵的按元素乘法称为Hadamard积（Hadamard product）\n",
    "\"\"\"\n",
    "A*B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n torch.Size([2, 3, 4]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。\n",
    "\"\"\"\n",
    "a=2\n",
    "X=torch.arange(24).reshape(2,3,4)\n",
    "a+X,(a*X).shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.6 降维"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 1., 2., 3.]), tensor(6.))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4,dtype=torch.float32)  # 默认生成行向量，但可以reshape\n",
    "x,x.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15],\n         [16, 17, 18, 19]]),\n torch.Size([5, 4]),\n tensor(190),\n tensor(190))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(20).reshape(5,4)\n",
    "A,A.shape,A.sum(),A.sum(axis=[0,1])  #沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 6, 22, 38, 54, 70]), torch.Size([5]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。\n",
    "竖0横1\n",
    "\"\"\"\n",
    "A_sum_axis0=A.sum(axis=0)  # 按列求值\n",
    "A_sum_axis0,A_sum_axis0.shape  # (tensor([40, 45, 50, 55]), torch.Size([4]))\n",
    "A_sum_axis1=A.sum(axis=1)  # 按行求值\n",
    "A_sum_axis1,A_sum_axis1.shape  # (tensor([ 6, 22, 38, 54, 70]), torch.Size([5]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([4., 5., 6., 7.]), tensor([4., 5., 6., 7.]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "计算任意形状张量的平均值\n",
    "\"\"\"\n",
    "A=torch.arange(12,dtype=torch.float32).reshape(3,4)\n",
    "A.mean(),A.sum()/A.numel() # (tensor(5.5000), tensor(5.5000))\n",
    "# 计算平均值的函数也可以沿指定轴降低张量的维度\n",
    "A.mean(axis=0),A.sum(axis=0) / A.shape[0] # (tensor([4., 5., 6., 7.]), tensor([4., 5., 6., 7.]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 6.],\n         [22.],\n         [38.]]),\n tensor([ 6., 22., 38.]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "非降维求和\n",
    "- 有时在调用函数来计算总和或均值时保持轴数不变会很有用\n",
    "\"\"\"\n",
    "sum_A=A.sum(axis=1,keepdims=True) # 按行求和后仍保持两个轴（二维）\n",
    "sum_B=A.sum(axis=1)\n",
    "sum_A,sum_B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n        [0.1818, 0.2273, 0.2727, 0.3182],\n        [0.2105, 0.2368, 0.2632, 0.2895]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。\n",
    "A/sum_A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  3.,  6.],\n        [ 4.,  9., 15., 22.],\n        [ 8., 17., 27., 38.]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "如果我们想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），我们可以调用cumsum函数。 此函数不会沿任何轴降低输入张量的维度。\n",
    "\"\"\"\n",
    "A.cumsum(axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.7 点积（Dot Product）\n",
    "- 相同位置的按元素乘积的和\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.), tensor(6.))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4,dtype=torch.float32)\n",
    "y=torch.ones(4,dtype=torch.float32)\n",
    "x,y,torch.dot(x,y),torch.sum(x*y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.8 矩阵-向量积"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([0., 1., 2., 3.]),\n torch.Size([3, 4]),\n torch.Size([4]),\n tensor([14., 38., 62.]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "在代码中使用张量表示矩阵-向量积，我们使用与点积相同的mv函数。 当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。\n",
    "\"\"\"\n",
    "A,x,A.shape,x.shape,torch.mv(A,x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.9. 矩阵-矩阵乘法\n",
    "- torch.mm(): 只能进行矩阵乘法,也就是输入的两个tensor维度只能是(n×m)和(m×p)\n",
    "- torch.bmm(): 是两个三维张量相乘, 两个tensor维度是(b×m×p) 和(b×n×p) , 第一维b代表batch_size\n",
    "- torch.matmul(): 可以进行张量乘法, 输入可以是高维"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]),\n tensor([[ 6.,  6.,  6.],\n         [22., 22., 22.],\n         [38., 38., 38.]]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.ones(4,3)\n",
    "A,B,torch.mm(A,B)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 1.2869e+00,  5.4340e-01, -6.4900e-01,  1.2655e+00,  1.1981e+00],\n          [-3.1634e+00,  4.8054e-01, -1.2489e-01,  3.0501e+00, -2.9527e+00],\n          [-1.6298e+00,  1.9307e-01, -3.1301e+00,  5.0450e+00, -3.8292e-01]],\n \n         [[ 1.8945e+00, -4.7095e-02,  1.6081e+00,  7.1716e-01,  2.5216e-01],\n          [ 1.0643e+00,  9.2610e-01, -1.4878e-01,  1.8059e+00,  1.9130e+00],\n          [ 1.1179e+00, -7.0503e-02, -2.3722e+00,  1.0583e+00,  2.1801e+00]],\n \n         [[-1.3917e-02,  1.6012e-01, -9.0822e-01, -1.8466e+00,  1.6212e+00],\n          [ 5.6102e-01, -3.1688e+00, -2.1476e-01, -3.1322e-03,  1.2873e+00],\n          [ 1.5051e+00, -2.2002e+00,  7.1345e-01,  2.5670e+00,  1.7332e+00]],\n \n         [[-7.5528e-01,  6.0891e-01,  2.8982e-01,  1.0778e+00, -2.3907e+00],\n          [ 1.0734e+00,  1.6837e+00, -1.8090e+00, -2.3877e-01, -1.2446e+00],\n          [ 1.1582e+00, -1.6925e+00,  2.8930e+00,  1.8467e+00,  1.1211e+00]],\n \n         [[ 2.5888e+00, -5.0152e+00,  2.6160e+00,  1.1441e+00,  7.2456e-01],\n          [-1.9572e+00,  2.1125e+00, -1.1206e+00,  1.5997e+00, -2.4097e+00],\n          [ 6.6801e-01,  9.2122e-02, -9.0471e-01, -1.8057e+00,  1.4809e+00]],\n \n         [[ 2.0268e-01, -3.9063e-01, -1.7120e-01,  1.0413e+00,  1.9116e-01],\n          [-8.9325e-02, -8.9054e-01,  3.3840e-01,  5.3688e-01, -1.0564e+00],\n          [ 6.7054e-01, -1.9339e+00,  5.5320e-01,  2.9096e+00, -4.0401e-01]],\n \n         [[-6.4870e-01, -1.2141e+00,  8.6926e-01, -1.7391e+00, -8.6232e-01],\n          [ 1.1390e+00,  6.1052e-01, -1.4526e+00, -2.2673e+00, -2.5732e+00],\n          [-1.6965e+00,  3.4956e+00, -3.0414e+00,  6.1463e+00,  4.5616e+00]],\n \n         [[-5.6787e+00,  6.5832e-01,  1.1653e+00,  1.8558e+00, -1.2363e+00],\n          [ 4.2518e+00, -5.4422e+00, -5.7706e+00, -2.6634e+00,  1.5825e+00],\n          [ 3.1220e-01, -6.5502e-01, -6.0201e-01, -4.4189e-01, -1.3735e-01]],\n \n         [[ 6.1386e-01,  1.9862e+00, -1.7801e+00,  1.9913e+00, -3.3638e-01],\n          [-3.5478e+00,  6.3294e-01, -3.3310e+00,  3.1002e+00,  3.2101e+00],\n          [-6.9405e-03, -3.9449e-01,  1.3419e+00,  1.3490e+00, -9.9086e-01]],\n \n         [[-1.7330e+00,  7.2751e-01,  3.7439e-01,  1.7233e+00,  1.5184e+00],\n          [-1.0032e+00,  3.5634e-01, -6.2031e-01, -4.6259e-01, -8.0181e-01],\n          [-2.4500e-01, -2.1407e+00,  2.3585e-01,  5.0507e-01,  1.2090e-01]]]),\n torch.Size([10, 3, 5]))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(10, 3, 4)\n",
    "mat2 = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(input, mat2)\n",
    "res,res.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-0.8106, -0.2337,  2.9683,  0.1881, -1.5804],\n          [-2.2073,  1.6057,  0.8353, -1.4305, -0.8777],\n          [-0.1955, -1.7692, -1.7137, -0.0165,  2.8448]],\n \n         [[-0.0710, -0.2827,  1.4211,  0.4182, -0.5209],\n          [-2.6890,  1.3873,  0.2829, -1.6331,  0.2025],\n          [ 1.3819,  0.0702,  1.2778,  1.2875, -1.0235]],\n \n         [[ 1.1067,  0.7503, -0.4365,  0.5425, -0.4199],\n          [-0.2693, -1.3357,  1.6641,  0.5138,  0.0794],\n          [-0.6061,  1.0902,  1.3000, -0.5112, -1.9469]],\n \n         [[ 1.0677, -1.5071, -3.0443,  0.4310,  3.1445],\n          [ 2.6681, -0.8588, -1.3872,  1.6036,  0.8767],\n          [-1.2608, -1.7937,  0.0459, -0.4050,  1.7671]],\n \n         [[-2.1911, -0.1293, -1.7103, -1.4113,  2.6390],\n          [-0.2113,  0.2995,  2.9220,  0.3037, -2.4575],\n          [-0.2887,  0.8255,  0.6270, -0.1782, -0.8921]],\n \n         [[-1.3978, -0.2046, -0.1452, -0.6965,  1.0732],\n          [-0.7633,  0.3200, -1.6520, -0.8019,  1.3471],\n          [ 1.5253,  0.6420, -0.8197,  0.8889,  0.1077]],\n \n         [[-0.2793, -0.7137, -1.7381, -0.2874,  2.0845],\n          [ 1.1062, -0.4336, -2.3479,  0.4441,  1.9577],\n          [-1.2732, -1.4484, -0.6188, -0.4922,  2.2561]],\n \n         [[ 0.1164, -0.1625,  0.7243,  0.2663, -0.4010],\n          [ 2.0540,  0.6374,  1.0203,  1.1747, -2.2888],\n          [-0.6039,  0.3185, -1.1841, -0.5663,  1.0513]],\n \n         [[-0.0397,  0.1057,  2.2742,  0.4045, -1.7154],\n          [-0.3108, -0.1509, -1.9166, -0.6579,  1.3544],\n          [-0.1664, -0.1036, -1.4580, -0.2955,  1.3561]],\n \n         [[ 0.4841, -0.5980,  1.9304,  0.7720, -1.1738],\n          [-0.9844,  0.6464, -1.5867, -0.9000,  1.3258],\n          [-0.3454, -1.0414,  0.8756,  0.1112,  0.1469]]]),\n torch.Size([10, 3, 5]))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "res = torch.matmul(tensor1, tensor2)\n",
    "res,res.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.10. 范数(norm)\n",
    "- 非正式地说，一个向量的范数告诉我们一个向量有多大。\n",
    "- 在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(5.)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u=torch.tensor([3.0,-4.0])\n",
    "torch.norm(u) # 计算向量的L2范数,即向量的长度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7.)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了计算L1范数，我们将绝对值函数和按元素求和组合起来。\n",
    "torch.abs(u).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6.)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "可用于 利用低秩矩阵来近似单一数据矩阵。\n",
    "用数学表示就是去找一个秩为k的矩阵B，使得矩阵B与原始数据矩阵A的差的F范数尽可能地小。\n",
    "\"\"\"\n",
    "\n",
    "# 矩阵A的Frobenius范数定义为矩阵A各项元素的平方的总和的开根\n",
    "# 以下函数将计算矩阵的Frobenius范数\n",
    "\n",
    "torch.norm(torch.ones((4,9)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
