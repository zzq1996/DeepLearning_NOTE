{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2.1. Data manipulation\n",
    "- 首先，我们介绍n维数组，也称为张量（tensor）。\n",
    "无论使用哪个深度学习框架，它的张量类（在MXNet中为ndarray， 在PyTorch和TensorFlow中为Tensor）都与Numpy的ndarray类似。\n",
    "- 但深度学习框架又比Numpy的ndarray多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。\n",
    "\n",
    "## 2.1.1 Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n"
     ]
    },
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12)  # 使用 arange 创建一个行向量 x, x= tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
    "print(x.shape)  # torch.Size([12])\n",
    "x.numel()  # 张量中元素的总数(形状的所有元素乘积), 12"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 1, 4, 3],\n        [1, 2, 3, 4],\n        [4, 3, 2, 1]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "通过-1来调用此自动计算出维度的功能:\n",
    "    X=x.reshape(-1,4)\n",
    "    X=x.reshape(3,-1)\n",
    "\"\"\"\n",
    "\n",
    "X = x.reshape(3, 4)  # X=tensor([[ 0,  1,  2,  3],\n",
    "                                # [ 4,  5,  6,  7],\n",
    "                                # [ 8,  9, 10, 11]])\n",
    "\n",
    "# 创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。\n",
    "torch.zeros((2, 3, 4))\n",
    "\n",
    "# 创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。\n",
    "torch.ones((2, 3, 4))\n",
    "\n",
    "# 创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。\n",
    "torch.randn(3, 4)\n",
    "\n",
    "# 通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。\n",
    "torch.tensor([[2, 1, 4, 3],\n",
    "              [1, 2, 3, 4],\n",
    "              [4, 3, 2, 1]\n",
    "              ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.2. Operations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、*、/和**）都可以被升级为按元素运算。\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2,2,2,2,])\n",
    "x+y  # (tensor([ 3.,  4.,  6., 10.])\n",
    "x-y  # tensor([-1.,  0.,  2.,  6.]),\n",
    "x*y  # tensor([ 2.,  4.,  8., 16.])\n",
    "x/y  # tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
    "x**y  # tensor([ 1.,  4., 16., 64.])\n",
    "torch.exp(x)  # tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n",
    "\n",
    "# 沿行（dim=0, 轴-0，形状的第一个元素） 和按列（dim=1, 轴-1，形状的第二个元素）连结两个矩阵\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n",
    "\n",
    "# 通过逻辑运算符构建二元张量。\n",
    "B = torch.tensor([[False,True,False,True],\n",
    "                  [False,False,False,False],\n",
    "                  [False,False,False,False]])\n",
    "B.sum()  # tensor(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.3. Broadcasting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1],\n        [1, 2],\n        [2, 3]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3,1))  # tensor([[0],\n",
    "                                           # [1],\n",
    "                                           # [2]])\n",
    "b = torch.arange(2).reshape((1,2))  # tensor([[0],\n",
    "                                           # [1]])\n",
    "\"\"\"\n",
    "矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。\n",
    "\n",
    "tensor([[0, 1],\n",
    "        [1, 2],\n",
    "        [2, 3]])\n",
    "\"\"\"\n",
    "a+b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.4. Indexing and Slicing\n",
    "- 第一个元素的索引是0，最后一个元素索引是-1\n",
    "- \":\":range of row/cell\n",
    "- (StartRow:numOfRow , StartCell:numOfCell)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[12, 12, 12, 12],\n        [12, 12, 12, 12],\n        [ 8,  9, 10, 11]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.arange(12).reshape((3,4))\n",
    "X[-1]  # 用[-1]选择最后一个元素, tensor([ 8,  9, 10, 11])\n",
    "X[1:3]  # 用[1:3]选择第二个和第三个元素, tensor([[ 4.,  5.,  6.,  7.],\n",
    "                                        #  [ 8.,  9., 10., 11.]])\n",
    "X[1, 3]  # 指定索引, tensor(7)\n",
    "\n",
    "X[0:2,:]=12\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.5. Saving Memory\n",
    "- 运行一些操作可能会导致为新结果分配内存\n",
    "    - 用Python的id()函数演示了这一点\n",
    "- 首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=torch.arange(3).reshape((3,1))\n",
    "before = id(Y)\n",
    "Y=X+Y\n",
    "id(Y)==before"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 140601383282912\n",
      "id(Z): 140601383282912\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "使用切片表示法将操作的结果分配给先前分配的数组，执行原地操作\n",
    "- define a tensor that equal the result's shape\n",
    "\"\"\"\n",
    "Z=torch.zeros_like(X)\n",
    "print('id(Z):',id(Z))\n",
    "Z[:]=X+Y\n",
    "print('id(Z):',id(Z))\n",
    "\n",
    "# 如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。\n",
    "before=id(X)\n",
    "X+=Y  # X[:]=X+Y\n",
    "id(X)==before"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.6 Conversion to Other Python Objects"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(numpy.ndarray, torch.Tensor)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=X.numpy()\n",
    "B=torch.from_numpy(A)\n",
    "type(A),type(B)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([3.5000]), 3.5, 3.5, 3)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.7 小结\n",
    "- 深度学习存储和操作数据的主要接口是张量（维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
