{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2.5 自动微分\n",
    "- 深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导\n",
    "- 实际中，根据我们设计的模型，系统会构建一个计算图（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出\n",
    "## 2.5.1 例子\n",
    "对函数y=2x<sup>T</sup>x关于列向量x求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 2., 3.])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.arange(4.0) # create a row vector\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(28., grad_fn=<MulBackward0>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "在我们计算y关于x的梯度之前，我们需要一个地方来存储梯度。\n",
    "\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "\n",
    "注意，一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。\n",
    "\"\"\"\n",
    "\n",
    "x.requires_grad=True  # 等价于 x = torch.arange(4.0,requires_grad=True)，需要计算梯度\n",
    "x.grad  # 默认值是None\n",
    "\n",
    "# torch.dot(x,x)结果为标量14(0+1+4+9)\n",
    "y = 2 * torch.dot(x,x)  # y = 2 * (x · x) = 2 * sum(x_i^2)\n",
    "y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.,  4.,  8., 12.])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。\n",
    "\"\"\"\n",
    "y.backward()\n",
    "x.grad\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "验证梯度是否计算正确\n",
    "\"\"\"\n",
    "x.grad==4*x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(6., grad_fn=<SumBackward0>), tensor([1., 1., 1., 1.]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "\"\"\"\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "y,x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.5.2 标量变量的反向传播\n",
    "\n",
    "---\n",
    "\n",
    "## 问题复述：\n",
    "\n",
    "我们有以下 PyTorch 代码：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.arange(4.0, requires_grad=True)  # x = [0., 1., 2., 3.]\n",
    "y = 2 * torch.dot(x, x)                    # y = 2 * (x · x) = 2 * sum(x_i^2)\n",
    "```\n",
    "\n",
    "我们的问题是：**如何计算 y 关于 x 的梯度 ∂y/∂x\\_i？**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1：数学上先推导目标\n",
    "\n",
    "设：\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "那么：\n",
    "\n",
    "$$\n",
    "y = 2 \\cdot (x_0^2 + x_1^2 + x_2^2 + x_3^2)\n",
    "= 2 \\sum_{i=0}^{3} x_i^2\n",
    "$$\n",
    "\n",
    "对每个分量求导：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = 2 \\cdot \\frac{d}{dx_i}(x_i^2) = 2 \\cdot 2x_i = 4x_i\n",
    "$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\n",
    "\\nabla_x y = \\left[ 4x_0, 4x_1, 4x_2, 4x_3 \\right]\n",
    "= 4 \\cdot x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2：在 PyTorch 中验证这个计算\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 定义变量\n",
    "x = torch.arange(4.0, requires_grad=True)  # x = tensor([0., 1., 2., 3.], requires_grad=True)\n",
    "\n",
    "# 构造函数 y = 2 * dot(x, x) = 2 * sum(x_i^2)\n",
    "y = 2 * torch.dot(x, x)  # y = 2*(0^2 + 1^2 + 2^2 + 3^2) = 2 * 14 = 28\n",
    "\n",
    "# 反向传播计算梯度\n",
    "y.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(x.grad)  # 输出：tensor([ 0.,  4.,  8., 12.])\n",
    "```\n",
    "\n",
    "这和我们数学计算的 $4 \\cdot x = [0., 4., 8., 12.]$ 完全一致 ✅\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3：逐行解释代码及工作原理\n",
    "\n",
    "| 代码                   | 含义                                                  |\n",
    "| -------------------- | --------------------------------------------------- |\n",
    "| `torch.arange(4.0)`  | 构造一个一维张量 `[0., 1., 2., 3.]`                         |\n",
    "| `requires_grad=True` | 告诉 PyTorch 跟踪这个变量的计算图，允许求导                          |\n",
    "| `torch.dot(x,x)`     | 执行内积（点积）：即 $\\sum x_i^2$                             |\n",
    "| `2 * torch.dot(x,x)` | 构造函数 $y = 2 \\sum x_i^2$                             |\n",
    "| `y.backward()`       | 启动自动反向传播，从 y 开始逐层反向计算每个变量的梯度                        |\n",
    "| `x.grad`             | 读取 x 的每个元素对 y 的梯度 $\\frac{\\partial y}{\\partial x_i}$ |\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4：验证 autograd 做了什么\n",
    "\n",
    "`y.backward()` 实际上在做：\n",
    "\n",
    "* 先从最终的标量 `y` 反向传播；\n",
    "* 追踪图中所有对 `x` 的操作（此处是 `dot` 和乘法）；\n",
    "* 对每一条计算路径应用**链式法则（chain rule）**；\n",
    "* 最终得到 `x.grad[i] = ∂y/∂x[i]`\n",
    "\n",
    "---\n",
    "\n",
    "## 小结（适合笔记整理）\n",
    "\n",
    "> 在 PyTorch 中，自动微分框架 `autograd` 可以通过 `.backward()` 方法对标量函数自动求梯度。当构造函数 $y = 2 \\cdot \\text{dot}(x,x)$ 时，实际数学含义为：\n",
    ">\n",
    "> $$\n",
    "> y = 2 \\sum x_i^2 \\quad \\Rightarrow \\quad \\frac{\\partial y}{\\partial x_i} = 4x_i\n",
    "> $$\n",
    ">\n",
    "> 使用 `requires_grad=True` 追踪张量后，调用 `y.backward()` 可以自动得到每个输入变量的梯度，并保存在 `x.grad` 中，精确符合手动推导结果。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5.3 非标量变量的反向传播\n",
    "\n",
    "##  定义\n",
    "\n",
    "在深度学习中，**反向传播（backpropagation）**的目标是计算函数的**梯度（gradient）**，即输出对输入变量的偏导数。\n",
    "\n",
    "当你使用 `.backward()` 方法时：\n",
    "\n",
    "* **标量变量**（如：损失函数） → 可以**直接调用 `y.backward()`**；\n",
    "* **非标量变量**（如：向量、矩阵） → 必须**提供 `gradient=` 参数**，即对该张量施加的外部“权重”导数。\n",
    "\n",
    "---\n",
    "\n",
    "##  本质：非标量变量没有“单一方向”的梯度\n",
    "\n",
    "### 标量情况（容易）：\n",
    "\n",
    "例如：\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.dot(x, x)   # y 是标量\n",
    "y.backward()\n",
    "```\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\left[ \\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}, \\dots \\right]\n",
    "$$\n",
    "\n",
    "这是非常标准的“从一个标量出发”的反向传播，返回的是梯度向量。\n",
    "\n",
    "---\n",
    "\n",
    "### 非标量情况（难点）：\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2             # y 是向量 [2.0, 4.0, 6.0]\n",
    "y.backward()          # ❌ 会报错\n",
    "```\n",
    "\n",
    "因为：\n",
    "\n",
    "* `y` 是一个**向量**，包含多个输出；\n",
    "* 那么，**“哪个方向”来传播导数呢？**\n",
    "* **你没有定义一个标量损失**，所以 PyTorch 不知道该如何聚合这些输出。\n",
    "\n",
    "---\n",
    "\n",
    "##  数学解释：雅可比矩阵与链式法则\n",
    "\n",
    "设：\n",
    "\n",
    "* $\\mathbf{x} \\in \\mathbb{R}^n$\n",
    "* $\\mathbf{y} = f(\\mathbf{x}) \\in \\mathbb{R}^m$\n",
    "\n",
    "那么：\n",
    "\n",
    "* 输出对输入的梯度是一个 **雅可比矩阵（Jacobian）**：\n",
    "\n",
    "$$\n",
    "J = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "当你调用 `.backward(v)` 时，其实是执行了链式法则中的：\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx} = v^\\top \\cdot \\frac{\\partial y}{\\partial x}\n",
    "$$\n",
    "\n",
    "其中 $v \\in \\mathbb{R}^{m}$ 是你提供的 `gradient=` 参数，相当于外部损失函数对 $y$ 的导数。\n",
    "\n",
    "---\n",
    "\n",
    "##  PyTorch 中的实际操作示例\n",
    "\n",
    "###  示例 1：非标量输出 + `.backward(gradient=...)`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2                        # y = [2.0, 4.0, 6.0]\n",
    "\n",
    "v = torch.tensor([1.0, 1.0, 1.0])  # 外部“损失对 y 的导数”： ∂L/∂y\n",
    "y.backward(gradient=v)            # 显式指定方向\n",
    "\n",
    "print(x.grad)  # 输出：tensor([2., 2., 2.])，因为 dy/dx = 2\n",
    "```\n",
    "\n",
    "你告诉系统：假设外部损失函数是 $L = \\sum y_i$，那么我们对每个 $y_i$ 的梯度是 1，这就能回传到 $x$ 上。\n",
    "\n",
    "---\n",
    "\n",
    "###  示例 2：更复杂的例子\n",
    "\n",
    "```python\n",
    "x = torch.tensor([1., 2.], requires_grad=True)\n",
    "y = torch.stack([x[0]**2, x[1]**3])     # y ∈ ℝ²，y = [x₀², x₁³]\n",
    "\n",
    "v = torch.tensor([1., 1.])             # ∂L/∂y 假设都是 1\n",
    "\n",
    "y.backward(v)\n",
    "\n",
    "# x.grad[0] = ∂L/∂x₀ = ∂L/∂y₀ · ∂y₀/∂x₀ = 1 · 2x₀ = 2\n",
    "# x.grad[1] = ∂L/∂x₁ = ∂L/∂y₁ · ∂y₁/∂x₁ = 1 · 3x₁² = 12\n",
    "\n",
    "print(x.grad)  # 输出：tensor([2., 12.])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  常见误区\n",
    "\n",
    "| 误区                    | 正确理解                    |\n",
    "| --------------------- | ----------------------- |\n",
    "| “非标量也能直接 backward”    |  只有标量才能直接调用 `.backward()` |\n",
    "| “自动微分总是能自动处理”         |  非标量必须提供 `gradient=`    |\n",
    "| “gradient 参数必须是 1 向量” |  不一定，可以是任何方向上的导数向量      |\n",
    "\n",
    "---\n",
    "\n",
    "##  总结（建议笔记整理）\n",
    "\n",
    "> 在 PyTorch 中，只有标量张量才能直接调用 `.backward()` 来自动反向传播。\n",
    "> 当输出变量是非标量（如向量或矩阵）时，系统无法确定从哪个方向传播导数，必须手动提供一个与输出形状相同的权重张量 `gradient=`。\n",
    "> 该张量表示“损失函数对输出变量的导数”，从而触发链式法则中雅可比矩阵乘法，完成反向传播。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 2., 4., 6.])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()  # 等价于y.backward(torch.ones(len(x)))\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5.3 分离计算\n",
    "- 将某些计算移动到记录的计算图之外。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。\n",
    "\"\"\"\n",
    "x.grad.zero_()  # 清0梯度\n",
    "y = x * x\n",
    "u = y.detach()  # u不再是关于x的函数，而是一个常数\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。\n",
    "\"\"\"\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5.4. Python控制流的梯度计算\n",
    "- 使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(-9.0155e-05, requires_grad=True), tensor(False))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。\n",
    "\"\"\"\n",
    "def f(a):\n",
    "    b = a*2\n",
    "    # print(b)\n",
    "    while b.norm()<1000:\n",
    "        b = b*2\n",
    "        # print(b)\n",
    "    if b.sum() > 0:\n",
    "        c=b\n",
    "    else:\n",
    "        c = 100*b\n",
    "    return c\n",
    "\n",
    "# 计算梯度\n",
    "a=torch.randn(size=(),requires_grad=True)  # 定义a为一个随机数\n",
    "d=f(a)\n",
    "d.backward()\n",
    "\"\"\"\n",
    "我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a。 因此，我们可以用d/a验证梯度是否正确。\n",
    "\"\"\"\n",
    "a,a.grad==d/a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5.5 小结\n",
    "- 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Q1 哪些是标量损失函数？哪些是非标量损失函数？他们是怎么工作的？\n",
    "\n",
    "## 一、什么是“标量损失函数”？\n",
    "\n",
    "### 定义：\n",
    "\n",
    "> 标量损失函数是指：模型输出通过损失函数后，返回一个**单个实数值（scalar）**，表示整个样本或样本批次的“预测误差”。\n",
    "\n",
    "### 常见例子：\n",
    "\n",
    "| 损失函数                                 | 输出形状     | 含义      |\n",
    "| ------------------------------------ | -------- | ------- |\n",
    "| `torch.nn.MSELoss(reduction='mean')` | `scalar` | 平均平方误差  |\n",
    "| `torch.nn.CrossEntropyLoss()`        | `scalar` | 交叉熵损失   |\n",
    "| `torch.nn.NLLLoss()`                 | `scalar` | 负对数似然损失 |\n",
    "| `torch.nn.BCELoss()`                 | `scalar` | 二分类交叉熵  |\n",
    "\n",
    "### 特点：\n",
    "\n",
    "* 可直接调用 `.backward()`：\n",
    "\n",
    "  ```python\n",
    "  loss = criterion(pred, target)\n",
    "  loss.backward()  # ✔️ 因为 loss 是 scalar\n",
    "  ```\n",
    "* 训练时几乎总是使用标量损失函数；\n",
    "* 是深度学习训练的**默认模式**。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、什么是“非标量损失函数”？\n",
    "\n",
    "### 定义：\n",
    "\n",
    "> 非标量损失函数是指：**每个样本**或**每个输出位置**都有自己的损失值，损失函数输出的是一个**向量或矩阵（tensor）**。\n",
    "\n",
    "例如：\n",
    "\n",
    "* 返回 shape = `[batch_size]`：每个样本一个损失；\n",
    "* 返回 shape = `[batch_size, C, H, W]`：用于像素级别监督任务。\n",
    "\n",
    "\n",
    "### 常见例子：\n",
    "\n",
    "| 损失函数                                     | 设置          | 输出             | 场景        |\n",
    "| ---------------------------------------- | ----------- | -------------- | --------- |\n",
    "| `torch.nn.MSELoss(reduction='none')`     | 无 reduction | `[B, C, H, W]` | 图像逐点监督    |\n",
    "| `F.cross_entropy(..., reduction='none')` | 无 reduction | `[B]`          | 每个样本自己的损失 |\n",
    "| 自定义 loss（如 contrastive loss）             | 手动构造        | 向量或矩阵          | 表示多个比较对   |\n",
    "\n",
    "### 特点：\n",
    "\n",
    "* **不能直接 `.backward()`**，因为非标量不能自定义传播方向；\n",
    "* **必须先聚合为标量（如 `.mean()` 或 `.sum()`）再传播**：\n",
    "\n",
    "```python\n",
    "loss = F.cross_entropy(pred, target, reduction='none')  # [B]\n",
    "loss = loss.mean()  # scalar\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "或者：\n",
    "\n",
    "```python\n",
    "loss = custom_vector_loss(pred, target)  # [B]\n",
    "loss.backward(torch.ones_like(loss))  # 手动提供导数权重向量\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 三、两种损失函数在训练中的比较\n",
    "\n",
    "| 特性                  | 标量损失（scalar loss）    | 非标量损失（tensor loss） |\n",
    "| ------------------- | -------------------- | ------------------ |\n",
    "| 输出形状                | `torch.Size([])`（0维） | 多维张量               |\n",
    "| 是否能直接 `.backward()` | ✔️ 可以                | ❌ 不可以              |\n",
    "| 是否常用于训练             | ✅ 是                  | ⛔ 通常先聚合            |\n",
    "| 使用场景                | 分类、回归、单任务训练          | 多任务、注意力监督、对比学习     |\n",
    "| 优势                  | 简洁、稳定、自动传播           | 灵活、细粒度监督、多尺度损失     |\n",
    "| 缺点                  | 不可细粒度控制              | 需手动聚合或手动传播方向       |\n",
    "\n",
    "---\n",
    "\n",
    "## 四、实例对比：MSELoss 标量 vs 非标量\n",
    "\n",
    "### 标量形式（默认）：\n",
    "\n",
    "```python\n",
    "criterion = torch.nn.MSELoss()  # 默认 reduction='mean'\n",
    "y_pred = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y_true = torch.tensor([1.0, 2.0, 1.0])\n",
    "loss = criterion(y_pred, y_true)  # loss 是 scalar\n",
    "loss.backward()  # ✅ 自动传播\n",
    "```\n",
    "\n",
    "### 非标量形式（逐元素）：\n",
    "\n",
    "```python\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "loss = criterion(y_pred, y_true)  # loss 是 [3] 向量\n",
    "print(loss)  # tensor([0.0, 0.0, 4.0])\n",
    "\n",
    "# loss.backward()  ❌ 报错，不能直接反向传播\n",
    "\n",
    "loss.mean().backward()  # ✅ 先聚合为标量\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 五、实际应用中的非标量 loss\n",
    "\n",
    "### 多任务学习：\n",
    "\n",
    "* 每个任务对应一个分量：`loss = [loss_cls, loss_reg, loss_mask]`\n",
    "* 最终手动聚合成标量（加权求和）\n",
    "\n",
    "### 语义分割（pixel-wise loss）：\n",
    "\n",
    "* `loss.shape = [B, C, H, W]`，每个像素都有损失；\n",
    "* 通常 `loss.mean()` 聚合后再 `.backward()`。\n",
    "\n",
    "### 对比学习、距离学习：\n",
    "\n",
    "* 每对样本有一个损失值：`loss.shape = [B, B]`\n",
    "* 聚合为 `loss.mean()` 或手动加权传播\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结（适合记笔记）\n",
    "\n",
    "> * **标量损失函数（scalar loss function）**：输出单个实数，可直接 `.backward()`，是训练神经网络时的默认形式（如 MSE、交叉熵）。\n",
    "> * **非标量损失函数（non-scalar loss function）**：输出多维张量，如每个样本、每个像素的损失，需要聚合为标量后才能反向传播。\n",
    "> * 在 PyTorch 中，非标量损失必须通过 `.mean()` 或手动提供 `.backward(gradient=...)` 来引导反向传播。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
