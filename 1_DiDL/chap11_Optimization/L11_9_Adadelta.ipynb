{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 11.9. Adadelta\n",
    "Adadelta是AdaGrad的另一种变体（11.7节），主要区别在于前者**减少了学习率适应坐标的数量**。此外，广义上Adadelta被称为没有学习率，因为它**使用变化量本身作为未来变化的校准**。Adadelta算法是在`Zeiler.2012`中提出的。\n",
    "\n",
    "## 11.9.1. Adadelta算法\n",
    "\n",
    "简而言之，Adadelta使用两个状态变量，$\\mathbf{s}_t$用于存储**梯度**二阶导数的泄露平均值，$\\Delta\\mathbf{x}_t$用于存储**模型本身中参数变化**二阶导数的泄露平均值。请注意，为了与其他出版物和实现的兼容性，我们使用作者的原始符号和命名（没有其它真正理由让大家使用不同的希腊变量来表示在动量法、AdaGrad、RMSProp和Adadelta中用于相同用途的参数）。\n",
    "\n",
    "以下是Adadelta的技术细节。鉴于参数du jour是$\\rho$，我们获得了与11.8节类似的以下泄漏更新：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & = \\rho \\mathbf{s}_{t-1} + (1 - \\rho) \\mathbf{g}_t^2.\n",
    "\\end{aligned}\n",
    "\\tag{11.9.1}\n",
    "$$\n",
    "\n",
    "与11.8节的区别在于，我们使用重新缩放的梯度$\\mathbf{g}_t'$执行更新，即\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{x}_t  & = \\mathbf{x}_{t-1} - \\mathbf{g}_t'. \\\\\n",
    "\\end{aligned}\n",
    "\\tag{11.9.2}\n",
    "$$\n",
    "\n",
    "那么，调整后的梯度$\\mathbf{g}_t'$是什么？我们可以按如下方式计算它：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{g}_t' & = \\frac{\\sqrt{\\Delta\\mathbf{x}_{t-1} + \\epsilon}}{\\sqrt{{\\mathbf{s}_t + \\epsilon}}} \\odot \\mathbf{g}_t, \\\\\n",
    "\\end{aligned}\n",
    "\\tag{11.9.3}\n",
    "$$\n",
    "\n",
    "其中$\\Delta \\mathbf{x}_{t-1}$是重新缩放梯度的平方$\\mathbf{g}_t'$的泄漏平均值。我们将$\\Delta \\mathbf{x}_{0}$初始化为$0$，然后在每个步骤中使用$\\mathbf{g}_t'$更新它，即\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\Delta \\mathbf{x}_t & = \\rho \\Delta\\mathbf{x}_{t-1} + (1 - \\rho) {\\mathbf{g}_t'}^2,\n",
    "\\end{aligned}\n",
    "\\tag{11.9.4}\n",
    "$$\n",
    "\n",
    "和$\\epsilon$（例如$10^{-5}$这样的小值）是为了保持数字稳定性而加入的。\n",
    "\n",
    "## 11.9.2. 代码实现\n",
    "\n",
    "Adadelta需要为每个变量维护两个状态变量，即$\\mathbf{s}_t$和$\\Delta\\mathbf{x}_t$。这将产生以下实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def init_adadelta_states(feature_dim):\n",
    "    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n",
    "    delta_w, delta_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n",
    "    return ((s_w, delta_w), (s_b, delta_b))\n",
    "\n",
    "def adadelta(params, states, hyperparams):\n",
    "    rho, eps = hyperparams['rho'], 1e-5\n",
    "    for p, (s, delta) in zip(params, states):\n",
    "        with torch.no_grad():\n",
    "            # In-placeupdatesvia[:]\n",
    "            s[:] = rho * s + (1 - rho) * torch.square(p.grad)\n",
    "            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad\n",
    "            p[:] -= g\n",
    "            delta[:] = rho * delta + (1 - rho) * g * g\n",
    "        p.grad.data.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于每次参数更新，选择$\\rho = 0.9$相当于10个半衰期。由此我们得到："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243, 0.004 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n",
    "d2l.train_ch11(adadelta, init_adadelta_states(feature_dim),\n",
    "               {'rho': 0.9}, data_iter, feature_dim);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.9.3. 简洁实现"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243, 0.003 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "为了简洁实现，我们只需使用Trainer类中的adadelta算法。\n",
    "\"\"\"\n",
    "trainer = torch.optim.Adadelta\n",
    "d2l.train_concise_ch11(trainer, {'rho': 0.9}, data_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.9.4. 小结\n",
    "* Adadelta没有学习率参数。相反，它使用参数本身的变化率来调整学习率。\n",
    "* Adadelta需要两个状态变量来存储梯度的二阶导数和参数的变化。\n",
    "* Adadelta使用泄漏的平均值来保持对适当统计数据的运行估计。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}