{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 11.5. 小批量随机梯度下降\n",
    "到目前为止，我们在基于梯度的学习方法中遇到了两个极端情况：11.3节中使用**完整数据集**来计算梯度并更新参数，11.4节中一次处理**一个训练样本**来取得进展。二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。这暗示了两者之间可能有折中方案，这便涉及到**小批量随机梯度下降**（minibatch gradient descent）。\n",
    "\n",
    "## 11.5.1. 向量化和缓存\n",
    "使用小批量的决策的核心是**计算效率**。当考虑与多个GPU和多台服务器并行处理时，这一点最容易被理解。在这种情况下，我们需要向每个GPU发送至少一张图像。有了每台服务器8个GPU和16台服务器，我们就能得到大小为128的小批量。\n",
    "\n",
    "当涉及到单个GPU甚至CPU时，事情会更微妙一些：这些设备有多种类型的内存、通常情况下多种类型的计算单元以及在它们之间不同的带宽限制。例如，一个CPU有少量寄存器（register），L1和L2缓存，以及L3缓存（在不同的处理器内核之间共享）。随着缓存的大小的增加，它们的延迟也在增加，同时带宽在减少。可以说，处理器能够执行的操作远比主内存接口所能提供的多得多。\n",
    "\n",
    "首先，具有16个内核和AVX-512向量化的2GHz CPU每秒可处理高达$2 \\cdot 10^9 \\cdot 16 \\cdot 32 = 10^{12}$个字节。同时，GPU的性能很容易超过该数字100倍。而另一方面，中端服务器处理器的带宽可能不超过100Gb/s，即不到处理器满负荷所需的十分之一。更糟糕的是，并非所有的内存入口都是相等的：内存接口通常为64位或更宽（例如，在最多384位的GPU上）。因此读取单个字节会导致由于更宽的存取而产生的代价。\n",
    "\n",
    "其次，第一次存取的额外开销很大，而按序存取（sequential access）或突发读取（burst read）相对开销较小。有关更深入的讨论，请参阅此[维基百科文章](https://en.wikipedia.org/wiki/Cache_hierarchy)。\n",
    "\n",
    "减轻这些限制的方法是使用足够快的CPU缓存层次结构来为处理器提供数据。这是深度学习中批量处理背后的推动力。举一个简单的例子：矩阵-矩阵乘法。比如$\\mathbf{A} = \\mathbf{B}\\mathbf{C}$，我们有很多方法来计算$\\mathbf{A}$。例如，我们可以尝试以下方法：\n",
    "\n",
    "1. 我们可以计算$\\mathbf{A}_{ij} = \\mathbf{B}_{i,:} \\mathbf{C}_{:,j}^\\top$，也就是说，我们可以通过点积进行逐元素计算。\n",
    "2. 我们可以计算$\\mathbf{A}_{:,j} = \\mathbf{B} \\mathbf{C}_{:,j}^\\top$，也就是说，我们可以一次计算一列。同样，我们可以一次计算$\\mathbf{A}$一行$\\mathbf{A}_{i,:}$。\n",
    "3. 我们可以简单地计算$\\mathbf{A} = \\mathbf{B} \\mathbf{C}$。\n",
    "4. 我们可以将$\\mathbf{B}$和$\\mathbf{C}$分成较小的区块矩阵，然后一次计算$\\mathbf{A}$的一个区块。\n",
    "\n",
    "如果我们使用第一个选择，每次我们计算一个元素$\\mathbf{A}_{ij}$时，都需要将一行和一列向量复制到CPU中。更糟糕的是，由于矩阵元素是按顺序对齐的，因此当从内存中读取它们时，我们需要访问两个向量中许多不相交的位置。第二种选择相对更有利：我们能够在遍历$\\mathbf{B}$的同时，将列向量$\\mathbf{C}_{:,j}$保留在CPU缓存中。它将内存带宽需求减半，相应地提高了访问速度。第三种选择表面上是最可取的，然而大多数矩阵可能不能完全放入缓存中。第四种选择提供了一个实践上很有用的方案：我们可以将矩阵的区块移到缓存中然后在本地将它们相乘。让我们来看看这些操作在实践中的效率如何。\n",
    "\n",
    "除了计算效率之外，Python和深度学习框架本身带来的额外开销也是相当大的。回想一下，每次我们执行代码时，Python解释器都会向深度学习框架发送一个命令，要求将其插入到计算图中并在调度过程中处理它。这样的额外开销可能是非常不利的。总而言之，我们最好用向量化（和矩阵）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "0.29970335960388184"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "timer=d2l.Timer()\n",
    "A=torch.zeros(256,256)\n",
    "B=torch.zeros(256,256)\n",
    "C=torch.zeros(256,256)\n",
    "\n",
    "\"\"\"\n",
    "按元素分配只需遍历分别为B和C的所有行和列，即可将该值分配给A。\n",
    "\"\"\"\n",
    "# 逐元素计算A=BC\n",
    "timer.start()\n",
    "for i in range(256):\n",
    "    for j in range(256):\n",
    "        A[i, j] = torch.dot(B[i, :], C[:, j])\n",
    "timer.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "0.005258798599243164"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "更快的策略是执行按列分配。\n",
    "\"\"\"\n",
    "# 逐列计算A=BC\n",
    "timer.start()\n",
    "for j in range(256):\n",
    "    A[:, j] = torch.mv(B, C[:, j])\n",
    "timer.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance in Gigaflops: element 6.673, column 380.315, full 421.708\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "最有效的方法是在一个区块中执行整个操作。让我们看看它们各自的操作速度是多少。\n",
    "\"\"\"\n",
    "# 一次性计算A=BC\n",
    "timer.start()\n",
    "A = torch.mm(B, C)\n",
    "timer.stop()\n",
    "\n",
    "# 乘法和加法作为单独的操作（在实践中融合）\n",
    "gigaflops = [2/i for i in timer.times]\n",
    "print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n",
    "      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.5.2. 小批量\n",
    "之前我们会理所当然地读取数据的*小批量*，而不是观测单个数据来更新参数，现在简要解释一下原因。处理单个观测值需要我们执行许多单一矩阵-矢量（甚至矢量-矢量）乘法，这耗费相当大，而且对应深度学习框架也要巨大的开销。这既适用于计算梯度以更新参数时，也适用于用神经网络预测。也就是说，每当我们执行$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g}_t$时，消耗巨大。其中\n",
    "\n",
    "$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} f(\\mathbf{x}_{t}, \\mathbf{w}).\\tag{11.5.1}$$\n",
    "\n",
    "我们可以通过将其应用于一个小批量观测值来提高此操作的**计算**效率。也就是说，我们**将梯度$\\mathbf{g}_t$替换为一个小批量而不是单个观测值**\n",
    "\n",
    "$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w}).\\tag{11.5.2}$$\n",
    "\n",
    "\n",
    "让我们看看这对$\\mathbf{g}_t$的统计属性有什么影响：由于$\\mathbf{x}_t$和小批量$\\mathcal{B}_t$的所有元素都是从训练集中随机抽出的，因此梯度的期望保持不变。另一方面，方差显著降低。由于小批量梯度由正在被平均计算的$b := |\\mathcal{B}_t|$个独立梯度组成，其标准差降低了$b^{-\\frac{1}{2}}$。这本身就是一件好事，因为这意味着更新与完整的梯度更接近了。\n",
    "\n",
    "\n",
    "直观来说，这表明选择大型的小批量$\\mathcal{B}_t$将是普遍可行的。然而，经过一段时间后，与计算代价的线性增长相比，标准差的额外减少是微乎其微的。在实践中我们选择一个足够大的小批量，它可以提供良好的计算效率同时仍适合GPU的内存。下面，我们来看看这些高效的代码。在里面我们执行相同的矩阵-矩阵乘法，但是这次我们将其一次性分为64列的“小批量”。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance in Gigaflops: block 317.666\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "for j in range(0, 256, 64):\n",
    "    A[:, j:j+64] = torch.mm(B, C[:, j:j+64])\n",
    "timer.stop()\n",
    "print(f'performance in Gigaflops: block {2 / timer.times[3]:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "显而易见，小批量上的计算基本上与完整矩阵一样有效。需要注意的是，在7.5节中，我们使用了一种在很大程度上取决于小批量中的方差的正则化。随着后者增加，方差会减少，随之而来的是批量规范化带来的噪声注入的好处。关于实例，请参阅`Ioffe.2017`，了解有关如何重新缩放并计算适当项目。\n",
    "\n",
    "## 11.5.3. 读取数据集\n",
    "让我们来看看**如何从数据中有效地生成小批量**。下面我们使用NASA开发的测试机翼的数据集[不同飞行器产生的噪声](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise)来比较这些优化算法。为方便起见，我们只使用前$1,500$样本。数据已作预处理：我们移除了均值并将方差重新缩放到每个坐标为$1$。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['airfoil']=(d2l.DATA_URL+'airfoil_self_noise.dat',\n",
    "                         '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
    "\n",
    "#@save\n",
    "def get_data_ch11(batch_size=10,n=1500):\n",
    "    data=np.genfromtxt(d2l.download('airfoil'),dtype=np.float32,delimiter='\\t')\n",
    "    data=torch.from_numpy((data-data.mean(axis=0)) / data.std(axis=0))\n",
    "    data_iter=d2l.load_array((data[:n,:-1],data[:n,-1]),batch_size,is_train=True)\n",
    "    return data_iter,data.shape[1]-1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.5.4. 从零开始实现\n",
    "3.2节中已经实现过小批量随机梯度下降算法。我们在这里将它的输入参数变得更加通用，主要是为了方便本章后面介绍的其他优化算法也可以使用同样的输入。具体来说，我们添加了一个状态输入`states`并将超参数放在字典`hyperparams`中。此外，我们将在训练函数里**对各个小批量样本的损失求平均**，因此优化算法中的梯度不需要除以批量大小。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.253,0.008 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "def sgd(params,states,hyperparams):\n",
    "    for p in params:\n",
    "        p.data.sub_(hyperparams['lr'] * p.grad)\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "\"\"\"\n",
    "下面实现一个通用的训练函数，以方便本章后面介绍的其他优化算法使用。 它初始化了一个线性回归模型，然后可以使用小批量随机梯度下降以及后续小节介绍的其他算法来训练模型。\n",
    "\"\"\"\n",
    "#@save\n",
    "def train_ch11(trainer_fn,states,hyperparams,data_iter,feature_dim,num_epochs=2):\n",
    "    # 初始化模型\n",
    "    w=torch.normal(mean=0.0,std=0.01,size=(feature_dim,1),requires_grad=True)\n",
    "    b=torch.zeros((1),requires_grad=True)\n",
    "    net,loss=lambda X:d2l.linreg(X,w,b),d2l.squared_loss\n",
    "\n",
    "    # 训练模型\n",
    "    animator=d2l.Animator(xlabel='epoch',ylabel='loss',xlim=[0,num_epochs],ylim=[0.22,0.35])\n",
    "    n,timer=0,d2l.Timer()\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for X,y in data_iter:\n",
    "            l=loss(net(X),y).mean()\n",
    "            l.backward()\n",
    "            trainer_fn([w,b],states,hyperparams)\n",
    "            n += X.shape[0]\n",
    "            if n%200 ==0:\n",
    "                timer.stop()\n",
    "                animator.add(n/X.shape[0]/len(data_iter),\n",
    "                             (d2l.evaluate_loss(net,data_iter,loss),))\n",
    "                timer.start()\n",
    "    d2l.plt.show()\n",
    "    print(f'loss:{animator.Y[0][-1]:.3f},{timer.avg():.3f} sec/epoch')\n",
    "    return timer.cumsum(),animator.Y[0]\n",
    "\n",
    "\"\"\"\n",
    "让我们来看看批量梯度下降的优化是如何进行的。 这可以通过将小批量设置为1500（即样本总数）来实现。 因此，模型参数每个迭代轮数只迭代一次。\n",
    "\"\"\"\n",
    "def train_sgd(lr,batch_size,num_epochs=2):\n",
    "    data_iter,feature_dim=get_data_ch11(batch_size)\n",
    "    return train_ch11(sgd,None,{'lr':lr},data_iter,feature_dim,num_epochs)\n",
    "\n",
    "gd_res=train_sgd(1,1500,10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.244,0.016 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "当批量大小为1时，优化使用的是随机梯度下降。为了简化实现，我们选择了很小的学习率。 在随机梯度下降的实验中，每当一个样本被处理，模型参数都会更新。在这个例子中，这相当于每个迭代轮数有1500次更新。可以看到，目标函数值的下降在1个迭代轮数后就变得较为平缓。尽管两个例子在一个迭代轮数内都处理了1500个样本，但实验中随机梯度下降的一个迭代轮数耗时更多。这是因为随机梯度下降更频繁地更新了参数，而且一次处理单个观测值效率较低。\n",
    "\"\"\"\n",
    "sgd_res=train_sgd(0.005,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.252,0.001 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "最后，当批量大小等于100时，我们使用小批量随机梯度下降进行优化。 每个迭代轮数所需的时间比随机梯度下降和批量梯度下降所需的时间短。\n",
    "\"\"\"\n",
    "mini1_res=train_sgd(.4,100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.246,0.003 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "将批量大小减少到10，每个迭代轮数的时间都会增加，因为每批工作负载的执行效率变得更低。\n",
    "\"\"\"\n",
    "mini2_res=train_sgd(.05,10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 350x250 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"261.44375pt\" height=\"183.35625pt\" viewBox=\"0 0 261.44375 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-10-28T15:49:00.094399</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 261.44375 183.35625 \nL 261.44375 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 145.8 \nL 245.44375 145.8 \nL 245.44375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 50.14375 145.8 \nL 50.14375 7.2 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"mdf0718526e\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mdf0718526e\" x=\"50.14375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(38.39375 160.398438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(0 0.765625)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0.765625)\"/>\n       <use xlink:href=\"#DejaVuSans-2212\" transform=\"translate(128.203125 39.046875)scale(0.7)\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(186.855469 39.046875)scale(0.7)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 115.24375 145.8 \nL 115.24375 7.2 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mdf0718526e\" x=\"115.24375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(103.49375 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(0 0.684375)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0.684375)\"/>\n       <use xlink:href=\"#DejaVuSans-2212\" transform=\"translate(128.203125 38.965625)scale(0.7)\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(186.855469 38.965625)scale(0.7)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 180.34375 145.8 \nL 180.34375 7.2 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mdf0718526e\" x=\"180.34375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(171.54375 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(0 0.765625)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0.765625)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(128.203125 39.046875)scale(0.7)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 245.44375 145.8 \nL 245.44375 7.2 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mdf0718526e\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(236.64375 160.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(0 0.684375)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0.684375)\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(128.203125 38.965625)scale(0.7)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path id=\"m8c523e98ce\" d=\"M 0 0 \nL 0 2 \n\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"69.740803\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"81.204344\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"89.337855\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"95.646697\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"100.801396\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"105.159632\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"108.934908\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"112.264937\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"134.840803\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"146.304344\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_19\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"154.437855\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"160.746697\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_21\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"165.901396\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"170.259632\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_23\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"174.034908\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"177.364937\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_21\">\n     <g id=\"line2d_25\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"199.940803\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_22\">\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"211.404344\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_23\">\n     <g id=\"line2d_27\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"219.537855\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_24\">\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"225.846697\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_25\">\n     <g id=\"line2d_29\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"231.001396\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_26\">\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"235.359632\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_27\">\n     <g id=\"line2d_31\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"239.134908\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_28\">\n     <g id=\"line2d_32\">\n      <g>\n       <use xlink:href=\"#m8c523e98ce\" x=\"242.464937\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- time(sec) -->\n     <g transform=\"translate(124.165625 174.076563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"66.992188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"164.404297\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"225.927734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"264.941406\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"317.041016\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"378.564453\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"433.544922\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_33\">\n      <path d=\"M 50.14375 142.505863 \nL 245.44375 142.505863 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <defs>\n       <path id=\"m4ab819d5aa\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m4ab819d5aa\" x=\"50.14375\" y=\"142.505863\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.24 -->\n      <g transform=\"translate(20.878125 146.305082)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_35\">\n      <path d=\"M 50.14375 114.043497 \nL 245.44375 114.043497 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_36\">\n      <g>\n       <use xlink:href=\"#m4ab819d5aa\" x=\"50.14375\" y=\"114.043497\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.26 -->\n      <g transform=\"translate(20.878125 117.842716)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_37\">\n      <path d=\"M 50.14375 85.581131 \nL 245.44375 85.581131 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_38\">\n      <g>\n       <use xlink:href=\"#m4ab819d5aa\" x=\"50.14375\" y=\"85.581131\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.28 -->\n      <g transform=\"translate(20.878125 89.38035)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_39\">\n      <path d=\"M 50.14375 57.118765 \nL 245.44375 57.118765 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_40\">\n      <g>\n       <use xlink:href=\"#m4ab819d5aa\" x=\"50.14375\" y=\"57.118765\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.30 -->\n      <g transform=\"translate(20.878125 60.917984)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_41\">\n      <path d=\"M 50.14375 28.656399 \nL 245.44375 28.656399 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_42\">\n      <g>\n       <use xlink:href=\"#m4ab819d5aa\" x=\"50.14375\" y=\"28.656399\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.32 -->\n      <g transform=\"translate(20.878125 32.455617)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- loss -->\n     <g transform=\"translate(14.798437 86.157813)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 43.650258 126.560406 \nL 63.36073 132.058515 \nL 75.442549 131.134467 \nL 84.348466 128.290634 \nL 89.933456 123.484309 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 65.31905 38.364584 \nL 84.032885 91.250553 \nL 95.218236 126.21583 \nL 103.324052 137.232694 \nL 109.572718 136.061514 \nL 114.733526 138.058679 \nL 118.994949 134.064241 \nL 122.696206 135.879865 \nL 125.880243 136.21964 \nL 128.798532 135.633064 \nL 131.522751 134.47174 \nL 134.098433 125.349377 \nL 136.389653 135.454241 \nL 138.486079 139.5 \nL 140.407849 136.686162 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M -1 64.554873 \nL 6.04577 93.156474 \nL 17.790125 123.637425 \nL 25.858468 128.296526 \nL 32.399637 131.44477 \nL 37.20667 136.361825 \nL 41.575252 137.282079 \nL 46.168894 130.84029 \nL 49.742514 135.14335 \nL 52.851069 136.522086 \nL 58.586453 131.998638 \nL 60.678818 117.746241 \nL 62.643847 114.084412 \nL 66.746455 134.068114 \nL 68.407169 126.027305 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 13.824297 43.551192 \nL 32.346557 100.183916 \nL 44.153765 130.185476 \nL 52.155962 135.32291 \nL 58.735575 132.48629 \nL 63.5447 124.768859 \nL 67.721259 136.416678 \nL 71.609205 136.687406 \nL 74.706046 130.426931 \nL 77.662427 135.188827 \nL 80.303256 139.033169 \nL 82.608884 133.489822 \nL 84.747124 137.152935 \nL 86.807296 131.169702 \nL 88.680785 134.35526 \n\" clip-path=\"url(#pd790ad3b30)\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 145.8 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 245.44375 145.8 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 145.8 \nL 245.44375 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 128.176562 73.9125 \nL 238.44375 73.9125 \nQ 240.44375 73.9125 240.44375 71.9125 \nL 240.44375 14.2 \nQ 240.44375 12.2 238.44375 12.2 \nL 128.176562 12.2 \nQ 126.176562 12.2 126.176562 14.2 \nL 126.176562 71.9125 \nQ 126.176562 73.9125 128.176562 73.9125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_47\">\n     <path d=\"M 130.176562 20.298438 \nL 140.176562 20.298438 \nL 150.176562 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- gd -->\n     <g transform=\"translate(158.176562 23.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_48\">\n     <path d=\"M 130.176562 34.976562 \nL 140.176562 34.976562 \nL 150.176562 34.976562 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- sgd -->\n     <g transform=\"translate(158.176562 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-73\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"52.099609\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"115.576172\"/>\n     </g>\n    </g>\n    <g id=\"line2d_49\">\n     <path d=\"M 130.176562 49.654687 \nL 140.176562 49.654687 \nL 150.176562 49.654687 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- batch size=100 -->\n     <g transform=\"translate(158.176562 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-7a\" d=\"M 353 3500 \nL 3084 3500 \nL 3084 2975 \nL 922 459 \nL 3084 459 \nL 3084 0 \nL 275 0 \nL 275 525 \nL 2438 3041 \nL 353 3041 \nL 353 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-62\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"314.111328\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"366.210938\"/>\n      <use xlink:href=\"#DejaVuSans-7a\" x=\"393.994141\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"446.484375\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"508.007812\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"591.796875\"/>\n      <use xlink:href=\"#DejaVuSans-30\" x=\"655.419922\"/>\n      <use xlink:href=\"#DejaVuSans-30\" x=\"719.042969\"/>\n     </g>\n    </g>\n    <g id=\"line2d_50\">\n     <path d=\"M 130.176562 64.332813 \nL 140.176562 64.332813 \nL 150.176562 64.332813 \n\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- batch size=10 -->\n     <g transform=\"translate(158.176562 67.832813)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-62\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"124.755859\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"163.964844\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"218.945312\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"282.324219\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"314.111328\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"366.210938\"/>\n      <use xlink:href=\"#DejaVuSans-7a\" x=\"393.994141\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"446.484375\"/>\n      <use xlink:href=\"#DejaVuSans-3d\" x=\"508.007812\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"591.796875\"/>\n      <use xlink:href=\"#DejaVuSans-30\" x=\"655.419922\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd790ad3b30\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "现在我们可以比较前四个实验的时间与损失。可以看出，尽管在处理的样本数方面，随机梯度下降的收敛速度快于梯度下降，但与梯度下降相比，它需要更多的时间来达到同样的损失，因为逐个样本来计算梯度并不那么有效。小批量随机梯度下降能够平衡收敛速度和计算效率。大小为10的小批量比随机梯度下降更有效；大小为100的小批量在运行时间上甚至优于梯度下降。\n",
    "\"\"\"\n",
    "d2l.set_figsize([6,3])\n",
    "d2l.plot(*list(map(list,zip(gd_res,sgd_res,mini1_res,mini2_res))),'time(sec)','loss',xlim=[1e-2,10],legend=['gd','sgd','batch size=100','batch size=10'])\n",
    "d2l.plt.gca().set_xscale('log')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.5.5. 简洁实现"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.245,0.003 sec/epoch\n"
     ]
    }
   ],
   "source": [
    "#@save\n",
    "def train_concise_ch11(trainer_fn,hyperparams,data_iter,num_epochs=4):\n",
    "    # 初始化模型\n",
    "    net=nn.Sequential(nn.Linear(5,1))\n",
    "    def init_weights(m):\n",
    "        if type(m)==nn.Linear:\n",
    "            torch.nn.init.normal_(m.weight,std=0.01)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    optimizer=trainer_fn(net.parameters(),**hyperparams)\n",
    "    loss=nn.MSELoss(reduction='none')\n",
    "    animator=d2l.Animator(xlabel='epoch',ylabel='loss',xlim=[0,num_epochs],ylim=[0.22,0.35])\n",
    "\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X,y in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            out=net(X)\n",
    "            y=y.reshape(out.shape)\n",
    "            l=loss(out,y)\n",
    "            l.mean().backward()\n",
    "            optimizer.step()\n",
    "            n += X.shape[0]\n",
    "            if n%200 ==0:\n",
    "                timer.stop()\n",
    "                # MSELoss计算平方误差时不带系数1/2\n",
    "                animator.add(n/X.shape[0]/len(data_iter),(d2l.evaluate_loss(net,data_iter,loss)/2,))\n",
    "                timer.start()\n",
    "    print(f'loss:{animator.Y[0][-1]:.3f},{timer.avg():.3f} sec/epoch')\n",
    "\n",
    "\"\"\"\n",
    "下面使用这个训练函数，复现之前的实验。\n",
    "\"\"\"\n",
    "data_iter,_=get_data_ch11(10)\n",
    "trainer=torch.optim.SGD\n",
    "train_concise_ch11(trainer,{'lr':0.01},data_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.5.6. 小结\n",
    "- 由于减少了深度学习框架的额外开销，使用更好的内存定位以及CPU和GPU上的缓存，向量化使代码更加高效。\n",
    "- 随机梯度下降的“统计效率”与大批量一次处理数据的“计算效率”之间存在权衡。小批量随机梯度下降提供了两全其美的答案：**计算和统计效率**。\n",
    "- 在小批量随机梯度下降中，我们处理通过训练数据的随机排列获得的批量数据（即每个观测值只处理一次，但按随机顺序）。\n",
    "- 在训练期间降低学习率有助于训练。\n",
    "- 一般来说，小批量随机梯度下降比随机梯度下降和梯度下降的速度快，收敛风险较小。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}