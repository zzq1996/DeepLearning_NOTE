{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 机器学习（Machine Learning）\n",
    "\n",
    "![](../img/0_2.png)\n",
    "\n",
    "机器学习是使计算机具备**从数据中学习经验并进行预测或决策**能力的一门学科，核心任务是从数据中学习函数映射关系 $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$。按监督信号的有无，主要分为：\n",
    "\n",
    "* 监督学习（Supervised Learning）\n",
    "* 无监督学习（Unsupervised Learning）\n",
    "* 强化学习（Reinforcement Learning）\n",
    "* 自监督学习（Self-Supervised Learning，介于监督与无监督之间）\n",
    "* 生成建模（Generative Modeling，与判别建模相对）\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 监督学习（Supervised Learning）\n",
    "\n",
    "![](../img/0_1.png)\n",
    "\n",
    "监督学习指的是：在输入数据 $\\boldsymbol{x}$ 与目标标签 $y$ 成对出现的训练集下，学习一个函数 $f(x) \\approx y$。常见任务包括回归与分类。\n",
    "\n",
    "### 1.1 回归（Regression）\n",
    "\n",
    "回归任务的目标是预测一个**连续数值型变量**。\n",
    "\n",
    "> 示例：预测房价、股价、温度，或某个病症的风险评分。\n",
    "\n",
    "* 模型输出：实数 $\\hat{y} \\in \\mathbb{R}$\n",
    "* 常见损失函数：均方误差（MSE）、平均绝对误差（MAE）\n",
    "* 示例模型：线性回归、SVR、神经网络回归器\n",
    "\n",
    "### 1.2 分类（Classification）\n",
    "\n",
    "分类任务的目标是预测样本属于哪一个**离散类别**。\n",
    "\n",
    "* 输出类别集合：$\\mathcal{Y} = \\{c_1, c_2, ..., c_k\\}$\n",
    "* 常用损失函数：交叉熵损失（cross-entropy loss）\n",
    "\n",
    "#### 1.2.1 二分类（Binary Classification）\n",
    "\n",
    "* 标签 $y \\in \\{0, 1\\}$\n",
    "* 典型任务：垃圾邮件识别、癌症检测\n",
    "\n",
    "#### 1.2.2 多分类（Multiclass Classification）\n",
    "\n",
    "* 标签 $y \\in \\{1, 2, ..., k\\}$\n",
    "* 典型任务：手写数字识别（MNIST）、图像分类（ImageNet）\n",
    "\n",
    "#### 1.2.3 层次分类（Hierarchical Classification）\n",
    "\n",
    "* 类别之间存在**层次结构**（例如 WordNet）\n",
    "* 错误也有“远近”：错分为近似类别比错到无关类别好\n",
    "\n",
    "#### 1.2.4 多标签分类（Multi-Label Classification）\n",
    "\n",
    "* 每个样本可同时属于多个类，标签为 $\\boldsymbol{y} \\in \\{0,1\\}^k$\n",
    "* 示例：图像中同时存在“人”、“车”、“建筑物”\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 序列学习（Sequence Learning）\n",
    "\n",
    "序列学习任务的输入和/或输出是**序列结构**（如语言、声音、时间序列）。\n",
    "\n",
    "#### 常见任务：\n",
    "\n",
    "* **序列标注（Sequence Labeling）**：词性标注、命名实体识别\n",
    "* **自动语音识别（ASR）**：语音 → 文本\n",
    "* **文本到语音（TTS）**：文本 → 语音波形\n",
    "* **机器翻译（MT）**：输入序列（源语言） → 输出序列（目标语言）\n",
    "\n",
    "常用方法：\n",
    "\n",
    "* 循环神经网络（RNN/LSTM/GRU）\n",
    "* Transformer 模型（现主流）\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 搜索与排序（Ranking）\n",
    "\n",
    "目标是对一组候选项进行排序，核心是**学习打分函数**。\n",
    "\n",
    "* 应用：信息检索、搜索引擎、推荐系统\n",
    "* 方法：Pointwise / Pairwise / Listwise 排序学习方法\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 推荐系统（Recommendation）\n",
    "\n",
    "推荐任务是在用户与物品的交互数据基础上预测用户可能喜欢的内容。\n",
    "\n",
    "* 协同过滤（Collaborative Filtering）\n",
    "* 基于内容的推荐（Content-based Recommendation）\n",
    "* 深度学习推荐模型（如 DeepFM、DIN、BERT4Rec）\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 无监督学习（Unsupervised Learning）\n",
    "\n",
    "无监督学习指的是**没有明确标签（target y）**的学习任务，目标是挖掘数据的**潜在结构、分布模式或生成机制**。\n",
    "\n",
    "### 2.1 聚类（Clustering）\n",
    "\n",
    "> 将样本划分为若干“相似”组\n",
    "\n",
    "* K-Means、DBSCAN、谱聚类（Spectral Clustering）\n",
    "* 应用：图像聚类、客户细分、生物样本分型\n",
    "\n",
    "### 2.2 降维与表示学习\n",
    "\n",
    "> 将高维数据映射到低维空间，保留关键信息\n",
    "\n",
    "* **主成分分析（PCA）**：线性降维，最大化投影方差\n",
    "* **t-SNE / UMAP**：非线性降维，用于可视化\n",
    "* **自编码器（Autoencoder）**：神经网络表示学习\n",
    "\n",
    "### 2.3 概率图模型 & 因果推理\n",
    "\n",
    "* **概率图模型**（PGM）：如贝叶斯网络、马尔可夫随机场\n",
    "* **因果推理（Causality）**：关注变量间的因果关系而非关联性\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 生成建模（Generative Modeling）\n",
    "\n",
    "目标是学习数据的分布 $p(x)$，可以用来**生成新样本、建模数据本质结构**。\n",
    "\n",
    "* **生成对抗网络（GAN）**\n",
    "* **变分自编码器（VAE）**\n",
    "* **扩散模型（Diffusion Model）**\n",
    "* 应用：图像生成、语音合成、分子生成、文本生成\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 与环境互动：交互式学习 / 联合优化类任务\n",
    "\n",
    "指模型不仅被动接收数据，还与外部环境交互，常见于控制系统、对话系统、机器人等领域。\n",
    "\n",
    "* 在线学习（Online Learning）\n",
    "* 主动学习（Active Learning）\n",
    "* 联合学习（Federated Learning）\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 强化学习（Reinforcement Learning）\n",
    "\n",
    "强化学习中，智能体（Agent）在环境中通过试错学习策略，以最大化长期奖励（cumulative reward）。\n",
    "\n",
    "### 基本要素：\n",
    "\n",
    "* 状态 $s$\n",
    "* 动作 $a$\n",
    "* 奖励 $r$\n",
    "* 策略 $\\pi(a|s)$\n",
    "* 状态转移函数 $P(s'|s,a)$\n",
    "\n",
    "### 常见算法：\n",
    "\n",
    "* Q-Learning / DQN（值函数方法）\n",
    "* Policy Gradient / PPO（策略梯度方法）\n",
    "* Actor-Critic（混合方法）\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "| 学习范式  | 是否有监督   | 输出结构      | 主要目标          |\n",
    "| ----- | ------- | --------- | ------------- |\n",
    "| 监督学习  | ✅ 是     | 标签（离散/连续） | 拟合目标函数        |\n",
    "| 无监督学习 | ❌ 否     | 聚类、嵌入、生成等 | 挖掘数据结构、密度分布   |\n",
    "| 强化学习  | ✅（弱监督）  | 行为策略      | 与环境互动，最大化长期奖励 |\n",
    "| 自监督学习 | ❌（伪标签）  | 任务相关预测    | 设计预训练任务学习表示   |\n",
    "| 生成建模  | 可监督/无监督 | 数据样本、分布   | 学习数据的生成机制     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 深度学习\n",
    "- 深度学习：一类机器学习问题，主要解决贡献度分配问题。\n",
    "- 神经网络：一种以（人工）神经元为基本单元的模型\n",
    "\n",
    "## 深度学习\n",
    "![](../img/0_3.png)\n",
    "![](../img/0_4.png)\n",
    "\n",
    "## 神经网络\n",
    "![](../img/0_5.png)\n",
    "![](../img/0_6.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Q1：如何理解深度神经网络解决贡献度分配问题？\n",
    "\n",
    "---\n",
    "\n",
    "##  1. 什么是“贡献度分配问题”？\n",
    "\n",
    "贡献度分配（Contribution Attribution）问题指的是：\n",
    "\n",
    "> 在深度神经网络模型做出预测之后，我们希望知道：\n",
    "> **输入的每个特征在这个预测中起了多大作用？哪些特征最关键？**\n",
    "\n",
    "举例说明：\n",
    "\n",
    "* 对一个图像分类模型来说，我们希望知道**图像的哪些像素区域**对预测结果的影响最大；\n",
    "* 对一个信用评分模型，我们想知道**年龄、收入、信用记录等输入变量**分别对最终评分贡献多大。\n",
    "\n",
    "这种问题的本质是：\n",
    "\n",
    "> **如何将预测结果归因（Attribute）到输入的不同部分。**\n",
    "\n",
    "---\n",
    "\n",
    "##  2. 为什么深度网络中的贡献度分配很难？\n",
    "\n",
    "深度神经网络是**高度非线性的黑箱模型**，包含多个非线性层次的变换，例如：\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(x) = f_L(f_{L-1}(...f_1(x)))\n",
    "$$\n",
    "\n",
    "每一层都对输入进行了高度复杂的嵌套变换，因此：\n",
    "\n",
    "* 特征之间存在强非线性交互；\n",
    "* 无法简单地通过权重来衡量每个特征的作用；\n",
    "* 某些特征可能在某些输入情况下作用很大，在其他情况下作用很小（动态重要性）；\n",
    "* 权重无法唯一决定贡献度。\n",
    "\n",
    "这就使得在深度网络中解决贡献度分配问题成为一个挑战。\n",
    "\n",
    "---\n",
    "\n",
    "##  3. 深度神经网络是如何“尝试”解决贡献度分配问题的？\n",
    "\n",
    "虽然深度神经网络本身不**自动**给出特征的重要性，但我们可以使用一些**可解释性方法（Interpretability Methods）**，从模型内部结构和梯度出发，为预测结果分配贡献度：\n",
    "\n",
    "###  常见方法如下：\n",
    "\n",
    "#### （1）**梯度方法（Gradient-based Methods）**\n",
    "\n",
    "* 利用输入对输出的梯度 $\\frac{\\partial \\hat{y}}{\\partial x_i}$ 作为特征 $x_i$ 的“边际影响”\n",
    "* 如：\n",
    "\n",
    "  * **Saliency Map**\n",
    "  * **Integrated Gradients**\n",
    "  * **SmoothGrad**\n",
    "\n",
    "> 思想：如果改变 $x_i$ 会显著改变预测值，则说明 $x_i$ 重要\n",
    "\n",
    "#### （2）**前向分解方法（Layer-wise Relevance Propagation, LRP）**\n",
    "\n",
    "* 将预测结果 $\\hat{y}$ 通过神经网络**层层反向分解**，将其“责任”分配给输入特征；\n",
    "* 原理类似能量守恒：总输出值 = 所有输入特征的分配值之和；\n",
    "* 常用于图像任务中的热力图（Heatmap）可视化。\n",
    "\n",
    "#### （3）**遮挡/扰动分析（Perturbation-based Methods）**\n",
    "\n",
    "* 遮蔽或扰动某些输入特征，观察输出变化；\n",
    "* 如果遮蔽某个特征让输出变化很大，说明它很重要；\n",
    "* 如：\n",
    "\n",
    "  * Occlusion Sensitivity（图像中用黑块遮蔽）\n",
    "  * SHAP / LIME（用局部近似模型建模扰动后的影响）\n",
    "\n",
    "#### （4）**Shapley Value 方法（SHAP）**\n",
    "\n",
    "* 来源于合作博弈论，把预测看成特征共同“合作”的结果；\n",
    "* 为每个特征计算其**边际贡献的期望值**；\n",
    "* 优点：有明确的公理基础，理论完备；\n",
    "* 缺点：计算量大（尤其在深度模型中）\n",
    "\n",
    "---\n",
    "\n",
    "##  4. 总结：DNN 中贡献度分配的理解\n",
    "\n",
    "| 项目       | 内容                                             |\n",
    "| -------- | ---------------------------------------------- |\n",
    "| **目标**   | 分析 DNN 输出中，每个输入特征的影响/贡献大小                      |\n",
    "| **挑战**   | 深度网络是非线性、多层嵌套的黑箱，贡献度不容易追踪                      |\n",
    "| **方法类别** | 梯度法、反向传播法、扰动法、博弈论法                             |\n",
    "| **常见方法** | Saliency, Integrated Gradient, LRP, SHAP, LIME |\n",
    "| **意义**   | 提高模型解释性，增强用户信任，辅助诊断或可视化                        |\n",
    "\n",
    "---\n",
    "\n",
    "##  5. 举例图示（建议你配图笔记）\n",
    "\n",
    "* 原图 → 神经网络输出类别 → 反向可视化热力图\n",
    "* 热区（高贡献度）显示网络是如何“看”图像的\n",
    "* 类似在文本中，哪些词被注意力机制或梯度机制赋予更高权重\n",
    "\n",
    "---\n",
    "\n",
    "##  推荐笔记整理语句\n",
    "\n",
    "> 在深度神经网络中，贡献度分配问题指的是如何确定每个输入特征对模型预测结果的相对重要性。由于 DNN 是非线性嵌套的结构，不能简单通过权重衡量。常用方法包括基于梯度（如 Integrated Gradients）、反向传播（如 LRP）、扰动（如 LIME、SHAP）等手段，用以从不同角度解释模型的预测来源。这对于理解模型行为、增强可解释性、构建可信 AI 系统具有重要意义。\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
