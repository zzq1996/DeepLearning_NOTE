{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在前面的章节中，我们遇到过图像数据。这种数据的每个样本都由一个二维像素网格组成， 每个像素可能是一个或者多个数值，取决于是黑白还是彩色图像。到目前为止，我们处理这类结构丰富的数据的方式还不够有效。我们仅仅**通过将图像数据展平成一维向量而忽略了每个图像的空间结构信息**，再将数据送入一个全连接的多层感知机中。 因为**这些网络特征元素的顺序是不变的**，因此最优的结果是利用先验知识，即**利用相近像素之间的相互关联性，从图像数据中学习得到有效的模型**。\n",
    "\n",
    "\n",
    "本章介绍的卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。\n",
    "\n",
    "\n",
    "现代卷积神经网络的设计得益于生物学、群论和一系列的补充实验。**卷积神经网络需要的参数少于全连接架构的网络，而且卷积也很容易用GPU并行计算**。因此卷积神经网络除了能够高效地采样从而获得精确的模型，还能够高效地计算。久而久之，从业人员越来越多地使用卷积神经网络。即使在通常使用循环神经网络的一维序列结构任务上（例如音频、文本和时间序列分析），卷积神经网络也越来越受欢迎。通过对卷积神经网络一些巧妙的调整，也使它们在图结构数据和推荐系统中发挥作用。\n",
    "\n",
    "\n",
    "在本章的开始，我们将介绍构成所有卷积网络主干的基本元素。这包括**卷积层本身**、**填充**（padding）和**步幅**（stride）的基本细节、用于在相邻区域汇聚信息的**汇聚层**（pooling）、在每一层中多**通道**（channel）的使用，以及有关现代卷积网络架构的仔细讨论。\n",
    "\n",
    "在本章的最后，我们将介绍一个完整的、可运行的LeNet模型：这是第一个成功应用的卷积神经网络，比现代深度学习兴起时间还要早。\n",
    "\n",
    "在下一章中，我们将深入研究一些流行的、相对较新的卷积神经网络架构的完整实现，这些网络架构涵盖了现代从业者通常使用的大多数经典技术。\n",
    "\n",
    "# 6.1. 从全连接层到卷积\n",
    "我们之前讨论的多层感知机十分适合处理**表格数据**，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。此时，多层感知机可能是最好的选择，然而对于**高维感知数据**，这种缺少结构的网络可能会变得不实用。\n",
    "\n",
    "然而，如今人类和机器都能很好地区分猫和狗：这是因为**图像中本就拥有丰富的结构**，而这些结构可以被人类和机器学习模型使用。卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。\n",
    "\n",
    "## 6.1.1. 不变性\n",
    "想象一下，假设你想从一张图片中找到某个物体。合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。沃尔多的样子并不取决于他潜藏的地方，因此我们可以使用一个“沃尔多检测器”扫描图像。该检测器将图像分割成多个区域，并为每个区域包含沃尔多的可能性打分。卷积神经网络正是将**空间不变性**（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。\n",
    "\n",
    "- **平移不变性**（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。\n",
    "- **局部性**（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。\n",
    "\n",
    "## 6.1.2. 多层感知机的限制\n",
    "首先，多层感知机的输入是二维图像$\\mathbf{X}$，其隐藏表示$\\mathbf{H}$在数学上是一个矩阵，在代码中表示为二维张量。其中$\\mathbf{X}$和$\\mathbf{H}$具有相同的形状。为了方便理解，我们可以认为，无论是输入还是隐藏表示都拥有空间结构。\n",
    "\n",
    "使用$[\\mathbf{X}]_{i, j}$和$[\\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中位置（$i$,$j$）处的像素。为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量$\\mathsf{W}$。假设$\\mathbf{U}$包含偏置参数，我们可以将全连接层形式化地表示为\n",
    "\n",
    "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\n",
    " &=  [\\mathbf{U}]_{i, j} +\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}$$\n",
    "\n",
    "其中，从$\\mathsf{W}$到$\\mathsf{V}$的转换只是形式上的转换，因为在这两个四阶张量的元素之间存在一一对应的关系。我们只需重新索引下标$(k, l)$，使$k = i+a$、$l = j+b$，由此可得$[\\mathsf{V}]_{i, j, a, b} = [\\mathsf{W}]_{i, j, i+a, j+b}$。索引$a$和$b$通过在正偏移和负偏移之间移动覆盖了整个图像。对于隐藏表示中任意给定位置（$i$,$j$）处的像素值$[\\mathbf{H}]_{i, j}$，可以通过在$x$中以$(i, j)$为中心对像素进行加权求和得到，加权使用的权重为$[\\mathsf{V}]_{i, j, a, b}$。\n",
    "\n",
    "\n",
    "### 平移不变性\n",
    "现在引用上述的第一个原则：平移不变性。这意味着检测对象在输入$\\mathbf{X}$中的平移，应该仅导致隐藏表示$\\mathbf{H}$中的平移。也就是说，$\\mathsf{V}$和$\\mathbf{U}$实际上不依赖于$(i, j)$的值，即$[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$。并且$\\mathbf{U}$是一个常数，比如$u$。因此，我们可以简化$\\mathbf{H}$定义为：\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b} [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "这就是**卷积**（convolution）。我们是在使用系数$[\\mathbf{V}]_{a, b}$对位置$(i, j)$附近的像素$(i+a, j+b)$进行加权得到$[\\mathbf{H}]_{i, j}$。注意，$[\\mathbf{V}]_{a, b}$的系数比$[\\mathsf{V}]_{i, j, a, b}$少很多，因为前者不再依赖于图像中的位置。这就是显著的进步！\n",
    "\n",
    "### 局部性\n",
    "\n",
    "现在引用上述的第二个原则：局部性。如上所述，为了收集用来训练参数$[\\mathbf{H}]_{i, j}$的相关信息，我们不应偏离到距$(i, j)$很远的地方。这意味着在$|a|> \\Delta$或$|b| > \\Delta$的范围之外，我们可以设置$[\\mathbf{V}]_{a, b} = 0$。因此，我们可以将$[\\mathbf{H}]_{i, j}$重写为\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "简而言之，上式是一个**卷积层**（convolutional layer），而卷积神经网络是包含卷积层的一类特殊的神经网络。在深度学习研究社区中，$\\mathbf{V}$被称为**卷积核**（convolution kernel）或者**滤波器**（filter），亦或简单地称之为该卷积层的**权重**，通常该权重是可学习的参数。\n",
    "\n",
    "当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异可能是巨大的：以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。\n",
    "\n",
    "参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。以上所有的权重学习都将依赖于**归纳偏置**。\n",
    "\n",
    "当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1.3. 卷积\n",
    "在进一步讨论之前，我们先简要回顾一下为什么上面的操作被称为卷积。在数学中，两个函数（比如$f, g: \\mathbb{R}^d \\to \\mathbb{R}$）之间的“卷积”被定义为\n",
    "\n",
    "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n",
    "\n",
    "也就是说，卷积是当把一个函数“翻转”并移位$\\mathbf{x}$时，测量$f$和$g$之间的重叠。当为离散对象时，积分就变成求和。例如：对于由索引为$\\mathbb{Z}$的、平方可和的、无限维向量集合中抽取的向量，我们得到以下定义：\n",
    "\n",
    "$$(f * g)(i) = \\sum_a f(a) g(i-a).$$\n",
    "\n",
    "对于二维张量，则为$f$的索引$(a, b)$和$g$的索引$(i-a, j-b)$上的对应加和：\n",
    "\n",
    "$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$$\n",
    "\n",
    "这看起来类似于局部性中的公式，但有一个主要区别：这里不是使用$(i+a, j+b)$，而是使用差值。我们在局部性中的公式中的原始定义更正确地描述了*互相关*（cross-correlation），这个问题将在下一节中讨论。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1.4. “沃尔多在哪里”回顾\n",
    "\n",
    "回到上面的“沃尔多在哪里”游戏，让我们看看它到底是什么样子。卷积层根据滤波器V选取给定大小的窗口，并加权处理图片，我们的目标是学习一个模型，以便探测出在“沃尔多”最可能出现的地方。\n",
    "\n",
    "![](../img/6_9.png)\n",
    "\n",
    "### 通道\n",
    "然而这种方法有一个问题：我们忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。实际上，图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量，比如包含$1024 \\times 1024 \\times 3$个像素。前两个轴与像素的空间位置有关，而第三个轴可以看作是每个像素的多维表示。因此，我们将$\\mathsf{X}$索引为$[\\mathsf{X}]_{i, j, k}$。由此卷积相应地调整为$[\\mathsf{V}]_{a,b,c}$，而不是$[\\mathbf{V}]_{a,b}$。\n",
    "\n",
    "此外，由于输入图像是三维的，我们的隐藏表示$\\mathsf{H}$也最好采用三维张量。换句话说，对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。因此，我们可以把隐藏表示想象为一系列具有二维张量的**通道**（channel）。这些通道有时也被称为**特征映射**（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。直观上你可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。\n",
    "\n",
    "为了支持输入$\\mathsf{X}$和隐藏表示$\\mathsf{H}$中的多个通道，我们可以在$\\mathsf{V}$中添加第四个坐标，即$[\\mathsf{V}]_{a, b, c, d}$。综上所述，\n",
    "\n",
    "$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$$\n",
    "\n",
    "其中隐藏表示$\\mathsf{H}$中的索引$d$表示输出通道，而随后的输出将继续以三维张量$\\mathsf{H}$作为输入进入下一个卷积层。所以，上式可以定义具有多个通道的卷积层，而其中$\\mathsf{V}$是该卷积层的权重。\n",
    "\n",
    "然而，仍有许多问题亟待解决。例如，图像中是否到处都有存在沃尔多的可能？如何有效地计算输出层？如何选择适当的激活函数？为了训练有效的网络，如何做出合理的网络设计选择？我们将在本章的其它部分讨论这些问题。\n",
    "\n",
    "![](../img/6_10.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1.5. 小结\n",
    "- 图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。\n",
    "\n",
    "- 局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。\n",
    "\n",
    "- 在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。\n",
    "\n",
    "- 卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。\n",
    "\n",
    "- 多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}